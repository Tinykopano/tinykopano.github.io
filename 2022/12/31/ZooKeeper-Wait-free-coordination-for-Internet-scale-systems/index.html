<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>ZooKeeper: Wait-free coordination for Internet-scale systems | KoPaNo&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="ZooKeeper: 互联网级系统的无等待协调摘要在本文中，我们描述了ZooKeeper，一个用于协调分布式应用程序进程的服务。由于ZooKeeper是关键基础设施的一部分，ZooKeeper的目的是为在客户端建立更复杂的协调原语提供一个简单和高性能的内核。它在一个复制的集中式服务中整合了群组消息传递、共享寄存器和分布式锁服务的元素。ZooKeeper提供的接口具有共享寄存器的免等待功能，以及类似">
<meta property="og:type" content="article">
<meta property="og:title" content="ZooKeeper: Wait-free coordination for Internet-scale systems">
<meta property="og:url" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/index.html">
<meta property="og:site_name" content="KoPaNo&#39;s Blog">
<meta property="og:description" content="ZooKeeper: 互联网级系统的无等待协调摘要在本文中，我们描述了ZooKeeper，一个用于协调分布式应用程序进程的服务。由于ZooKeeper是关键基础设施的一部分，ZooKeeper的目的是为在客户端建立更复杂的协调原语提供一个简单和高性能的内核。它在一个复制的集中式服务中整合了群组消息传递、共享寄存器和分布式锁服务的元素。ZooKeeper提供的接口具有共享寄存器的免等待功能，以及类似">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure01.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure02.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure03.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure04.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure05.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/table01.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure06.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure07.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure08.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/table02.jpg">
<meta property="og:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/table03.jpg">
<meta property="article:published_time" content="2022-12-31T22:22:00.000Z">
<meta property="article:modified_time" content="2023-03-26T04:16:16.798Z">
<meta property="article:author" content="KoPaNo">
<meta property="article:tag" content="distributed system">
<meta property="article:tag" content="zookeeper">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure01.jpg">
  
    <link rel="alternate" href="/atom.xml" title="KoPaNo's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 6.3.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">KoPaNo&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="https://tinykopano.github.io"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-ZooKeeper-Wait-free-coordination-for-Internet-scale-systems" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/" class="article-date">
  <time class="dt-published" datetime="2022-12-31T22:22:00.000Z" itemprop="datePublished">2023-01-01</time>
</a>
    
  <div class="article-category">
    <a class="article-category-link" href="/categories/Course/">Course</a>►<a class="article-category-link" href="/categories/Course/Paper/">Paper</a>
  </div>

  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      ZooKeeper: Wait-free coordination for Internet-scale systems
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <h1 id="ZooKeeper-互联网级系统的无等待协调"><a href="#ZooKeeper-互联网级系统的无等待协调" class="headerlink" title="ZooKeeper: 互联网级系统的无等待协调"></a>ZooKeeper: 互联网级系统的无等待协调</h1><h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>在本文中，我们描述了ZooKeeper，一个用于协调分布式应用程序进程的服务。由于ZooKeeper是关键基础设施的一部分，ZooKeeper的目的是为在客户端建立更复杂的协调原语提供一个简单和高性能的内核。它在一个复制的集中式服务中整合了群组消息传递、共享寄存器和分布式锁服务的元素。ZooKeeper提供的接口具有共享寄存器的免等待功能，以及类似于分布式文件系统的缓存失效的事件驱动机制，以提供一个简单而强大的协调服务。</p>
<p>ZooKeeper接口能够实现高性能的服务。除了无等待的特性外，ZooKeeper还为每个客户端提供了以下保证：请求的FIFO执行和所有改变ZooKeeper状态请求的可线性化。这些设计决定使高性能的处理管道得以实现，读取请求由本地服务器满足读取请求。我们展示了对于目标工作负载，即2:1到100:1的读写比，ZooKeeper可以每秒处理几万到几十万的事务。这种性能使ZooKeeper可以被客户端应用程序广泛使用。</p>
<span id="more"></span>

<h2 id="一、-引言"><a href="#一、-引言" class="headerlink" title="一、   引言"></a>一、   引言</h2><p>大规模的分布式应用需要不同形式的协调。配置是协调的最基本形式之一。在最简单的形式中，配置只是系统进程的操作参数列表，而更复杂的系统则有动态配置参数。在分布式系统中，群组成员和领导者的选举也很常见：经常有进程需要知道哪些其他进程是活的，这些进程负责什么。锁构成了一个强大的协调原语，实现了对关键资源的互斥访问。</p>
<p>协调的方法之一是为每个不同的协调需求开发服务。例如，Amazon Simple Queue Service[3]专门关注队列问题。还有的服务是专门为领导者选举[25]和配置[27]开发的。已经实现了更强大的原语的服务可以用来实现弱一些的原语。例如 例如，Chubby[6]是一个锁定服务，具有强大的同步保证。然后，锁可以被用来实现领导者选举、组成员资格等。</p>
<p>在设计我们的协调服务时，我们放弃了在服务器端实现特定的原语，而是选择了暴露一个API，使应用开发者能够实现他们自己的原语。这样的选择导致了协调内核的实现，它可以在不需要改变服务核心的情况下实现新的原语。这种方法使多种形式的协调适应应用程序的要求，而不是将开发者限制在一套固定的原语中。</p>
<p>在设计ZooKeeper的API时，我们摒弃了阻塞原语，如锁。协调服务的阻塞原语会导致，除其他问题外，缓慢或有问题的客户端会对快速客户端的性能产生负面影响。如果处理请求依赖于其他客户端的响应和故障检测，那么服务的实现本身就会变得更加复杂。因此，我们的系统Zookeeper实现了一个操作简单的无等待数据对象的API，像文件系统一样分层组织。事实上，ZooKeeper的API类似于其他文件系统的API，只看API的签名，ZooKeeper似乎是没有锁方法、打开和关闭的Chubby。然而，实现无等待的数据对象使ZooKeeper与基于阻塞原语的系统（如锁）有明显的区别。</p>
<p>尽管无等待属性对性能和容错很重要，但它对协调来说是不够的。我们还必须为操作提供顺序保证。特别是，我们发现，保证所有操作的先进先出（FIFO）客户端顺序和可线性化的写入可以有效地实现服务，这足以实现我们应用所关心的协调原语。事实上，我们可以用我们的API为任何数量的进程实现共识，根据Herlihy的层次结构，ZooKeeper实现了一个通用对象[14]。</p>
<p>ZooKeeper服务包括一个使用复制来实现高可用性和性能的服务器集合。它的高性能使包括大量进程的应用程序能够使用这样一个协调内核来管理协调的所有方面。我们能够使用一个简单的流水线架构来实现ZooKeeper，该架构允许我们有成百上千的请求未处理，同时仍然实现低延迟。这样的流水线自然能够以先进先出的顺序执行来自单个客户端的操作。保证先进先出的客户端顺序使客户端能够异步地提交操作。有了异步操作，一个客户端就能在同一时间有多个未完成的操作。这个功能是可取的，例如，当一个新的客户端成为领导者，它必须操作元数据并相应地更新它。如果没有多个未完成操作的可能性，初始化的时间可能是几秒钟，而不是亚秒级的。</p>
<p>为了保证更新操作满足线性化，我们实现了一个基于领导的原子广播协议[23]，称为Zab[24]。然而，ZooKeeper应用程序的典型工作负载是由读操作主导的，因此扩展读吞吐量是可取的。在ZooKeeper中，服务器在本地处理读取操作，我们不使用Zab来对其进行完全排序。</p>
<p>在客户端缓存数据是提高读取性能的一项重要技术。例如，对于一个进程来说，缓存当前领导者的标识符是非常有用的，而不是在每次需要知道领导者的时候去探测ZooKeeper。ZooKeeper使用观察机制，使客户端能够在不直接管理客户端缓存的情形下存储数据。通过这种机制，客户端可以监视某个数据对象的更新，并在更新时收到通知。Chubby直接管理客户端的缓冲区。它阻止更新，使所有缓存被改变的数据的客户端的缓存失效。在这种设计下，如果这些客户端中的任何一个速度慢或有问题，更新就会被延迟。Chubby使用租约来防止一个有问题的客户无限期地阻塞系统。然而，租约只限制了慢的或有问题的客户端的影响，而ZooKeeper的观察机制完全避免了这个问题。</p>
<p>在本文中，我们讨论了我们对ZooKeeper的设计和实现。通过ZooKeeper，我们能够实现我们的应用程序所需要的所有协调原语，尽管只有写是可线性化的。为了验证我们的方法，我们展示了我们如何用ZooKeeper实现一些协调原语。<br>总而言之，本文的主要贡献是：</p>
<p><strong>协调内核</strong>：我们提出了一种具有宽松的一致性保证的无等待协调服务，用于分布式系统中。特别是，我们描述了我们对协调内核的设计和实现，我们已经在许多关键的应用中使用它来实现各种协调技术。</p>
<p><strong>协调的秘诀</strong>：我们展示了如何使用ZooKeeper来构建更高层次的协调原语，甚至是分布式应用中经常使用的阻塞和强一致性原语。</p>
<p><strong>协调的经验</strong>：我们分享一些我们使用ZooKeeper的方法，并评估其性能。</p>
<h2 id="二、-Zookeeper服务"><a href="#二、-Zookeeper服务" class="headerlink" title="二、   Zookeeper服务"></a>二、   Zookeeper服务</h2><p>客户端通过客户端API使用ZooKeeper客户端库向ZooKeeper提交请求。除了通过客户端API公开ZooKeeper服务接口外，客户端库还负责管理客户端和ZooKeeper服务器之间的网络连接。</p>
<p>在这一节中，我们先从高层次了解ZooKeeper服务。然后我们讨论客户端用来与ZooKeeper交互的API。</p>
<p><strong>术语</strong>：在本文中，我们用客户端表示ZooKeeper服务的用户，用服务器表示提供ZooKeeper服务的进程，用znode表示ZooKeeper数据中的一个内存数据节点，它被组织在一个被称为数据树的分层命名空间中。我们还使用update和write来指代任何修改数据树状态的操作。客户端在连接到ZooKeeper时建立一个会话，并获得一个会话句柄，通过它发出请求。</p>
<h3 id="2-1-服务概述"><a href="#2-1-服务概述" class="headerlink" title="2.1 服务概述"></a>2.1 服务概述</h3><p>ZooKeeper为其客户端提供一组数据节点（znodes）的抽象，这些节点按照分层的名称空间组织。这个层次结构中的znodes是客户通过ZooKeeper API操作的数据对象。分层名称空间通常在文件系统中使用。这是一种理想的数据对象组织方式，因为用户已经习惯了这种抽象，它可以更好地组织应用程序的元数据。为了指代一个给定的znode，我们使用文件系统路径的标准UNIX符号。例如，我们使用 <em>&#x2F;A&#x2F;B&#x2F;C</em> 来表示到Znode <em>C</em> 的路径，其中 <em>C</em> 有 <em>B</em> 作为它的父节点， <em>B</em> 有 <em>A</em> 作为它的父节点。所有的节点都存储数据，并且除了临时节点外的所有的节点，都可以有子节点。</p>
<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure01.jpg" alt="Zookeeper分层命名空间图解" title="Zookeeper分层命名空间图解"></p>
<div style="text-align: center;"><b>图1</b>：Zookeeper分层命名空间图解。</div>

<p>客户端可以创建两种类型的znodes：</p>
<p><strong>普通的</strong>：客户端通过明确地创建和删除它们来操纵常规的znodes。</p>
<p><strong>临时的</strong>：客户端创建这样的znodes，他们或者明确地删除它们，或者让系统在创建它们的会话终止时（故意的或由于失败）自动删除它们。</p>
<p>此外，当创建一个新的znode时，客户可以设置一个顺序标志。在设置了顺序标志的情况下创建的节点有一个单调增加的计数器的值附加在它的名字上。如果 <em>n</em> 是新的znode， <em>p</em> 是父znode，那么 <em>n</em> 的序列值永远不会小于 <em>p</em> 下曾经创建的任何其他序列znode的名字中的值。</p>
<p>ZooKeeper实现了观察功能，允许客户端及时收到变化的通知，而不需要轮询。当客户端发出一个设置了watch标志的读操作时，除了服务器承诺在返回的信息发生变化时通知客户端外，操作会正常完成。观察是与会话相关的一次性触发器；一旦被触发或会话关闭，它们就不再被注册。观察器表明发生了变化，但不提供具体变化信息。例如，如果客户端在”&#x2F;foo”被改变两次之前发出 <em>getData(‘’&#x2F;foo’’, true)</em> ，客户端只会得到一个观察事件，告诉客户端”&#x2F;foo “的数据已经改变。会话事件，如连接丢失事件，也会被发送到观察回调，以便客户端知道观察事件可能会被延迟。</p>
<p><strong>数据模型</strong>：ZooKeeper的数据模型本质上是一个简化了API的文件系统，只有全数据的读写，或者是一个带有分层键的键&#x2F;值表。分层的命名空间对于为不同应用程序的名字空间分配子树和设置这些子树的访问权限是很有用的。我们还利用客户端的目录概念来构建更高层次的原语，我们将在第2.4节看到。</p>
<p>与文件系统中的文件不同，znodes不是为一般的数据存储而设计的。相反，znodes映射到客户端应用程序的抽象，通常对应于用于协调目的的元数据。为了说明问题，在图1中我们有两个子树，一个是应用1（&#x2F;app1），另一个是应用2（&#x2F;app2）。应用1的子树实现了一个简单的组成员协议：每个客户进程 <em>p<sub>i</sub></em> 在&#x2F;app1下创建一个znode p_i，只要该进程在运行，它就会一直存在。</p>
<p>尽管znodes并不是为一般的数据存储而设计的，但ZooKeeper确实允许客户端存储一些信息，这些信息可以用于分布式计算中的元数据或配置。例如，在基于领导者的应用中，对于刚刚开始的应用服务器来说，了解其他哪台服务器目前是领导者是很有用的。为了实现这个目标，我们可以让当前的领导者在znode空间的一个已知位置上写下这个信息。Znode也有相关的元数据，有时间戳和版本计数器，允许客户端跟踪Znode的变化，并根据Znode的版本执行条件更新。</p>
<p><strong>会话</strong>：客户端连接到ZooKeeper并启动一个会话。会话有一个相关的超时时间。如果客户端在超过该超时时间内没有收到任何来自其会话的信息，ZooKeeper就认为它是有问题的。当客户端明确关闭会话句柄或ZooKeeper检测到客户端有问题时，会话就会结束。在一个会话中，客户端观察到的是反映其操作执行情况的一系列状态变化。会话使客户端能够在ZooKeeper集合中透明地从一个服务器移动到另一个服务器，并因此在ZooKeeper服务器之间持续存在。</p>
<h3 id="2-2-客户端API"><a href="#2-2-客户端API" class="headerlink" title="2.2 客户端API"></a>2.2 客户端API</h3><p>我们在下面介绍ZooKeeper API的相关子集，并讨论每个请求的语义。</p>
<p><strong>create(path, data, flags)</strong>:创建一个具有路径名称 <em>path</em> 的znode，在其中存储 <em>data[]</em> ，并返回新znode的名称。 <em>flags</em> 使客户端能够选择znode的类型：普通的、临时的，并设置顺序的标识;</p>
<p><strong>delete(path, version)</strong>:如果该znode处于预期的版本中，删除 <em>path</em> 代表的znode;</p>
<p><strong>exists(path, watch)</strong>:如果路径名称为 <em>path</em> 的znode存在，则返回true，否则返回false。 <em>watch</em> 标志使客户端能够在znode上设置一个监视;</p>
<p><strong>getData(path, watch)</strong>:返回与znode相关的数据和元数据，例如版本信息。 <em>watch</em> 标志的工作方式与 <em>exists()</em> 相同，只是如果znode不存在，ZooKeeper不会设置watch;</p>
<p><strong>setData(path, data, version)</strong>:如果版本号是 znode的当前版本 ,写入 <em>data[]</em> 到 znode <em>path</em>;</p>
<p><strong>getChildren(path, watch)</strong>:返回子节点的名称集合;</p>
<p><strong>sync(path)</strong>:等待所有在操作开始时挂起的更新传播到客户端连接的服务器上。path目前被忽略。</p>
<p>所有的方法都有一个同步和异步的版本，可以通过API使用。当一个应用程序需要执行一个单一的ZooKeeper操作，并且没有并发的任务要执行时，它就会使用同步API，所以它进行必要的ZooKeeper调用和阻塞。然而，异步API使一个应用程序能够同时拥有多个未完成的ZooKeeper操作和其他并行执行的任务。ZooKeeper客户端保证每个操作的相应回调被依次调用。</p>
<p>需要注意的是，ZooKeeper不使用句柄来访问znode。每个请求都包括被操作的znode的完整路径。这种选择不仅简化了API（没有open()或close()方法），而且还消除了服务器需要维护的额外状态。</p>
<p>每个更新方法都需要一个预期的版本号，这使得有条件的更新得以实现。如果znode的实际版本号与预期的版本号不一致，则更新失败，出现意外的版本错误。如果版本号为-1，则不执行版本检查。</p>
<h3 id="2-3-ZooKeeper的保证"><a href="#2-3-ZooKeeper的保证" class="headerlink" title="2.3 ZooKeeper的保证"></a>2.3 ZooKeeper的保证</h3><p>ZooKeeper有两个基本的排序保证：</p>
<p><strong>可线性化的写入</strong>：所有更新ZooKeeper状态的请求都是可序列化的，并尊重优先权；</p>
<p><strong>FIFO的客户端顺序</strong>：来自特定客户端的所有请求都按照客户端发送的顺序执行。</p>
<p>请注意，我们对可线性化的定义与Herlihy[15]最初提出的定义不同，我们称之为A-线性化（异步线性化）。在Herlihy对线性化的最初定义中，一个客户端在同一时间只能有一个未完成的操作（一个客户端就是一个线程）。在我们的定义中，我们允许一个客户端有多个未完成的操作，因此我们可以选择保证同一客户端的未完成操作没有特定的顺序，或者保证先进先出的顺序。我们选择后者作为我们的属性。需要注意的是，所有对可线性化对象成立的结果也对A-线性化对象成立，因为一个满足A-线性化的系统也满足线性化。因为只有更新请求是可线性化的，ZooKeeper在每个副本中都会本地处理读取请求。这使得服务可以随着服务器的增加而线性地扩展到系统中。</p>
<p>为了了解这两种保证是如何相互作用的，请考虑以下情况。一个由若干进程组成的系统选出一个领导者来命令worker进程。当一个新的领导者负责该系统时，它必须改变大量的配置参数，并在完成后通知其他进程。这样我们必须满足如下两个要求：</p>
<ul>
<li>当新的领导者开始进行改变时，我们不希望其他进程开始使用正在被改变的配置；</li>
<li>如果新的领导者在配置完全更新之前死亡，我们不希望进程使用这个部分配置。</li>
</ul>
<p>请注意，分布式锁，如Chubby提供的锁，将有助于满足第一个要求，但不足以满足第二个要求。在ZooKeeper中，新的领导者可以指定一个路径作为ready znode；其他进程只有在该znode存在时才会使用配置。新的领导者通过删除ready，更新各种配置的znode，并创建ready来进行配置的改变。所有这些变化都可以通过流水线和异步发布来快速更新配置状态。虽然改变操作的延迟是2毫秒的量级，但如果一个新的领导者必须更新5000个不同的znodes，如果请求是一个接一个地发出，将需要10秒钟；通过异步发出请求，请求将花费不到一秒钟。由于排序的保证，如果一个进程看到了ready znode，它也必须看到新的领导者所做的所有配置改变。如果新的领导者在ready znode创建之前就死亡，其他进程知道配置还没有最终完成，就不会使用它。</p>
<p>上述方案仍有一个问题：如果一个进程在新的领导者开始进行变更之前看到ready存在，然后在变更进行时开始读取配置，会发生什么情况。这个问题通过对通知的排序保证得到了解决：如果一个客户端正在监听变更，客户端将在看到变更后系统的新状态之前看到通知事件。因此，如果读取ready znode的进程要求被通知该znode的变更，它将在读取任何新的配置之前看到通知，告知客户端的变更。</p>
<p>当客户端在ZooKeeper之外还有自己的通信通道时，会出现另一个问题。例如，考虑到两个客户端A和B在ZooKeeper中拥有一个共享配置，并通过一个共享通信通道进行通信。如果A改变了ZooKeeper中的共享配置，并通过共享通信通道告诉B这个改变，B就会在重新读取配置时看到这个改变。如果B的ZooKeeper副本稍微落后于A的，它可能看不到新的配置。利用上述保证，B可以通过在重新读取配置之前发出一个写入指令来确保它看到最新的信息。为了更有效地处理这种情况，ZooKeeper提供了sync请求：当紧随其后的是读，就构成了慢速读。sync使服务器在处理读之前应用所有悬而未决的写请求，而不需要完全写的开销。这个原语与ISIS[5]的flush原语的想法相似。</p>
<p>ZooKeeper还具有以下两个有效性和持久性保证：如果大多数ZooKeeper服务器处于活动状态并进行通信，那么服务将是可用的；如果ZooKeeper服务成功地响应了一个变更请求，那么只要有一组 quorum 服务器最终能够恢复，该变更在任何数量的故障中都会持续。</p>
<h3 id="2-4-原语的例子"><a href="#2-4-原语的例子" class="headerlink" title="2.4 原语的例子"></a>2.4 原语的例子</h3><p>在本节中，我们将展示如何使用ZooKeeper的API来实现更强大的原语。ZooKeeper服务对这些更强大的原语一无所知，因为它们完全是在客户端使用ZooKeeper客户端API实现的。一些常见的原语，如群组成员和配置管理也是无需等待的。对于其他的原语，如rendezvous，客户端需要等待一个事件。尽管ZooKeeper是无等待的，但我们可以用ZooKeeper实现高效的阻塞原语。ZooKeeper的排序保证允许对系统状态进行有效的推理，而观察机制允许有效的等待。</p>
<p><strong>配置管理</strong>  ZooKeeper可用于在分布式应用程序中实现动态配置。在其最简单的形式中，配置被存储在一个znode中，即 <em>z<sub>c</sub></em> 。进程启动时使用 <em>z<sub>c</sub></em> 的完整路径名。启动进程通过读取 <em>z<sub>c</sub></em> 来获得他们的配置，并将watch标志设置为true。如果 <em>z<sub>c</sub></em> 中的配置被更新，进程会被通知并读取新的配置，并再次将watch标志设为true。</p>
<p>请注意，在这个方案中，和其他大多数使用观察的方案一样，观察被用来确保进程拥有最新的信息。例如，如果一个观察 <em>z<sub>c</sub></em> 的进程被通知了 <em>z<sub>c</sub></em> 的变化，而在它发出对 <em>z<sub>c</sub></em> 的读取之前， <em>z<sub>c</sub></em> 又有三个变化，那么这个进程就不会再收到三个通知事件。这并不影响进程的行为，因为这三个事件只是通知进程它已经知道的事情：它拥有的关于 <em>z<sub>c</sub></em> 的信息是过时的。</p>
<p><strong>会和（Rendezvous）</strong>  有时候在分布式系统中并不能清晰的预知系统的最终配置会是什么情形。例如，某个客户端可能会希望启动一个master进程和几个worker进程，不过由于节点的启动是由某个调度器执行，客户端并不能事先知道某些需要的信息，例如工作节点需要连接的主节点的地址和端口号。这个问题可以由客户端通过 ZooKeeper 创建一个 rendezvous 节点 <em>z<sub>r</sub></em> 来解决。客户端将 <em>z<sub>r</sub></em> 的全路径作为启动参数传给master和worker进程。当master启动后，它就将自己的地址和端口号写入到 <em>z<sub>r</sub></em> 中。当woker启动后，它就能从 <em>z<sub>r</sub></em> 中读取，并设置watch标识味true。如果 <em>z<sub>r</sub></em> 仍未被填满，那么worker就会等待 <em>z<sub>r</sub></em> 更新的通知。如果 <em>z<sub>r</sub></em> 是临时节点，master和worker就能观察到 <em>z<sub>r</sub></em> 被删除通知，并在完成资源清理后退出。</p>
<p><strong>群组成员</strong>  我们可以利用临时节点的特性来实现群组成员关系管理。具体来说，我们利用了临时节点允许观测创建该节点的 session 状态这一特性。首先创建一个节点 <em>z<sub>g</sub></em> 来表示群组。当群组中的某个进程启动时，它会在 <em>z<sub>g</sub></em> 下创建一个临时的子节点。如果每个进程都有唯一的命名或标识，那么这个命名或标识就可以作为 ZooKeeper 节点的名称；否则就可以在创建节点时设置 SEQUENTIAL 标记让 ZooKeeper 自动在节点名称后追加一个单调递增的数字，以保证名称的唯一性。各进程可以将进程相关的信息放到子节点中，例如当前进程的地址和端口号。</p>
<p>在节点 <em>z<sub>g</sub></em> 下创建完子节点后，进程就可以正常启动。它不需要做其他任何事。如果这个进程失败或者结束，那么它所创建的 <em>z<sub>g</sub></em> 下的子节点也会自动被删除。</p>
<p>各进程可以简单的通过查询 <em>z<sub>g</sub></em> 的所有子节点来获取当前群组成员的信息。如果某个进程想要监控群组成员的变化，那么它可以设置watch标记为true（总是设置watch标记为true），当它收到变更通知时，就可以刷新群组信息。</p>
<p><strong>简单锁</strong>  ZooKeeper虽然不是锁服务，但是可以用来实现锁。 使用 ZooKeeper 的应用程序通常使用根据其需要定制的同步原语，例如上面显示的那些。 这里我们展示如何使用 ZooKeeper 实现锁，以表明它可以实现各种通用同步原语。</p>
<p>最简单的锁实现借助于 lock files。使用一个 znode 来表示一把锁。为了获取锁，客户端会尝试以 EPHEMERAL 标记创建指定的 znode。如果创建成功，那么这个客户端就获得了锁。否则，客户端就会去读取这个 znode 并设置watch标识，从而当这个领导者被删除时能收到通知。当持有锁的客户端发生异常或者主动删除该节点时，则代表释放了锁。其他监听的客户端就会收到通知并尝试重新创建临时节点来获取锁。</p>
<p>虽然这个简单的锁定协议管用，但它确实存在一些问题。 首先，它受到惊群效应的影响。 如果有很多客户端在等待获取锁，即使只有一个客户端可以获取锁，当锁被释放时，它们也会争抢。 其次，它只实现了排它锁。以下两个原语显示了如何克服这两个问题。</p>
<p><strong>无惊群效应的简单锁</strong>  我们定义一个锁znode <em>l</em> 来实现这样的锁。直观上，我们排列所有请求锁的客户端，每个客户端都按请求到达的顺序获得锁。因此，希望获得锁的客户端会有如下操作:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Lock</span><br><span class="line">1 n = create(l + “/lock-”, EPHEMERAL|SEQUENTIAL)</span><br><span class="line">2 C = getChildren(l, false)</span><br><span class="line">3 if n is lowest znode in C, exit</span><br><span class="line">4 p = znode in C ordered just before n</span><br><span class="line">5 if exists(p, true) wait for watch event</span><br><span class="line">6 goto 2</span><br><span class="line"></span><br><span class="line">Unlock</span><br><span class="line">1 delete(n)</span><br></pre></td></tr></table></figure>

<p>第1行 SEQUENTIAL 的标记用来将所有希望获取锁的客户端进行排序。每个客户端首先在节点 <em>l</em> 下创建一个临时顺序的子节点，然后获取 <em>l</em> 的所有子节点。之后在第3行判断自己创建的节点是否在所有子节点中有着最小的序号，如果是，则表示当前客户端获得了锁。如果不是，说明有其他序号更小的子节点存在，当前客户端需要排在这之后获取锁。然后客户端会尝试判断排在当前序号前的子节点是否存在，如果存在则设置监听状态等待前一个节点删除的通知，如果不存在，则继续回到第2行执行。每个客户端只监听排在自己前面的子节点避免了惊群效应，因为任何一个子节点删除的通知只会发给其中的一个客户端。每当客户端收到前面节点删除的通知时，需要再次获取 l 的所有子节点来判断自己是否是最小子节点。（因为排在前面的子节点并不一定持有锁，可能是更前面的子节点持有锁。）</p>
<p>释放锁就是简单的删除对应的临时节点 _n_。而通过 EPHEMERAL 标记创建节点能保证进程异常时自动释放锁或者放弃对锁的获取请求。</p>
<p>这种锁实现有以下几个优势：</p>
<ol>
<li>一个节点的删除只会唤醒一个客户端，因为每个节点都只会被一个客户端监听，所以也不会有惊群效应。</li>
<li>锁的获取和释放不依赖轮询或超时。</li>
<li>使用这种方式创建锁使得可以通过查看 ZooKeeper 中的数据来监测锁竞争的数量，以及调试锁相关的问题。</li>
</ol>
<p><strong>读写锁</strong>  为了实现读写锁，我们稍微改变了锁相关的程序代码，并有独立的读锁和写锁过程。释放过程与普通锁的情况相同。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Write Lock</span><br><span class="line">1 n = create(l + “/write-”, EPHEMERAL|SEQUENTIAL)</span><br><span class="line">2 C = getChildren(l, false)</span><br><span class="line">3 if n is lowest znode in C, exit</span><br><span class="line">4 p = znode in C ordered just before n</span><br><span class="line">5 if exists(p, true) wait for event</span><br><span class="line">6 goto 2</span><br><span class="line"></span><br><span class="line">Read Lock</span><br><span class="line">1 n = create(l + “/read-”, EPHEMERAL|SEQUENTIAL)</span><br><span class="line">2 C = getChildren(l, false)</span><br><span class="line">3 if no write znodes lower than n in C, exit</span><br><span class="line">4 p = write znode in C ordered just before n</span><br><span class="line">5 if exists(p, true) wait for event</span><br><span class="line">6 goto 3</span><br></pre></td></tr></table></figure>

<p>该锁的过程与之前的锁略有不同。 写锁仅在命名上有所不同。 由于读锁可能是共享的，第3行和第4行略有不同，因为只有较早的写锁 znode 会阻止客户端获得读锁。 当有多个客户端等待读锁并且收到删除具有较低序列号的“写”znode的通知时，我们可能会出现“惊群效应”； 事实上，这种行为是我们锁期望的，所有那些读的客户端都应该被释放，因为它们现在可能已经持有了该锁。</p>
<p><strong>双重屏障</strong> 双重屏障使客户端能够同步一个计算的开始和结束。当由屏障阈值定义的足够多的进程加入屏障时，进程开始计算，并在完成后离开屏障。 我们用 Znode 来表示ZooKeeper的屏障，称为 _b_。 每一个进程 <em>p</em> 都在 <em>b</em> 通过创建一个 Znode 作为 <em>b</em> 的子节点来注册，在它准备好离开时取消注册。 当 <em>b</em> 的子 Znode 的数目超过屏障阈值时，进程可以进入屏障。 进程可以在所有的进程都已移除其子节点时离开屏障。 我们使用监视器高效地等待进入和退出条件得到满足。 如果要进入屏障的话，进程需要监视 <em>b</em> 的 <em>ready</em> 状态的子节点，当子节点数量超过屏障阈值时，该进程才可以进入。 如果要离开屏障，进程监视某个特定的子节点消失，并且只有在这个特定的 Znode 被删除后才检查退出条件。</p>
<h2 id="三、-Zookeeper应用"><a href="#三、-Zookeeper应用" class="headerlink" title="三、   Zookeeper应用"></a>三、   Zookeeper应用</h2><p>我们下面会描述一些使用了ZooKeeper的应用程序，并简要说明它们是怎样使用的。我们用<strong>粗体</strong>显示每个例子的原语。</p>
<p><strong>The Fetching Service</strong>  爬虫是搜索引擎的重要组成部分，Yahoo!抓取数十亿个Web文档。The Fetching Service（FS）是Yahoo！爬虫的一部分，并且已经投入生产。从本质上讲，它由主进程向页面爬取进程主进程发出命令。主进程为爬取进程提供配置，爬取进程回写通知其状态和运行状况。在FS中使用 ZooKeeper 的主要优点是从主节点的故障中恢复，在发生故障时保证可用性，以及将客户端与服务器分离，允许它们通过从 ZooKeeper 读取其状态来将请求定向到健康的服务器。因此，FS 主要使用 ZooKeeper 来管理<strong>配置元数据</strong>，尽管它也使用 ZooKeeper 来选举主节点（<strong>领导者选举</strong>）。</p>
<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure02.jpg" alt="FS中Zookeeper的负载" title="FS中Zookeeper的负载"></p>
<div style="text-align: center;"><b>图2</b>：FS中Zookeeper的工作负载。每个点代表了一秒的采样。</div>

<p>图2显示了 FS 使用的 ZooKeeper 服务器在三天内的读写流量。为了生成此图，我们统计了这段时间内每秒的操作数，每个点对应一秒。我们观察到读取流量比写入流量高得多。 在速率高于每秒 1, 000 次操作期间，读：写比率在 10:1 和 100:1 之间变化。 此工作负载中的读取操作是 _getData()<em>、_getChildren()</em> 和 _exists()_，按普遍程度递增的顺序排列。</p>
<p><strong>Katta</strong>  Katta [17] 是一个使用 ZooKeeper 进行协调的分布式索引器，它是非 Yahoo! 应用。 Katta 使用分片来划分索引的工作。 主服务器将分片分配给从属服务器并跟踪进度。 从站可能会发生故障，因此主站必须在从站来来去去时重新分配负载。 主服务器也可能发生故障，因此其他服务器必须准备好在发生故障时接管。 Katta 使用 ZooKeeper 来跟踪从服务器和主服务器的状态（<strong>群组成员</strong>），并处理主服务器故障转移（<strong>领导者选举</strong>）。 Katta 还使用 ZooKeeper 来跟踪和传播分片分配给从属（<strong>配置管理</strong>）。</p>
<p><strong>Yahoo! Message Broker</strong>  Yahoo! Message Broker (YMB) 是一个分布式发布-订阅系统。 该系统管理着数千个主题，客户端可以向这些主题发布消息或从中接收消息。 主题分布在一组服务器中以提供可伸缩性。 每个主题都使用主备方案进行复制，该方案确保将消息复制到两台机器以确保可靠的消息传递。 构成 YMB 的服务器使用无共享分布式架构，这使得协调对于正确操作至关重要。 YMB 使用 ZooKeeper 来管理主题的分发（<strong>配置元数据</strong>），处理系统中机器的故障（<strong>故障检测</strong>和<strong>群组成员</strong>），以及控制系统运行。</p>
<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure03.jpg" alt="YMB在Zookeeper中的架构布局" title="YMB在Zookeeper中的架构布局"></p>
<div style="text-align: center;"><b>图3</b>：YMB在Zookeeper中的架构布局。</div>

<p>图 3 显示了 YMB 的部分 znode 数据布局。 每个broker域都有一个称为 <em>nodes</em> 的 znode，它为组成 YMB 服务的每个活动服务器提供一个临时 znode。 每个 YMB 服务器在 <em>nodes</em> 下创建一个临时 znode，其中包含负载和状态信息，通过 ZooKeeper 提供组成员资格和状态信息。 <em>shutdown</em> 和  <em>migration_prohibited</em> 等节点由构成服务的所有服务器监控，并允许对 YMB 进行集中控制。 对于 YMB 管理的每个主题，<em>topics</em> 目录都有一个子 znode。 这些主题 znode 具有子 znode，它们指示每个主题的主服务器和备用服务器以及该主题的订阅者。 <em>primary</em> 和 <em>backup</em> 服务器 znode 不仅可以让服务器发现负责某个主题的服务器，还可以管理<strong>领导者选举</strong>和服务器崩溃。</p>
<h2 id="四、-Zookeeper实现"><a href="#四、-Zookeeper实现" class="headerlink" title="四、   Zookeeper实现"></a>四、   Zookeeper实现</h2><p>ZooKeeper通过在组成服务的每个服务器上复制ZooKeeper数据来提供高可用性。我们假设服务器会因崩溃而失败，而这些有问题的服务器后来可能会恢复。图4显示了ZooKeeper服务的高级组件。在收到一个请求后，一个服务器准备执行它（请求处理器）。如果这样的请求需要服务器之间的协调（写请求），那么他们就会使用一个协议（原子广播的实现），最后服务器将变化提交给ZooKeeper数据库，并在该集合的所有服务器上完全复制。在读请求的情况下，服务器只需读取本地数据库的状态并生成对请求的响应。</p>
<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure04.jpg" alt="Zookeeper服务组件" title="Zookeeper服务组件"></p>
<div style="text-align: center;"><b>图4</b>：Zookeeper服务组件。</div>

<p>复制的数据库是一个包含整个数据树的内存数据库。树上的每个节点默认存储最大1MB的数据，但这个最大值是一个配置参数，可以在特定情况下改变。为了保证可恢复性，我们有效地将更新记录到磁盘上，并强制写在磁盘介质上，然后再应用到内存数据库上。事实上，就像Chubby[8]一样，我们保留了一个已提交操作的重放日志（在我们的例子中，是一个写前日志），并定期生成内存数据库的快照。</p>
<p>每个ZooKeeper服务器都为客户提供服务。客户端正好连接到一个服务器来提交其请求。如前所述，读取请求是由每个服务器数据库的本地副本提供服务。改变服务状态的请求，即写请求，由一个一致性协议处理。</p>
<p>作为一致性协议的一部分，写请求被转发到一个单独的服务器，称为领导者1。其余的ZooKeeper服务器，称为追随者，从领导者那里接收由状态变化组成的消息提议，并同意状态变化。</p>
<h3 id="4-1-请求处理器"><a href="#4-1-请求处理器" class="headerlink" title="4.1 请求处理器"></a>4.1 请求处理器</h3><p>由于消息传递层是原子的，我们保证本地副本永远不会出现分歧，尽管在任何时候，一些服务器可能比其他服务器应用了更多的事务。与客户发送的请求不同，事务是幂等的。当领导者收到一个写请求时，它计算当写被应用时系统的状态是什么，并将其转换为一个捕捉这个新状态的事务。未来的状态必须被计算出来，因为可能有未完成的事务还没有被应用到数据库中。例如，如果一个客户做了一个附带条件的 <em>setData</em> ，并且请求中的版本号与被更新的节点的未来版本号相匹配，服务会生成一个 <em>setDataTXN</em> ，其中包含新数据、新版本号和更新的时间戳。如果发生错误，例如版本号不匹配或被更新的节点不存在，则会生成一个 <em>errorTXN</em> 。</p>
<h3 id="4-2-原子广播"><a href="#4-2-原子广播" class="headerlink" title="4.2 原子广播"></a>4.2 原子广播</h3><p>所有更新ZooKeeper状态的请求都被转发给领导者。领导者执行请求，并通过Zab[24]这个原子广播协议广播ZooKeeper状态的变化。收到客户端请求的服务器在传递相应的状态变化时对客户端进行响应。Zab默认使用简单的多数 quorums 机制来决定提案，所以Zab以及ZooKeeper只有在大多数服务器都正确的情况下才能工作（也就是说，在2f + 1个服务器的情况下，我们可以容忍f个失败）。</p>
<p>为了实现高吞吐量，ZooKeeper试图让请求处理管道保持满员。它可能在处理管道的不同部分有成千上万的请求。因为状态的改变取决于先前状态改变的应用，Zab提供了比常规原子广播更强的顺序保证。更具体地说，Zab保证领导者广播的变化按照它们被发送的顺序传递，并且在它广播自己的变化之前，所有来自先前领导者的变化都被传递给已建立的领导者。</p>
<p>有一些实现细节简化了我们的实现，并给我们带来了出色的性能。我们使用TCP进行传输，所以消息的顺序由网络来维持，这使我们能够简化我们的实现。我们使用Zab选择的领导者作为ZooKeeper的领导者，这样，创建事务的进程也会提议事务。我们使用日志来跟踪提议，作为内存数据库的写前日志，这样我们就不必将信息两次写入磁盘。</p>
<p>在正常的操作中，Zab确实按顺序准确地传递了所有的消息，但是由于Zab没有持久地记录每条传递的消息的ID，Zab可能在恢复期间重新传递一条消息。因为我们使用幂等型事务，所以只要是按顺序传递的，多次传递是可以接受的。事实上，ZooKeeper要求Zab至少重新交付所有在最后一次快照开始后交付的消息。</p>
<h3 id="4-3-复制数据库"><a href="#4-3-复制数据库" class="headerlink" title="4.3 复制数据库"></a>4.3 复制数据库</h3><p>每个副本在内存中都有一份ZooKeeper状态的副本。当ZooKeeper服务器从崩溃中恢复时，它需要恢复这个内部状态。在服务器运行一段时间后，重放所有传递的消息来恢复状态会花费太多时间，所以ZooKeeper使用定期快照，只要求重新传递快照开始后的消息。我们称ZooKeeper快照为模糊快照（_fuzzy snapshot_），因为我们不锁定ZooKeeper状态来进行快照；相反，我们对树进行深度扫描，原子式地读取每个znode的数据和元数据，并将它们写入磁盘。由于产生的模糊快照可能已经应用了快照生成过程中的一些状态变化子集，所以结果可能不符合ZooKeeper在任何时间点的状态。然而，由于状态变化是等价的，只要我们按顺序应用状态变化，我们就可以接受它们两次。</p>
<p>例如 例如，假设在 ZooKeeper 数据树中，两个节点 <em>&#x2F;foo</em> 和 <em>&#x2F;goo</em> 的值分别为 <em>f1</em> 和 _g1_，并且在模糊快照开始时都处于版本 1，并且以下状态更改流到达时具有以下形式 <em>&lt;transactionType, path, value, new-version&gt;</em>:</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;SetDataTXN, /foo, f2, 2&gt;</span><br><span class="line">&lt;SetDataTXN, /goo, g2, 2&gt;</span><br><span class="line">&lt;SetDataTXN, /foo, f3, 3&gt;</span><br></pre></td></tr></table></figure>

<p>处理完这些状态更改后，_&#x2F;foo_ 和 <em>&#x2F;goo</em> 的值分别为版本 3 和版本 2 的 <em>f3</em> 和 _g2_。 然而，模糊快照可能记录了 <em>&#x2F;foo</em> 和 <em>&#x2F;goo</em> 分别具有版本 3 和 1 的值 <em>f3</em> 和 _g1_，这不是 ZooKeeper 数据树的有效状态。 如果服务器崩溃并使用此快照恢复并且 Zab 重新交付状态更改，则结果状态对应于崩溃前的服务状态。</p>
<h3 id="4-4-客户端-服务端交互"><a href="#4-4-客户端-服务端交互" class="headerlink" title="4.4 客户端-服务端交互"></a>4.4 客户端-服务端交互</h3><p>当一个服务器处理一个写请求时，它也会发送和清除与该更新相对应的任何观察的通知。服务器按顺序处理写，不同时处理其他写或读。这确保了通知的严格连续。请注意，服务器在本地处理通知。只有客户端连接的服务器才会跟踪并触发该客户端的通知。</p>
<p>读取请求在每个服务器上进行本地处理。每个读取请求都会被处理，并被标记为一个 <em>zxid</em> ，该 <em>zxid</em> 对应于服务器看到的最后一个事务。这个 <em>zxid</em> 定义了读请求相对于写请求的部分顺序。通过本地处理读取，我们获得了出色的读取性能，因为它只是本地服务器上的一个内存操作，没有磁盘活动或协议运行。这一设计选择是实现我们对以读为主的工作负载的卓越性能目标的关键。</p>
<p>使用快速读取的一个缺点是不能保证读取操作的优先顺序。也就是说，一个读操作可能会返回一个陈旧的值，即使最近对同一个 znode 的更新已经提交。并非所有的应用都需要优先顺序，但对于需要优先顺序的应用，我们已经实现了同步。这个原语是异步执行的，并由领导者在对其本地副本的所有未决写操作之后进行排序。为了保证一个给定的读操作返回最新的更新值，客户端在读操作之后调用 <em>sync</em> 。客户端操作的FIFO顺序保证和同步的全局保证使得读取操作的结果能够反映在同步发出之前发生的任何变化。在我们的实现中，我们不需要原子化地广播同步，因为我们使用的是基于领导者的算法，我们只需将同步操作放在领导者和执行同步调用的服务器之间的请求队列的最后。为了使这一方法奏效，追随者必须确定领导者仍然是领导者。如果有未决的事务提交，那么服务器就不会怀疑领导者。如果挂起的队列是空的，领导者需要发出一个空事务来提交，并在该事务之后下令同步。这有一个很好的特性，就是当领导者处于负载状态时，不会产生额外的广播流量。在我们的实现中，超时的设置使领导者在追随者放弃他们之前意识到他们不是领导者，所以我们不发布空事务。</p>
<p>ZooKeeper服务器按照先进先出的顺序处理来自客户端的请求。响应包括响应所对应的 <em>zxid</em> 。甚至在没有活动的间隔期间的心跳信息也包括客户端连接到的服务器所看到的最后一个 <em>zxid</em> 。如果客户端连接到一个新的服务器，新的服务器会通过检查客户端的最后一个 <em>zxid</em> 和它的最后一个 <em>zxid</em> 来确保它对ZooKeeper数据的查看至少和客户端的查看一样是最新的。如果客户端的视图比服务器的更新，服务器就不会重新建立与客户端的会话，直到服务器赶上。由于客户端只看到复制到大多数ZooKeeper服务器上的变化，因此可以保证客户端能够找到另一个拥有最新视图的服务器。这种行为对于保证持久性非常重要。</p>
<p>ZooKeeper使用超时来检测客户端会话的失败。如果在会话超时内没有其他服务器收到来自客户端会话的任何信息，领导者就会确定发生了故障。如果客户端发送的请求足够频繁，那么就不需要发送任何其他消息。否则，客户端会在活动少的时期发送心跳消息。如果客户端不能与服务器通信以发送请求或心跳，它会连接到不同的ZooKeeper服务器以重新建立会话。为了防止会话超时，ZooKeeper客户端库在会话空闲 s&#x2F;3 ms后发送心跳信息，如果2s&#x2F;3 ms内没有服务器的消息，则切换到新的服务器，其中s是会话超时时间，单位为毫秒。</p>
<h2 id="五、-评估"><a href="#五、-评估" class="headerlink" title="五、   评估"></a>五、   评估</h2><p>我们在一个50台服务器组成的集群上做了所有评估。每台服务器都配有有 Xeon 2.1GHz 的双核处理器，4GB内存，千兆以太网，以及2块SATA硬盘。我们将后面的讨论分为两部分：吞吐量和请求的延迟。</p>
<h3 id="5-1-吞吐量"><a href="#5-1-吞吐量" class="headerlink" title="5.1 吞吐量"></a>5.1 吞吐量</h3><p>为了评估我们的系统，我们对系统饱和时的吞吐量以及各种注入的故障的吞吐量变化进行了基准测试。我们改变了组成ZooKeeper服务的服务器的数量，但始终保持客户端的数量不变。为了模拟大量的客户，我们用35台机器来模拟250个同时进行的客户。</p>
<p>我们有一个Java实现的ZooKeeper服务器，以及Java和C客户端。在这些实验中，我们使用Java服务器，配置为在一个专用磁盘上记录，在另一个磁盘上进行快照。我们的基准客户端使用异步的Java客户端API，每个客户端至少有100个请求未完成。每个请求包括对1K数据的读或写。我们没有显示其他操作的基准，因为所有修改状态的操作的性能大致相同，而非状态修改操作的性能，不包括同步，也大致相同。(同步的性能接近于轻量级的写，因为请求必须送到领导那里，但不会被广播。) 客户端每300ms发送一次已完成操作数的计数，我们每6s采样一次。为了防止内存溢出，服务器对系统中的并发请求数量进行节制。ZooKeeper使用请求节流来保持服务器不被淹没。在这些实验中，我们将ZooKeeper服务器配置为最多有2,000个总请求在处理中。</p>
<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure05.jpg" alt="饱和系统随读写比率吞吐量变化" title="饱和系统随读写比率吞吐量变化"></p>
<div style="text-align: center;"><b>图5</b>：饱和系统随读写比率吞吐量变化。</div>

<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/table01.jpg" alt="饱和系统极限吞吐量" title="饱和系统极限吞吐量"></p>
<div style="text-align: center;"><b>表1</b>：饱和系统极限吞吐量。</div>

<p>在图5中，我们显示了当我们改变读和写请求的比例时的吞吐量，每条曲线对应于提供ZooKeeper服务的不同数量的服务器。表1显示了在读取负载的极端情况下的数字。读取吞吐量比写入吞吐量高，因为读取不使用原子广播。该图还显示，服务器的数量对广播协议的性能也有负面影响。从这些图表中，我们观察到，系统中的服务器数量不仅影响到服务可以处理的故障数量，而且还影响到服务可以处理的工作量。请注意，三台服务器的曲线在60%左右与其他服务器相交。这种情况并不是三台服务器配置所独有的，由于本地读取的并行性，所有的配置都会发生。然而，在图中的其他配置中无法观察到这种情况，因为我们为可读性设定了Y轴最大吞吐量的上限。</p>
<p>有两个原因导致写请求比读请求耗时更长。首先，写请求必须经过原子广播，这需要一些额外的处理并增加请求的延迟。写请求处理时间较长的另一个原因是，服务器必须确保在向领导发送确认信息之前，将事务记录到非易失性存储中。原则上，这个要求是过分的，但对于我们的生产系统，我们用性能来换取可靠性，因为ZooKeeper构成了应用的基础真理。我们使用更多的服务器来容忍更多的故障。我们通过将ZooKeeper数据分割成多个ZooKeeper集合来增加写入量。Gray等人[12]曾观察到复制和分区之间的这种性能权衡。</p>
<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure06.jpg" alt="所有客户端连接到领导者时" title="所有客户端连接到领导者时"></p>
<div style="text-align: center;"><b>图6</b>：所有客户端连接到领导者时，饱和系统随读写比率吞吐量变化。</div>

<p>ZooKeeper能够通过在组成服务的服务器之间分配负载来实现如此高的吞吐量。我们可以分配负载，因为我们有宽松的一致性保证。Chubby客户端会将所有的请求指向领导者。图6显示了如果我们不利用这种放松，强迫客户只连接到领导者，会发生什么。正如预期的那样，以读为主的工作负载的吞吐量要低得多，但即使是以写为主的工作负载的吞吐量也要低。为客户提供服务所造成的额外的CPU和网络负载影响了领导者协调建议广播的能力，这反过来又对整个写性能产生了不利影响。</p>
<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure07.jpg" alt="孤立原子广播的平均吞吐量" title="孤立原子广播的平均吞吐量"></p>
<div style="text-align: center;"><b>图7</b>：孤立原子广播的平均吞吐量。误差线表示了最大最小值。</div>

<p>原子广播协议完成了系统的大部分工作，因此对ZooKeeper性能的限制超过了其他任何组件。图7显示了原子广播组件的吞吐量。为了衡量其性能，我们通过直接在领导者处生成事务来模拟客户，所以没有客户连接或客户请求和回复。在最大的吞吐量下，原子广播组件成为CPU的约束。理论上，图7的性能将与ZooKeeper的100%写入性能相匹配。然而，ZooKeeper客户端通信、ACL检查和请求到事务的转换都需要CPU。对CPU的争夺降低了ZooKeeper的吞吐量，大大低于孤立的原子广播组件。因为ZooKeeper是一个重要的生产组件，到目前为止，我们对ZooKeeper的开发重点是正确性和健壮性。有很多机会可以通过消除额外的拷贝、同一对象的多次序列化、更有效的内部数据结构等来大幅提高性能。</p>
<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/figure08.jpg" alt="故障情形下的吞吐量" title="故障情形下的吞吐量"></p>
<div style="text-align: center;"><b>图8</b>：故障情形下的吞吐量。</div>

<p>为了显示系统随着时间的推移在故障注入时的行为，我们运行了一个由5台机器组成的ZooKeeper服务。我们运行了与之前相同的饱和基准，但这次我们将写入比例保持在30%，这是我们预期工作负载的一个保守比例。我们定期地杀死一些服务器进程。图8显示了系统的吞吐量随时间的变化情况。图中标出的事件有以下几点：</p>
<ol>
<li>一个追随者的故障和恢复；</li>
<li>一个不同的追随者的故障和恢复；</li>
<li>领导者的故障；</li>
<li>两个追随者（a，b）在前两个标记中故障，在第三个标记中恢复（c）；</li>
<li>领导者的失败；</li>
<li>领导者的恢复；</li>
</ol>
<p>从这张图中有几个重要的观察。首先，如果追随者故障并迅速恢复，那么ZooKeeper能够在故障的情况下保持高吞吐量。一个追随者的故障并不妨碍服务器形成一个法定人数，而且只减少了服务器在故障前所处理的读取请求的份额，所以吞吐量大致如此。其次，我们的领导者选举算法能够快速恢复，足以防止吞吐量大幅下降。根据我们的观察，ZooKeeper选举一个新的领导者需要不到200ms。因此，尽管服务器在几分之一秒内停止提供请求，但由于我们的采样周期为秒级，我们没有观察到吞吐量为零的情况。第三，即使追随者需要更多的时间来恢复，一旦他们开始处理请求，ZooKeeper也能够再次提高吞吐量。在事件1、2和4之后，我们没有恢复到完整的吞吐量水平，原因之一是客户端只有在与追随者的连接中断时才会切换追随者。因此，在事件4之后，客户端没有重新分配自己，直到事件3和5的领导者失败。在实践中，这种不平衡随着时间的推移，随着客户端的到来和离开而自行解决。</p>
<h3 id="5-2-请求延迟"><a href="#5-2-请求延迟" class="headerlink" title="5.2 请求延迟"></a>5.2 请求延迟</h3><p>为了评估请求的延迟，我们创建了一个以Chubby基准[6]为模型的基准测试。我们创建了一个工作进程，简单地发送一个创建，等待它完成，发送一个新节点的异步删除，然后开始下一个创建。我们相应地改变工人的数量，在每次运行中，我们让每个工人创建50,000个节点。我们通过将完成的创建请求数除以所有工作者完成的总时间来计算吞吐量。</p>
<p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/table02.jpg" alt="每秒创建请求处理" title="每秒创建请求处理"></p>
<div style="text-align: center;"><b>表2</b>：每秒创建请求处理。</div>

<p>表2显示了我们基准测试的结果。创建请求包括1K的数据，而不是Chubby基准中的5字节，以更好地与我们的预期使用相吻合。即使有这些较大的请求，ZooKeeper的吞吐量也比Chubby公布的吞吐量高3倍多。单个ZooKeeper工作者基准的吞吐量表明，3台服务器的平均请求延迟为1.2ms，9台服务器为1.4ms。</p>
<h3 id="5-3-屏障的性能"><a href="#5-3-屏障的性能" class="headerlink" title="5.3 屏障的性能"></a>5.3 屏障的性能</h3><p><img src="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/table03.jpg" alt="秒级屏障实验" title="秒级屏障实验"></p>
<div style="text-align: center;"><b>表3</b>：秒级屏障实验。每个点都是客户端运行5次后的平均值。</div>

<p>在该实验中，我们按顺序执行一些障碍，以评估用ZooKeeper实现的原语的性能。对于给定的障碍数量b，每个客户端首先进入所有b个障碍，然后连续离开所有b个障碍。由于我们使用了第2.4节的双壁垒算法，一个客户端首先等待所有其他客户端执行enter()程序，然后再进入下一个调用（与leave()类似）。</p>
<p>我们在表3中报告了我们的实验结果。在该实验中，我们有50、100和200个客户端连续进入一定数量的障碍，b∈{200、400、800、1600}。尽管一个应用程序可以有成千上万的ZooKeeper客户端，但相当多的时候，只有一个小得多的子集参与每个协调操作，因为客户端通常是根据应用程序的具体情况分组的。</p>
<p>从这个实验中可以看到两个有趣的现象：处理所有障碍的时间与障碍的数量大致呈线性增长，这表明对数据树的同一部分的并发访问并没有产生任何意外的延迟，而延迟的增加与客户端的数量成比例。这是不使ZooKeeper服务饱和的结果。事实上，我们观察到，即使客户端以锁步方式进行，在所有情况下，障碍操作（进入和离开）的吞吐量都在每秒1,950到3,100次之间。在ZooKeeper操作中，这相当于每秒10,700到17,000个操作的吞吐量值。由于在我们的实现中，我们的读与写的比例为4:1（80%的读操作），我们的基准代码使用的吞吐量与ZooKeeper可以达到的原始吞吐量（根据图5，超过40,000）相比要低很多。这是由于客户端在等待其他客户端。</p>
<h2 id="六、-相关工作"><a href="#六、-相关工作" class="headerlink" title="六、   相关工作"></a>六、   相关工作</h2><p>ZooKeeper的目标是提供一种服务，缓解分布式应用中协调进程的问题。为了实现这个目标，它的设计采用了以前的协调服务、容错系统、分布式算法和文件系统的思想。</p>
<p>我们并不是第一个提出分布式应用协调系统的人。一些早期的系统提出了用于事务性应用的分布式锁服务[13]，以及用于计算机集群中的信息共享[19]。最近，Chubby提出了一个为分布式应用管理咨询锁的系统[6]。Chubby分享了ZooKeeper的几个目标。它也有一个类似文件系统的接口，并且它使用一个协议来保证副本的一致性。然而，ZooKeeper不是一个锁服务。它可以被客户端用来实现锁，但它的API中没有锁操作。与Chubby不同，ZooKeeper允许客户端连接到任何ZooKeeper服务器，而不仅仅是领导者。ZooKeeper客户端可以使用他们的本地副本来提供数据和管理观察器，因为它的一致性模型要比Chubby宽松得多。这使得ZooKeeper能够提供比Chubby更高的性能，使应用程序能够更广泛地使用ZooKeeper。</p>
<p>文献中已经提出了一些容错系统，目的是为了减轻建立容错的分布式应用的问题。一个早期的系统是ISIS[5]。ISIS系统将抽象的类型规范转化为容错的分布式对象，从而使容错机制对用户透明。Horus[30]和Ensemble[31]是由ISIS发展而来的系统。ZooKeeper接纳了ISIS的虚拟同步概念。最后，Totem在一个利用局域网硬件广播的架构中保证了消息传递的总顺序[22]。ZooKeeper在各种网络拓扑结构下工作，这促使我们依靠服务器进程之间的TCP连接，而不假设任何特殊的拓扑结构或硬件特征。我们也没有公开ZooKeeper内部使用的任何集合通信。</p>
<p>构建容错服务的一个重要技术是状态机复制[26]，而Paxos[20]是一种算法，能够有效实现异步系统的状态机复制。我们使用的算法具有Paxos的一些特点，但它将共识所需的交易日志与数据树恢复所需的写前日志结合起来，以实现高效的实施。已经有一些关于实际实现拜占庭容忍复制状态机的协议建议[7, 10, 18, 1, 28]。ZooKeeper并不假定服务器可以是拜占庭的，但我们确实采用了诸如校验和和理智检查等机制来捕捉非恶意的拜占庭故障。Clement等人讨论了一种使ZooKeeper完全拜占庭式容错的方法，而无需修改当前的服务器代码库[9]。到目前为止，我们还没有在生产中观察到使用完全拜占庭容错协议可以防止的故障。[29].</p>
<p>Boxwood[21]是一个使用分布式锁服务器的系统。Boxwood为应用程序提供了更高层次的抽象，它依赖于一个基于Paxos的分布式锁服务。与Boxwood一样，ZooKeeper也是一个用于构建分布式系统的组件。然而，ZooKeeper有高性能的要求，并且更广泛地用于客户端应用程序。ZooKeeper暴露了低级别的基元，应用程序用它来实现高级别的基元。</p>
<p>ZooKeeper类似于一个小型的文件系统，但它只提供了文件系统操作的一个小子集，并增加了大多数文件系统不具备的功能，如排序保证和条件写入。然而，ZooKeeper监视在精神上与AFS[16]的缓存回调相似。</p>
<p>Sinfonia[2]引入了迷你交易，这是一种构建可扩展分布式系统的新模式。Sinfonia被设计用来存储应用数据，而ZooKeeper则存储应用元数据。ZooKeeper将其状态完全复制并保存在内存中，以获得高性能和一致的延迟。我们使用类似文件系统的操作和排序，实现了类似于小型交易的功能。znode是一个方便的抽象，我们在此基础上添加了观察器，这是Sinfonia中缺少的功能。Dynamo[11]允许客户在一个分布式键值存储中获取和放置相对较小（小于1M）的数据量。与ZooKeeper不同，Dynamo的键空间不是分层的。Dynamo也没有为写入提供强大的耐久性和一致性保证，而是在读取时解决冲突。</p>
<p>DepSpace[4]使用元组空间来提供拜占庭式的容错服务。像ZooKeeper一样，DepSpace使用一个简单的服务器接口，在客户端实现强大的同步基元。虽然DepSpace的性能比ZooKeeper低得多，但它提供了更强的容错和保密性保证。</p>
<h2 id="七、-结论"><a href="#七、-结论" class="headerlink" title="七、   结论"></a>七、   结论</h2><p>ZooKeeper采用了一种无等待的方法来解决分布式系统中协调进程的问题，它向客户公开无等待的对象。我们发现ZooKeeper对雅虎内部和外部的一些应用很有用。ZooKeeper通过使用快速读取和观察，实现了每秒数十万次的操作，这些操作都是由本地副本提供的，从而实现了以读为主的工作负载。虽然我们对读和监视的一致性保证似乎很弱，但我们的用例表明，这种组合允许我们在客户端实现高效和复杂的协调协议，即使读不是按优先级排序的，数据对象的实现是无等待的。事实证明，无等待的特性对高性能是至关重要的。</p>
<p>虽然我们只描述了几个应用，但还有很多其他的应用在使用ZooKeeper。我们相信这样的成功是由于其简单的接口以及人们可以通过这个接口实现的强大的抽象功能。此外，由于ZooKeeper的高吞吐量，应用程序可以广泛地使用它，不仅仅是过程粒度的锁。</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://tinykopano.github.io/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/" data-id="clfow1dd70009gjoqa7yp5cht" data-title="ZooKeeper: Wait-free coordination for Internet-scale systems" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/distributed-system/" rel="tag">distributed system</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/zookeeper/" rel="tag">zookeeper</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2022/11/30/In-Search-of-an-Understandable-Consensus-Algorithm/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">In Search of an Understandable Consensus Algorithm (Extended Version)</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Categories</h3>
    <div class="widget">
      <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/Course/">Course</a><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link" href="/categories/Course/Document/">Document</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Course/Paper/">Paper</a></li></ul></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/Raft/" rel="tag">Raft</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/big-data/" rel="tag">big data</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/consensus-algorithm/" rel="tag">consensus algorithm</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/distributed-system/" rel="tag">distributed system</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/go/" rel="tag">go</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/programming-language/" rel="tag">programming language</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/virtual-machine/" rel="tag">virtual machine</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/zookeeper/" rel="tag">zookeeper</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/Raft/" style="font-size: 10px;">Raft</a> <a href="/tags/big-data/" style="font-size: 15px;">big data</a> <a href="/tags/consensus-algorithm/" style="font-size: 10px;">consensus algorithm</a> <a href="/tags/distributed-system/" style="font-size: 20px;">distributed system</a> <a href="/tags/go/" style="font-size: 10px;">go</a> <a href="/tags/programming-language/" style="font-size: 10px;">programming language</a> <a href="/tags/virtual-machine/" style="font-size: 10px;">virtual machine</a> <a href="/tags/zookeeper/" style="font-size: 10px;">zookeeper</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2023/01/">January 2023</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/12/">December 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/11/">November 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/10/">October 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/09/">September 2022</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2022/08/">August 2022</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/">ZooKeeper: Wait-free coordination for Internet-scale systems</a>
          </li>
        
          <li>
            <a href="/2022/11/30/In-Search-of-an-Understandable-Consensus-Algorithm/">In Search of an Understandable Consensus Algorithm (Extended Version)</a>
          </li>
        
          <li>
            <a href="/2022/10/31/The-GO-Memory-Model/">The Go Memory Model</a>
          </li>
        
          <li>
            <a href="/2022/09/30/The-Design-of-a-Practical-System-for-Fault-Tolerant-Virtual-Machines/">The Design for a Practical System for Fault-Tolerant Virtual Machines</a>
          </li>
        
          <li>
            <a href="/2022/08/31/The-Google-File-System/">The Google File System</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2023 KoPaNo<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>