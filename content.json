{"posts":[{"title":"A simple totally ordered broadcast protocol","text":"一种简单的完全有序的广播协议摘要这是一个关于ZooKeeper使用的完全有序的广播协议的简短概述，称为Zab。它在概念上很容易理解，容易实现，而且性能高。在本文中，我们介绍了ZooKeeper对Zab的要求，展示了该协议的使用方法，并概述了该协议的工作方式。 一、 引言在雅虎，我们开发了一个高性能的高可用协调服务，称为ZooKeeper[9]，允许大规模的应用程序执行协调任务，如领导者选举、状态传播和会合。这个服务实现了一个数据节点的分层空间，称为znodes，客户使用它来实现他们的协调任务。我们发现这项服务很灵活，其性能很容易满足我们在雅虎的网络规模、关键任务应用程序的生产需求。ZooKeeper放弃了锁，而是实现了无等待的共享数据对象，对这些对象的操作顺序有很强的保证。客户端库利用这些保证来实现他们的协调任务。一般来说，ZooKeeper的主要前提之一是更新顺序对应用程序来说比其他典型的协调技术（如阻塞）更重要。 嵌入ZooKeeper的是一个完全有序的广播协议： Zab。在实现我们的客户端保证时，有序的广播是至关重要的；在每个ZooKeeper服务器上维护ZooKeeper状态的副本也是必要的。这些副本使用我们的完全有序的广播协议保持一致，如复制的状态机[13]。本文主要介绍ZooKeeper对这个广播协议的要求以及对其实现的概述。 一个ZooKeeper服务通常由三到七台机器组成。我们的实现支持更多的机器，但三到七台机器提供了足够的性能和恢复力。客户端连接到任何一台提供服务的机器上，并始终对ZooKeeper的状态有一个一致的看法。该服务最多可以容忍f个崩溃故障，它至少需要2f+1个服务器。 应用程序广泛地使用ZooKeeper，并且有几十到几千个客户端同时访问它，所以我们需要高吞吐量。我们将ZooKeeper设计为读写操作比例高于2:1的工作负载；但是，我们发现ZooKeeper的高写吞吐量也允许它用于一些写为主的工作负载。ZooKeeper通过为每个服务器上的ZooKeeper状态的本地副本的读取提供服务来提供高的读取吞吐量。因此，容错性和读取吞吐量都可以通过增加服务器来扩展服务。写入吞吐量不会因为增加服务器而扩大；相反，它受限于广播协议的吞吐量，因此我们需要一个高吞吐量的广播协议。 图1：Zookeeper服务的逻辑组件。 图1显示了ZooKeeper服务的逻辑构成。读取请求是由包含ZooKeeper状态的本地数据库提供服务。写入请求从ZooKeeper请求转化为empotent事务，并在生成响应之前通过Zab发送。许多ZooKeeper的写请求在本质上是有条件的：一个znode只有在没有任何子节点的情况下才能被删除；一个znode在创建时可以附加一个名字和一个序列号；对数据的改变只有在它处于预期的版本时才会被应用。即使是无条件的写请求，也会以不等价的方式修改元数据，如版本号。 通过将所有的更新通过一台服务器（被称为领导者）发送，我们将非同位素的请求转化为同位素的事务。在本文中，我们使用术语事务来表示请求的空当性版本。领导可以进行转换，因为它对复制的数据库的未来状态有一个完美的看法，可以计算出新记录的状态。idempotent事务是这个新状态的记录。ZooKeeper在很多方面都利用了同位素事务，这不在本文的讨论范围之内，但是我们的事务的同位素性质也允许我们在恢复期间放松我们的广播协议的排序要求 二、 要求三、 为什么需要另一种协议","link":"/2023/01/31/A-simple-totally-ordered-broadcast-protocol/"},{"title":"In Search of an Understandable Consensus Algorithm (Extended Version)","text":"寻找一种可理解的共识算法摘要Raft是一种共识算法，用于管理复制的日志。它产生的结果等同于（多）Paxos，并且它和Paxos一样高效，但它的结构与Paxos不同；这使得Raft比Paxos更容易理解，同时也为构建实用系统提供了更好的基础。为了提高可理解性，Raft将共识的关键元素分开，如leader、日志复制和安全等关键要素，并执行了更强的一致性，以减少必须考虑的状态数量。一个用户研究的结果表明，Raft比Paxos更容易被学生掌握。Raft还包括一个新的机制来改变群集成员，它使用重叠多数来保证安全。 一、 引言共识算法允许一组机器作为一致的团体工作，可以在一些成员的故障中幸存下来。正因为如此，它们在构建可靠的大规模软件系统中发挥了关键作用。Paxos[15, 16]在过去十年中主导了关于共识算法的讨论：大多数共识的实现都是基于Paxos或受其影响，Paxos已经成为教学生了解共识的主要工具。 不幸的是，Paxos是相当难以理解的，尽管有许多尝试使它更容易理解。此外，它的结构需要复杂的变化以支持实际的系统。因此，无论是系统构建者和学生都在为Paxos而努力。 在我们为Paxos努力之后，我们开始寻找一种新的共识算法，为系统建设和教育提供一个更好的基础。我们的方法与普通方法不同，因为我们的主要目标是可理解性：我们能否为实用系统定义一个共识算法，并以一种明显比Paxos更容易学习的方式来描述它？此外，我们希望该算法能够促进直觉的发展，这对系统建设者来说是至关重要的。重要的是，不仅要让算法发挥作用，而且要让它显而易见地发挥作用。 这项工作的结果是一种叫做Raft的共识算法。在设计Raft的过程中，我们应用了特定的技术来提高可理解性，包括分解（Raft将leader选举、日志复制和安全分开）和减少状态空间（相对于Paxos，Raft减少了非确定性的程度和服务器之间的不一致方式）。对两所大学的43名学生进行的用户研究表明，Raft明显比Paxos更容易理解：在学习了两种算法之后，这些学生中有33人能够更好地回答有关Raft的问题，而不是有关Paxos的问题。 Raft在许多方面与现有的共识算法相似（最显著的是Oki和Liskov的Viewstamped Replication[29, 22]），但它有几个新的特点。 强leader：与其他共识算法相比，Raft使用了一种更强的领导力形式。比如说。日志条目只从leader流向其他服务器。 这简化了对复制日志的管理 并使Raft更容易理解。 leader选举：Raft使用随机的计时器来选举leader。这在任何共识算法已经需要的心跳上只增加了少量的机制，同时简单而迅速地解决了冲突。 成员变更：Raft改变集群中的服务器集的机制使用了一种新的联合共识方法，在这种方法中，两个不同配置的多数在过渡期间重叠在一起。这使得集群在配置变化期间能够继续正常运行。 我们相信Raft优于Paxos和其他共识算法，无论是出于教育目的还是作为实施的基础。它比其他算法更简单，更容易理解；它的描述很完整，足以满足实际系统的需要。它有几个开源的实现，并被几个公司使用；它的安全属性已被正式规定和证明；它的效率可与其他算法相媲美。 本文的其余部分介绍了复制状态机问题（第2节），讨论了Paxos的优点和缺点（第3节），描述了我们对可理解性的一般方法（第4节），提出了Raft共识算法（第5-8节），评估了 Raft（第9节），并讨论了相关工作（第10节）。 二、 复制状态机一致性算法是从复制状态机[37]的背景下提出的。在这种方法中，一组服务器上的状态机产生相同状态的副本，并且在一些机器宕掉的情况下也可以继续运行。复制状态机在分布式系统中被用于解决很多容错的问题。例如，大规模的系统中通常都有一个集群leader，像 GFS、HDFS 和 RAMCloud，典型应用就是一个独立的的复制状态机去管理leader选举和存储配置信息并且在leader宕机的情况下也要存活下来。比如 Chubby 和 ZooKeeper。 图1：复制状态机的架构。一致性算法管理着来自客户端指令的复制日志。状态机从日志中处理相同顺序的相同指令，所以产生的结果也是相同的。 复制状态机通常都是基于复制日志实现的，如图 1。每一个服务器存储一个包含一系列指令的日志，并且按照日志的顺序进行执行。每一个日志都按照相同的顺序包含相同的指令，所以每一个服务器都执行相同的指令序列。因为每个状态机都是确定的，每一次执行操作都产生相同的状态和同样的序列。 保证复制日志相同就是一致性算法的工作了。在一台服务器上，一致性模块接收客户端发送来的指令然后增加到自己的日志中去。它和其他服务器上的一致性模块进行通信来保证每一个服务器上的日志最终都以相同的顺序包含相同的请求，尽管有些服务器会宕机。一旦指令被正确的复制，每一个服务器的状态机按照日志顺序处理他们，然后输出结果被返回给客户端。因此，服务器集群看起来形成一个高可靠的状态机。 实际系统中使用的一致性算法通常含有以下特性： 在非拜占庭错误情况下，包括网络延迟、分区、丢包、冗余和乱序等错误都可以保证安全性（绝对不会返回一个错误的结果） 。 只要任何大多数的服务器都在运行，并能相互之间以及与客户进行通信，它们就能完全发挥作用（_可用_）。因此，一个典型的由五个服务器组成的集群可以容忍任何两个服务器的故障。服务器被停止就认为是失败；它们后来可能从稳定存储的状态中恢复并重新加入集群。 不依赖时序来保证一致性：物理时钟错误或者极端的消息延迟只有在最坏情况下才会导致可用性问题。 通常情况下，一条指令可以尽可能快的在集群中大多数节点响应一轮远程过程调用时完成；小部分比较慢的节点不会影响系统整体的性能。 三、 Paxos有哪些问题在过去的十年里，Leslie Lamport的Paxos协议[15]几乎成了共识的代名词：它是课程中最常教授的协议，大多数共识的实现都以它为起点。Paxos首先定义了一个能够在单一决策上达成协议的协议，例如单一复制的日志条目。我们把这个子集称为single-decree Paxos。然后，Paxos结合这个协议的多个实例，以促进一系列的决定，如日志（多Paxos）。Paxos既保证了安全性，又保证了有效性，而且它支持集群成员的变化。它的正确性已被证明，而且在正常情况下是有效的。 不幸的是，Paxos有两个显著的缺点。第一个缺点是Paxos特别难理解。完整的解释[15]是出了名的不透明；而且在付出巨大努力之后，也很少有人能成功地理解它。因此，已经有一些人试图用更简单的术语来解释Paxos[16, 20, 21]。这些解释集中在single-decree子集上，但它们仍然具有挑战性。在对NSDI 2012的与会者进行的非正式调查中，我们发现很少有人对Paxos感到满意，即使在经验丰富的研究人员中。我们自己也在为Paxos挣扎；直到读了几个简化的解释和设计我们自己的替代协议后，我们才能够理解完整的协议，这个过程几乎花了一年时间。 我们假设，Paxos的不透明性来自于它选择single-decree子集作为基础。Single-decree Paxos是密集而微妙的：它分为两个阶段，没有简单的直观解释，不能被独立理解。正因为如此，很难发展出关于single-decree协议为何有效的直觉。Multi-Paxos的组成规则大大增加了复杂性和微妙性。我们认为，就多个决定达成共识的整体问题（即一个日志而不是一个条目）可以用其他更直接和明显的方式进行分解。 Paxos的第二个问题是，它没有为建立实际的实现提供一个良好的基础。原因之一是没有广泛认同的multi-Paxos的算法。Lamport的描述主要是关于single-decree Paxos的；他勾画了multi-Paxos的可能方法，但缺少许多细节。已经有一些尝试来充实和优化Paxos，如[26]、[39]和[13]，但这些尝试都不尽相同。彼此之间以及与Lamport的草图不同。诸如Chubby[4]等系统已经实现了类似Paxos的算法，但在大多数情况下，它们的细节还没有被公布。 此外，Paxos架构对于构建实用系统来说是一个糟糕的架构；这也是single-decree分解的另一个后果。例如，独立地选择一个日志条目集合，然后将它们拼接成一个连续的日志，这没有什么好处；这只会增加复杂性。围绕日志设计一个系统更简单、更有效，新的条目是以受限的顺序依次追加的。另一个问题是，Paxos的核心是使用对称的点对点方法（尽管它最终建议采用弱的leader形式作为性能优化）。这在一个只做一个决定的简化世界中是有意义的，但很少有实际的系统使用这种方法。如果必须做出一系列的决定，首先选举一个leader，然后让leader协调这些决定，这样做更简单，也更快。 因此，实际的系统与Paxos没有什么相似之处。每一个实现都是从Paxos开始的，发现实现它的困难，然后开发一个明显不同的架构。这既费时又容易出错，而理解Paxos的困难又加剧了这个问题。Paxos的表述对于证明其正确性的定理来说可能是一个很好的表述，但是真正的实现与Paxos有很大的不同，所以证明的价值不大。以下是一个典型的评论，来自于Chubby实现者: 在Paxos算法的描述和真实世界系统的需求之间存在着巨大的差距······最终的系统将基于一个未证明的协议[4]。 由于这些问题，我们得出结论，Paxos既不能为系统建设也不能为教育提供一个良好的基础。考虑到共识在大规模软件系统中的重要性，我们决定看看我们是否能设计出一种比Paxos具有更好特性的替代性共识算法。Raft就是这个实验的结果。 四、 为可理解性设计我们在设计Raft时有几个目标：它必须为系统建设提供一个完整而实用的基础，从而大大减少开发人员所需的设计工作量；它必须在所有条件下都是安全的，并且在典型的操作条件下可用；它必须对普通操作有效。但我们最重要的目标和最困难的挑战是可理解性。必须让广大受众能够舒适地理解该算法。此外，还必须有可能发展关于该算法的直觉，以便系统构建者能够进行扩展，而这在现实世界的实现中是不可避免的。 在Raft的设计中，有很多地方我们必须在备选方法中做出选择。在这些情况下，我们根据可理解性对备选方案进行评估：解释每个备选方案有多难？(例如，它的状态空间有多复杂，它是否有微妙的影响？），读者完全理解该方法及其影响有多容易？ 我们认识到，这种分析有很大程度的主观性；然而，我们使用了两种普遍适用的技术。第一种技术是众所周知的问题分解方法：在可能的情况下，我们把问题分成独立的部分，可以相对独立地解决、解释和理解。例如，在Raft中，我们将leader选举、日志复制、安全和成员变更分开。 我们的第二个方法是通过减少需要考虑的状态数量来简化状态空间，使系统更加连贯，并尽可能消除非确定性。具体来说，不允许日志有漏洞，而且Raft限制了日志相互之间不一致的方式。尽管在大多数情况下，我们试图消除非确定性，但在某些情况下，非确定性实际上提高了可理解性。特别是，随机化的方法引入了非确定性，但它们倾向于通过以类似的方式处理所有可能的选择来减少状态空间（”choose any; it doesn’t matter”）。我们使用随机化来简化Raft leader选举算法。 五、 Raft共识算法Raft是一种用于管理第2节所述形式的复制日志的算法。图2概括了该算法的浓缩形式以供参考，图3列出了该算法的关键属性；这些图中的内容将在本节的其余部分逐一讨论。 图2：Raft共识算法的简明摘要（不包括成员变化和日志压缩）。左上角方框中的server行为被描述为一组独立和重复触发的规则。其它小节，如§5.2表示讨论特定功能的地方。一个正式的规范[31]更精确地描述了该算法。 Raft通过首先选举一个特等的leader来实现共识，然后让leader完全负责管理复制的日志。leader接受来自客户端的日志条目，将其复制到其他服务器上，并告诉服务器何时可以将日志条目应用到他们的状态机上。这种一个leader的模式可以简化对复制日志的管理。例如，leader可以决定在日志中放置新条目的位置，而不需要咨询其他服务器，并且数据以一种简单的方式从leader流向其他服务器。一个leader可能会失败或与其他服务器断开连接，在这种情况下，会选出一个新的leader。 考虑到leader的选举方法，Raft将共识问题分解为三个相对独立的子问题，这将在下面的小节中讨论： leader选举：当现有的leader失败后，新的leader必须被选举产生（§5.2）。 日志复制：leader必须接收客户端的日志条目然后通过集群复制，强制其他服务器的日志和leader的一样（§5.3）。 安全性：Raft的关键安全属性是图3中的状态机安全属性：如果任何服务器对其状态机应用了一个特定的日志条目，那么其他服务器就不能对相同的日志索引应用不同的命令。第5.4节描述了Raft如何确保这一属性；该解决方案涉及对第5.2节中描述的选举机制的额外限制。 在介绍了共识算法之后，本节讨论了系统中的可用性问题和计时的作用。 图3：Raft保证这些属性总是为true。编号表示每个属性的讨论的章节。 5.1 Raft基础一个Raft集群包含多个服务器；五个是典型的数量，这使得系统可以容忍两个故障。在任何时候，每个服务器都处于三种状态之一：leader、follower或candidate。在正常操作中，有且仅有一个leader，其他所有的服务器都是follower。follower是被动的：他们自己不发出任何请求，只是回应leader和candidate的请求。leader处理所有客户端的请求（如果客户端联系follower，follower将其重定向到leader）。第三种状态，candidate，被用来选举一个新的leader，如第5.2节所述。图4显示了这些状态和它们的转换；下面将讨论这些转换。 图4：Server状态。Follower只对来自其他服务器的请求作出回应。如果一个follower没有收到通信。它就会成为candidate并发起选举。一个candidate收到整个集群中大多数人的投票，就会成为新的leader。leader通常会一直运行到他们失败为止。 图5：时间被划分为几个任期，每个任期以选举开始。选举成功后，由一位leader管理整个集群，直到任期结束。有些选举失败，在这种情况下，任期结束时没有选择leader。在不同的服务器上，可以在不同的时间观察到任期之间的转换。 如图5所示，Raft将时间划分为任意长度的任期。任期用连续的整数来编号。每个任期以选举开始，其中一个或多个candidate试图成为leader，如第5.2节所述。如果一个candidates 在选举中获胜，那么他将在剩下的任期内担任leader。在某些情况下，选举的结果为分裂选票。在这种情况下，任期将在没有leader的情况下结束；新的任期（重新选举）将很快开始。Raft确保在一个给定的任期内最多有一个leader。 不同的服务器可能会在不同的时间观察到任期转换，在某些情况下，一个服务器可能不会观察到一个选举甚至整个任期。任期在Raft中充当了逻辑时钟[14]，它们允许服务器检测过时的信息，如过时的leader。每个服务器都存储一个当前的任期编号，该编号随时间单调地增加。每当服务器进行通信时，就会交换当前任期；如果一个服务器的当前任期比另一个服务器的小，那么它就将其当前任期更新为较大的值。如果一个candidate或leader发现它的任期已经过时，它将立即恢复到follower状态。如果一个服务器收到的请求是一个过时的任期编号，它将拒绝该请求。 Raft服务器使用远程过程调用（RPCs）进行通信，基本的共识算法只需要两种类型的RPCs。RequestVote RPCs由candidate在选举期间发起（第5.2节），AppendEntries RPCs由leader发起，用于复制日志条目并提供一种心跳形式（第5.3节）。第7节增加了第三个RPC，用于在服务器之间传输快照。如果服务器没有及时收到响应，它们会重试RPC，并且为了获得最佳性能，它们会并行地发出RPC。 5.2 leader选举Raft使用心跳机制来触发leader选举。当服务器启动时，它们开始是follower。只要服务器收到来自leader或candidate的有效RPC，它就一直处于follower状态。leader定期向所有follower发送心跳（AppendEntries RPCs，不携带任何日志条目），以保持他们的权威。如果follower在一段被称为选举超时的时间内没有收到任何通信，那么它就认为没有可行的leader，并开始选举以选择一个新的leader。 为了开始选举，follower增加它的当前任期并过渡到candidate状态。然后，它为自己投票，并向集群中的每个其他服务器发出RequestVote RPCs。candidate一直处于这种状态，直到发生以下三种情况之一：（a）它赢得了选举，（b）另一个服务器确立了自己成为leader，或者（c）一段时间内没有赢家。这些结果将在下面的段落中分别讨论。 如果一个candidate在同一任期内获得了整个集群中大多数服务器的投票，那么它就赢得了选举。每台服务器在给定的任期内最多为一名candidate投票，以先来后到为原则（注：第5.4节对投票增加了一个额外的限制）。少数服从多数的原则保证了最多只有一名candidate能够在某一任期内赢得选举（图3中的选举安全属性）。一旦一个candidate在选举中获胜，它就成为leader。然后，它向所有其他服务器发送心跳信息，以建立其权威并防止新的选举。 在等待投票的过程中，candidate可能会收到一个来自另一个服务器的 AppendEntries RPC，该RPC来自另一服务器，声称自己是leader。如果leader的任期（包括在其RPC中）至少与candidate的当前任期一样大，那么candidate就会就承认该leader是合法的，并返回到follower状态。如果RPC中的任期小于candidate的，那么candidate就会拒绝RPC，并继续处于candidate状态。 第三种可能的结果是candidate既没有赢得选举也没有输掉选举：如果许多follower同时成为candidate，票数可能被分割，因此没有candidate获得多数票。当这种情况发生时，每个candidate都会超时，并通过增加其任期和启动新一轮的RequestVote RPC来开始新的选举。然而，如果没有额外的措施，分裂选票可能会无限期地重复。 Raft使用随机的选举超时，以确保分裂投票很少发生，并能迅速解决。为了从一开始就防止分裂投票，选举超时是从一个固定的时间间隔中随机选择的（例如150-300ms）。这就分散了服务器，所以在大多数情况下，只有一个服务器会超时；它赢得了选举，并在任何其他服务器超时之前发送心跳信号。同样的机制被用来处理分裂投票。每个候选人在选举开始时重新启动其随机选举超时，并等待超时过后再开始下一次选举；这减少了在新的选举中再次出现分裂票的可能性。第9.3节显示，这种方法可以迅速选出一个leader。 选举是说明可理解性如何指导我们在设计方案之间进行选择的一个例子。最初我们计划使用一个排名系统：每个candidate被分配一个独特的排名，用来在竞争的candidate之间进行选择。如果一个candidate发现了另一个排名更高的candidate，它就会回到follower状态，这样排名更高的candidate就能更容易地赢得下一次选举。我们发现这种方法在可用性方面产生了一些微妙的问题（如果一个排名较高的服务器失败了，一个排名较低的服务器可能需要超时并再次成为candidate，但如果它过早地这样做，它可能会重置选举leader的进展）。我们对算法进行了多次调整，但每次调整后都会出现新的corner case。最终我们得出结论，随机重试的方法更加明显和容易理解。 5.3 日志复制一旦一个leader被选出，它就开始为客户端请求提供服务。每个客户请求都包含一个要由复制的状态机执行的命令。leader将命令作为一个新的条目附加到它的日志中，然后向其他每个服务器并行地发出AppendEntries RPCs来复制该条目。当条目被安全复制后（如下所述），leader将条目应用于其状态机，并将执行结果返回给客户端。如果follower崩溃或运行缓慢，或者网络数据包丢失，leader会无限期地重试AppendEntries RPCs（甚至在它回应了客户端之后），直到所有follower最终存储所有日志条目。 图6：日志是由条目组成的，这些条目按顺序编号。每个条目都包含创建它的任期（每个框中的数字）和状态机的命令。如果一个条目可以安全地应用于状态机，那么该条目就被认为已经提交。 日志的组织方式如图6所示。每个日志条目都存储了一个状态机命令，以及leader收到该条目时的任期编号。日志条目中的任期编号被用来检测日志之间的不一致，并确保图3中的一些属性。每个日志条目都有一个整数的索引，用来标识它在日志中的位置。 leader决定何时将日志条目应用于状态机是安全的；这样的条目被称为已提交。Raft保证已提交条目是持久化的，并且最终会被所有可用的状态机执行。一旦创建该条目的leader将其复制到大多数服务器上，该日志条目就会被提交（例如，图6中的条目7）。这也会提交leader日志中所有之前的条目，包括之前leader创建的条目。第5.4节讨论了在leader变更后应用这一规则时的一些微妙之处，它还表明这种提交的定义是安全的。leader会跟踪它所知道的已提交的最高索引，并且它在未来的AppendEntries RPC（包括心跳）中包括该索引，以便其他服务器最终发现。一旦follower得知一个日志条目被提交，它就会将该条目应用到它的本地状态机（按日志顺序）。 我们设计的Raft日志机制在不同服务器上的日志之间保持了高度的一致性。这不仅简化了系统的行为，使其更具可预测性，而且是确保安全的重要组成部分。Raft维护以下属性，它们共同构成了图3中的日志匹配属性： 如果不同日志中的两个条目具有相同的索引和任期，那么它们存储的是同一个命令。 如果不同日志中的两个条目具有相同的索引和任期，那么日志中的所有前面的条目都是相同的。 第一个属性来自于这样一个事实，即一个leader在一个给定的任期中最多创建一个具有给定日志索引的条目，并且日志条目永远不会改变它们在日志中的位置。第二个属性是由AppendEntries的一个简单一致性检查来保证的。当发送AppendEntries RPC时，leader包括其日志中紧接新条目之前的条目的索引和任期。如果follower在其日志中没有找到具有相同索引和任期的条目，那么它将拒绝新条目。一致性检查作为一个归纳步骤：日志的初始空状态满足了日志匹配属性，并且每当日志被扩展时，一致性检查都会保留日志匹配属性。因此，每当AppendEntries成功返回时，leader知道follow的日志与自己的日志在新条目之前是相同的。 在正常运行期间，leader和follower的日志保持一致，所以AppendEntries一致性检查从未失败。然而，leader崩溃会使日志不一致（旧leader可能没有完全复制其日志中的所有条目）。这些不一致会在一系列leader和follower的崩溃中加剧。图7说明了follower的日志可能与新leader的日志不同的方式。follower可能缺少leader的条目，可能有leader没有的额外条目，或者两者都有。日志中缺失和多余的条目可能跨越多个任期。 图7：当顶部的leader上位后，在follower的日志中可能会出现（a-f）中的任何一种情况。每个框代表一个日志条目；框里的数字是其任期。一个follower可能缺少条目（a-b），可能有额外的未提交的条目（c-d），或者两者都有（e-f）。例如，如果该服务器是第2期的leader，在其日志中增加了几个条目，然后在提交任何条目之前就崩溃了；它很快重新启动，成为第3期的leader，并在其日志中增加了几个条目；在第2期或第3期的任何条目被提交之前，该服务器又崩溃了，并在几个任期内一直处于停机状态。 在Raft中，leader通过强迫follower复制自己的日志来处理不一致的问题。follower的日志与自己的日志重复。这意味着follower日志中的冲突条目将被覆盖用leader日志中的条目覆盖。第5.4节将表明如果再加上一个限制，这就是安全的。 为了使follower的日志与自己的日志保持一致，leader必须找到两个日志一致的最新日志条目，删除follower日志中此后的任何条目，并向follower发送此后leader的所有条目。所有这些动作都是为了响应AppendEntries RPCs所进行的一致性检查而发生的。leader为每个follower维护一个nextIndex，它是leader将发送给该follower的下一个日志条目的索引。当leader第一次上台时，它将所有的nextIndex值初始化为其日志中最后一条的索引（图7中的11）。如果follower的日志与leader的日志不一致，在下一个AppendEntries RPC中，AppendEntries一致性检查将失败。在拒绝之后，leader会递减nextIndex并重试AppendEntries RPC。最终，nextIndex将达到一个点，即leader和follower的日志匹配。当这种情况发生时，AppendEntries将会成功，它将删除follower日志中任何冲突的条目，并从leader的日志中添加条目（如果有的话）。一旦AppendEntries成功，follower的日志就会与leader的日志一致，并且在剩下的时间里都会保持这种状态。 如果需要，该协议可以被优化以减少被拒绝的AppendEntries RPC的数量。例如，当拒绝一个AppendEntries请求时，follower可以包括冲突条目的任期和它为该任期存储的第一个索引。有了这些信息，leader可以递减nextIndex以绕过该任期中的所有冲突条目；每个有冲突条目的任期将需要一个AppendEntries RPC，而不是每个条目一个RPC。在实践中，我们怀疑这种优化是否有必要，因为故障不常发生，而且不太可能有很多不一致的条目。 有了这种机制，leader在上位后不需要采取任何特别的行动来恢复日志的一致性。它只是开始正常的操作，而日志会自动收敛以应对AppendEntries一致性检查的失败。一个leader永远不会覆盖或删除它自己日志中的条目（图3中leader的只可追加属性）。 这种日志复制机制表现出第2节中所描述的理想的共识属性：只要大多数服务器是正常的，Raft就可以接受、复制和应用新的日志条目；在正常情况下，一个新的条目可以通过单轮RPC复制到集群的大多数；而且一个缓慢的follower不会影响性能。 5.4 安全性前面的章节描述了Raft如何选举leader和复制日志条目。然而，到目前为止所描述的机制还不足以确保每个状态机以相同的顺序执行完全相同的命令。例如，当leader提交几个日志条目时，一个follower可能无法使用，然后它可能被选为leader，并用新的条目覆盖这些条目；结果，不同的状态机可能执行不同的命令序列。 本节通过增加对哪些服务器可以被选为leader的限制，完善了Raft算法。该限制确保任何给定任期的leader都包含了之前任期中提交的所有条目（图3中的leader完整性属性）。考虑到选举限制，我们会使提交的规则更加精确。最后，我们提出一个leader完整性属性的简略证明，并说明它如何保证复制状态机的正确行为。 5.4.1 选举约束在任何基于leader的共识算法中，leader必须最终存储所有已提交的日志条目。在一些共识算法，例如Viewstamped Replication [22]，即使leader没有最初包含所有已提交的条目。这些算法包含额外的机制来识别丢失的条目，并在选举过程中或之后不久将它们传送给新的leader。不幸的是，这导致了相当多的额外机制和复杂性。Raft使用了一种更简单的方法，即它保证所有先前提交的条目的所有条目都存在于每个新leader身上。其选举的那一刻起就存在于每个新leader身上，而不需要将这些条目转移到leader。这意味着日志条目只在一个方向流动，即从leader到follower，而且leader永远不会覆盖他们日志中的现有条目。 Raft使用投票程序来防止candidate赢得选举，除非其日志包含所有已提交的条目。candidate必须与集群中的大多数人联系才能当选，这意味着每个已提交的条目必须至少存在于其中一个服务器中。如果candidate的日志至少和该多数人中的任何其他日志一样是最新的（这里的 “最新 “在下面有精确的定义），那么它将包含所有已提交的条目。RequestVote RPC实现了这一限制：RPC包括有关candidate日志的信息，如果投票人自己的日志比candidate的日志更及时，则拒绝投票。 Raft通过比较日志中最后条目的索引和任期来确定两个日志中哪一个是最新的。如果日志的最后条目有不同的任期，那么任期较晚的日志是更最新的。如果日志以相同的任期结束，那么日志更长的就是最新的。 5.4.2 从前一个任期提交条目如第5.3节所述，一旦一个条目被存储在大多数服务器上，leader就知道其当前任期的条目被提交。如果leader在提交条目之前崩溃了，未来的leader将试图完成对该条目的复制。然而，尽管前一任期的条目被存储在大多数服务器上，leader仍然不能立即得出结论该条目已被提交。图8展示了这样一种情况：一个旧的日志条目被存储在大多数服务器上，但仍然可以被未来的leader所覆盖。 图8：一个时间序列显示了为什么leader不能使用旧任期的日志条目来确定提交。在（a）中，S1是leader，并部分复制了索引2的日志条目。在(b)中，S1崩溃了；S5凭借S3、S4和它自己的投票当选为第3任期的leader，并接受了日志索引2的不同条目。在（c）中，S5崩溃了；S1重新启动，被选为leader，并继续复制。在这一点上，第2项的日志条目已经在大多数服务器上复制，但它没有被提交。如果S1像(d)那样崩溃，S5可以被选为leader（有S2、S3和S4的投票），并用它自己的第3任期的条目覆盖该条目。然而，如果S1在崩溃前在大多数服务器上复制了其当前任期的条目，如(e)，那么这个条目就被提交了（S5不能赢得选举）。在这一点上，日志中所有前面的条目也被提交。 为了消除类似图8中的问题，Raft从不通过计算副本来提交以前的日志条目。只有leader当前任期的日志条目是通过计算副本来提交的；一旦当前任期的条目以这种方式被提交，那么由于日志匹配特性，所有之前的条目都被间接地提交。在某些情况下，leader可以安全地断定一个较早的日志条目已被提交（例如，如果该条目存储在每个服务器上），但Raft为简单起见采取了更保守的方法。 Raft在提交规则中产生了这种额外的复杂性，因为当leader复制前几期的条目时，日志条目会保留其原来的任期编号。在其他共识算法中，如果一个新的leader复制之前 “任期 “的条目，它必须用新的 “任期编号 “来做。Raft的方法使得对日志条目的推理更加容易，因为它们在不同的时间和不同的日志中保持着相同的任期编号。此外，与其他算法相比，Raft的新leader从以前的任期中发送较少的日志条目（其他算法必须发送多余的日志条目，在它们被提交之前对其重新编号）。 5.4.3 安全性争论考虑到完整的Raft算法，我们现在可以更精确地论证leader完整性属性是否成立（这个论证是基于安全证明的，见第9.2节）。我们假设了leader完整性属性不成立，然后我们证明其矛盾。假设任期T的leader（leaderT）从其任期中提交了一个日志条目，但是这个日志条目没有被未来某个任期的leader所存储。考虑最小的任期U &gt; T，其leader（leaderU）不存储该条目。 图9：如果S1（任期T的leader）在其任期内提交了一个新的日志条目，而S5被选为后来任期U的leader，那么至少有一个服务器（S3）接受了这个日志条目，并且也投票给S5。 在leaderU当选时，已提交条目必须不在leaderU的日志中（leader从不删除或覆盖条目）。 leaderT在集群的大多数上复制了该条目，而leaderU从集群的大多数上收到了投票。因此，至少有一台服务器（”投票者”）既接受了leaderT的条目，又投票给leaderU，如图9所示。投票者是达成矛盾的关键。 投票者必须在投票给leaderU之前接受leaderT的已提交条目；否则它就会拒绝leaderT的AppendEntries请求（它当前的任期会比T高）。 投票人在投票给leaderU时仍然存储了该条目，因为每个介入的leader都包含该条目（根据假设），leader从不删除条目，而follower只有在与leader冲突时才会删除条目。 投票人将自己的投票权授予leaderU，所以leaderU的日志肯定和投票人的一样是最新的。这导致了两个矛盾中的一个矛盾。 首先，如果投票人和leaderU共享最后一个日志任期，那么leaderU的日志至少要和投票人的一样长，所以它的日志包含了投票人日志中的每一个条目。这是一个矛盾，因为投票者包含了已提交的条目，而leaderU被认为不包含。 否则，leaderU的最后一个日志任期一定比投票者的大。而且，它比T大，因为投票人的最后一个日志任期至少是T（它包含任期T的提交条目）。创建leaderU的最后一个日志任期的较早的leader，其日志中一定包含了已提交的条目（根据假设）。那么，根据日志匹配属性，leaderU的日志也必须包含已提交的条目，这就是一个矛盾。 这就完成了这个矛盾。因此，所有大于T的任期的leader必须包含任期T中所有在任期T中已提交的条目。 日志匹配属性保证了未来的leader也将包含间接提交的条目，如图8(d)中的索引2。 鉴于leader完备性属性，我们可以证明图3中的状态机安全属性，即如果一个服务器在其状态机上应用了一个给定索引的日志条目，那么没有其他服务器会在同一索引上应用一个不同的日志条目。当一个服务器在其状态机上应用一个日志条目时，其日志必须与leader的日志相同，直到该条目，并且该条目必须被提交。现在考虑任何服务器应用一个给定的日志索引的最低任期；日志完整性属性保证所有更高任期的leader将存储相同的日志条目，因此在以后的任期中应用索引的服务器将应用相同的值。因此，状态机安全属性成立。 最后，Raft要求服务器按照日志索引顺序应用条目。结合状态机安全属性，这意味着所有的服务器将以相同的顺序对其状态机应用完全相同的日志条目集。 5.5 Follower和candidate宕机在这之前，我们一直关注的是leader的失败。follower和candidate的崩溃比leader的崩溃要简单得多，而且它们的处理方式都是一样的。如果一个follower或candidate崩溃了，那么未来发送给它的RequestVote和AppendEntries RPC将会失败。Raft通过无限期地重试来处理这些失败；如果崩溃的服务器重新启动，那么RPC将成功完成。如果服务器在完成RPC后但在响应前崩溃，那么它将在重新启动后再次收到相同的RPC。Raft的RPC是幂等的，所以不会有危害。例如，如果一个follower收到一个AppendEntries请求，其中包括已经存在于其日志中的日志条目，它就会在新的请求中忽略这些条目。 5.6 计时和可用性我们对Raft的要求之一是安全不能依赖于计时：系统不能因为某些事件发生得比预期的快或慢而产生错误的结果。然而，可用性（系统及时响应客户的能力）必须不可避免地取决于时间。例如，如果消息交换的时间超过了服务器崩溃之间的典型时间，那么candidate就不会保持足够长的时间来赢得选举；没有一个稳定的leader，Raft就无法取得进展。 leader选举是Raft中计时最关键的部分。只要系统满足以下时间要求，Raft就能选出并维持一个稳定的leader： broadcastTime ≪ electionTimeout ≪ MTBF 在这个不等式中，broadcastTime是一台服务器向集群中的每台服务器并行发送RPC并接收其响应所需的平均时间；electionTimeout是第5.2节中描述的选举超时；MTBF是单台服务器的平均故障间隔时间。broadcast time应该比election timeout少一个数量级，这样leader就可以可靠地发送心跳信息，以防止follower开始选举；考虑到选举超时使用的随机方法，这种不平等也使得分裂投票不太可能发生。选举超时应该比MTBF小几个数量级，这样系统才能稳步前进。当leader崩溃时，系统将在大约选举超时的时间内不可用；我们希望这只占总体时间的一小部分。 broadcastTime和MTBF是底层系统的属性，而elelctionTimeout是我们必选的。 Raft的RPC通常需要接收者将信息持久化，因此broadcastTime可能从0.5毫秒到20毫秒不等，具体取决于存储技术。因此，electionTimeout可能在10毫秒至500毫秒之间。典型服务器的MTBF一般是几个月甚至更长，很容易就满足了计时的需求。 六、 集群成员变更到目前为止，我们一直假设集群配置（参与共识算法的服务器集合）是固定的。在实践中，偶尔有必要改变配置，例如在服务器故障时更换服务器或改变复制的程度。虽然这可以通过关闭整个集群，更新配置文件，然后重新启动集群来完成，但这将使集群在改变期间不可用。此外，如果有任何手动步骤，就有可能出现操作错误。为了避免这些问题，我们决定将配置变更自动化，并将其纳入Raft共识算法中。 图10：直接从一个配置切换到另一个配置是不安全的，因为不同的服务器会在不同时间切换。在这个例子中，集群从三个服务器增长到五个。不幸的是，有一个时间点，两个不同的leader可以在同一任期内当选，一个是旧配置的多数（Cold），另一个是新配置的多数（Cnew）。 为了使配置变更机制安全，在过渡期间必须没有任何一点可以让两个leader当选为同一任期。不幸的是，任何服务器直接从旧配置切换到新配置的方法都是不安全的。不可能一次性原子化地切换所有的服务器，所以集群在过渡期间有可能分裂成两个独立的多数（见图10）。 为了确保安全，配置变化必须使用两阶段的方法。实现这两个阶段的方法有很多种。例如，一些系统 (如[22]）使用第一阶段禁用旧的配置，使其不能处理客户端请求；然后第二阶段启用新的配置。在Raft中，集群首先切换到一个过渡性配置，我们称之为联合共识；一旦联合共识被提交，系统就会过渡到新的配置。联合共识结合了新旧两种配置： 日志条目被复制到两个配置中的所有服务器。 任何一个配置的服务器都可以作为leader。 达成协议（选举和进入承诺）需要新旧配置分别获得多数票。 联合共识允许单个服务器在不同时间在配置之间转换，而不影响安全。此外，联合共识允许集群在整个配置变化过程中继续为客户请求提供服务。 图11：配置变更的时间线。虚线表示已经创建但未提交的配置条目，实线表示最新提交的配置条目。leader首先在其日志中创建 Cold,new 配置条目，并将其提交给 Cold,new（Cold 的大多数和 Cnew 的大多数）。然后，它创建了Cnew条目，并将其提交给多数的Cnew。在这个时间点上，Cold和Cnew都不能独立做出决定。 集群配置使用复制日志中的特殊条目进行存储和通信；图11说明了配置变化过程。当leader收到将配置从Cold改成Cnew的请求时，它将联合共识的配置（图中的Cold,new）存储为一个日志条目，并使用前面描述的机制复制该条目。一旦某个服务器将新的配置条目添加到其日志中，它就会将该配置用于所有未来的决策（一个服务器总是使用其日志中的最新配置，无论该条目是否被提交）。这意味着leader将使用 Cold,new 的规则来决定 Cold,new 的日志条目何时被提交。如果leader崩溃了，一个新的leader可能会在Cold或Cold,new下被选择，这取决于获胜的候选人是否已经收到了Cold,new。在任何情况下，Cnew都不能在这段时间内做出单边决定。 一旦Cold,new被承诺，无论是Cold还是Cnew都不能在未经对方批准的情况下做出决定，而且leader完整性属性确保只有拥有Cold,new日志记录的服务器才能被选为leader。现在，leader创建描述Cnew的日志条目并将其复制到集群中是安全的。同样，这个配置一旦被看到，就会在每个服务器上生效。当新的配置在Cnew的规则下被提交后，旧的配置就不重要了，不在新配置中的服务器可以被关闭。如图11所示，没有任何时候Cold和Cnew可以同时做出单边决定；这保证了安全。 对于重新配置，还有三个问题需要解决。第一个问题是，新服务器最初可能不会存储任何日志条目。如果它们在这种状态下被添加到集群中，可能需要相当长的时间才能赶上，在此期间，可能无法提交新的日志条目。为了避免可用性差距，Raft在配置改变之前引入了一个额外的阶段，在这个阶段，新的服务器作为非投票成员加入集群（leader将日志条目复制给他们，但他们不被考虑为多数）。一旦新的服务器赶上了集群的其他部分、 重新配置就可以按上述方法进行。 第二个问题是，集群leader可能不是新配置的一部分。在这种情况下，一旦它提交了Cnew日志条目，leader就会下台（返回到follower状态）。这意味着会有一段时间（在它提交Cnew的时候），leader在管理一个不包括自己的集群；它复制日志条目，但不把自己算在多数中。leader过渡发生在Cnew被提交的时候，因为这是新配置可以独立运行的第一个点（它将始终有可能从Cnew中选择一个leader）。在这之前，可能只有Cold的一个服务器可以被选为leader。 第三个问题是，被移除的服务器（那些不在Cnew中的服务器）会扰乱集群。这些服务器不会收到心跳，所以它们会超时并开始新的选举。然后他们会发送带有新任期编号的RequestVote RPCs，这将导致当前的leader恢复到follower状态。一个新的leader最终将被选出，但被移除的服务器将再次超时，这个过程将重复，导致可用性差。 为了防止这个问题，当服务器认为存在一个当前的leader时，他们会忽略RequestVote RPC。具体来说，如果一个服务器在听到当前leader的最小选举超时内收到RequestVote RPC，它不会更新其任期或授予其投票。这并不影响正常的选举，每个服务器在开始选举之前至少要等待一个最小的选举超时。然而，这有助于避免被移除的服务器的干扰：如果一个leader能够得到其集群的心跳，那么它就不会被较大的任期数字所废黜。 七、 日志压缩Raft的日志在正常运行期间不断增长，以纳入更多的客户端请求，但在一个实际的系统中，它不能无限制地增长。随着日志的增长，它占据了更多的空间，需要更多的时间来重放。如果没有某种机制来丢弃积累在日志中的过时信息，这最终会导致可用性问题。 快照是最简单的压缩方法。在快照中，整个当前的系统状态被写入稳定存储的快照中，然后丢弃截至该点的整个日志。快照在Chubby和ZooKeeper中使用，本节的其余部分描述了Raft中的快照。 增量压缩的方法，如日志清理[36]和日志结构化合并树[30, 5]，也是可能的。这些方法一次性对部分数据进行操作，因此它们将压缩的负载更均匀地分散在时间上。它们首先选择一个已经积累了许多删除和覆盖对象的数据区域，然后将该区域的活对象重写得更紧凑，并释放该区域。与快照相比，这需要大量的额外机制和复杂性，因为快照总是对整个数据集进行操作，从而简化了问题。虽然日志清理需要对Raft进行修改，但状态机可以使用与快照相同的接口实现LSM树。 图12：一个服务器用一个新的快照替换其日志中已提交的条目（索引1到5），该快照只存储当前的状态（本例中的变量x和y）。快照最后包含的索引和术语用于定位快照在日志第6条之前的位置。 图12显示了Raft中快照的基本思路。每台服务器独立进行快照，只覆盖其日志中已提交的条目。大部分的工作包括状态机将其当前状态写入快照。Raft还在快照中包含了少量的元数据：最后包含的索引是快照所取代的日志中最后一个条目的索引（状态机应用的最后一个条目），最后包含的术语是这个条目的术语。这些被保留下来是为了支持快照后的第一个日志条目的AppendEntries一致性检查，因为该条目需要一个先前的日志索引和术语。为了实现集群成员的变化（第6节），快照还包括日志中的最新配置，即最后包含的索引。一旦服务器完成写入快照，它可以删除所有的日志条目，直到最后包含的索引，以及任何先前的快照。 尽管服务器通常是独立进行快照的，但leader偶尔也必须向落后的follower发送快照。这种情况发生在leader已经丢弃了它需要发送给follower的下一个日志条目。幸运的是，这种情况在正常操作中不太可能发生：一个跟上leader的follower已经有了这个条目。然而，一个特别慢的follower或一个新加入集群的服务器（第6节）就不会有这样的情况。让这样的follower跟上的方法是，leader通过网络向它发送一个快照。 图13：InstallSnapshot RPC的摘要。快照被分割成若干chunk进行传输；通过给follower提供各个chunk，leader以此表明其存活状态，因此follower可以重置其选举定时器。 leader使用一个叫做InstallSnapshot的新RPC来向落后于它的follower发送快照；见图13。当follower收到这个RPC的快照时，它必须决定如何处理其现有的日志条目。通常情况下，快照将包含新的信息，而不是在接收者的日志中。在这种情况下，follower会丢弃它的整个日志；它都被快照取代了，而且可能有与快照冲突的未提交的条目。相反，如果follower收到描述其日志前缀的快照（由于重传或错误），那么快照覆盖的日志条目被删除，但快照之后的条目仍然有效，必须保留。 这种快照方法背离了Raft的强leader原则，因为follower可以在leader不知情的情况下进行快照。然而，我们认为这种背离是合理的。虽然有一个leader有助于在达成共识时避免冲突的决定，但在快照时已经达成了共识，所以没有决定冲突。数据仍然只从leader流向follower，只是follower现在可以重新组织他们的数据。 我们考虑了另一种基于leader的方法，即只有leader会创建一个快照，然后它将这个快照发送给它的每个follower。然而，这有两个缺点。首先，向每个follower发送快照会浪费网络带宽，并减缓快照过程。每个follower都已经拥有产生自己快照所需的信息，而且对于服务器来说，从其本地状态产生一个快照通常要比在网络上发送和接收一个快照便宜很多。第二，leader的实现将更加复杂。例如，leader需要在向follower发送快照的同时，向他们回复新的日志条目，这样就不会阻碍新的客户请求。 还有两个影响快照性能的问题。首先，服务器必须决定何时进行快照。如果服务器快照的频率过高，就会浪费磁盘带宽和能源；如果快照的频率过低，就会有耗尽其存储容量的风险，而且会增加重启时重放日志的时间。一个简单的策略是，当日志达到一个固定的字节大小时进行快照。如果这个大小被设定为明显大于快照的预期大小，那么用于快照的磁盘带宽就会很小。 第二个性能问题是，写一个快照可能需要相当长的时间，我们不希望因此而延误正常的操作。解决方案是使用写时复制技术，这样就可以接受新的更新而不影响正在写入的快照。例如，用功能数据结构构建的状态机自然支持这一点。另外，可以使用操作系统的写时拷贝支持（例如Linux上的fork）来创建整个状态机的内存快照（我们的实现采用了这种方法）。 八、 客户端交互本节介绍了客户端如何与Raft交互，包括客户端如何找到集群leader以及Raft如何支持可线性化语义[10]。这些问题适用于所有基于共识的系统，而且Raft的解决方案与其他系统相似。 Raft的客户端将他们所有的请求发送给leader。当客户端第一次启动时，它连接到一个随机选择的服务器。如果客户端的第一选择不是leader，该服务器将拒绝客户端的请求，并提供它最近听到的leader的信息（AppendEntries请求包括leader的网络地址）。如果leader崩溃了，客户端的请求就会超时；然后客户端会在随机选择的服务器上再次尝试。 我们对Raft的目标是实现可线性化的语义（每个操作看起来都是瞬时执行的，在其调用和响应之间的某个点上正好执行一次）。然而，正如目前所描述的那样，Raft可以多次执行一个命令：例如，如果leader在提交日志条目后但在响应客户端之前崩溃，客户端将用一个新的leader重试该命令，导致它被第二次执行。解决方案是让客户端为每个命令分配唯一的序列号。然后，状态机跟踪为每个客户端处理的最新序列号，以及相关的响应。如果它收到一个序列号已经被执行的命令，它会立即响应，而不重新执行该请求。 只读操作可以在不向日志中写入任何内容的情况下进行处理。然而，如果没有额外的措施，这将有返回陈旧数据的风险，因为响应请求的leader可能已经被它不知道的较新的leader所取代了。可线性化的读取必须不返回陈旧的数据，Raft需要两个额外的预防措施来保证这一点而不使用日志。首先，leader必须拥有关于哪些条目被提交的最新信息。leader完整性属性保证leader拥有所有已提交的条目，但在其任期开始时，它可能不知道哪些是。为了找到答案，它需要从其任期内提交一个条目。Raft通过让每个leader在其任期开始时向日志提交一个空白的无操作条目来处理这个问题。第二，leader在处理只读请求之前，必须检查它是否已经被废黜（如果最近的leader已经当选，那么它的信息可能是过时的）。Raft通过让leader在响应只读请求之前与集群中的大多数人交换心跳信息来处理这个问题。另外，leader可以依靠心跳机制来提供一种租约形式[9]，但这要依靠时间来保证安全（它假定了有界的时钟偏移）。 九、 实现和评估我们将Raft实现为复制状态机的一部分，该状态机存储RAMCloud[33]的配置信息并协助RAMCloud协调器的故障转移。Raft的实现包含大约2000行的C++代码，不包括测试、注释或空行。源代码可以免费获得[23]。根据本文的草稿，还有大约25个独立的第三方开源实现[34]，处于不同的开发阶段。另外，各种公司也在部署基于Raft的系统[34]。 本节的其余部分使用三个标准对Raft进行评估：可理解性、正确性和性能。 9.1 可理解性为了衡量Raft相对于Paxos的可理解性，我们对斯坦福大学高级操作系统课程和加州大学伯克利分校分布式计算课程的高年级本科生和研究生进行了一项实验性研究。我们录制了Raft和Paxos的视频讲座，并制作了相应的测验。Raft的讲座涵盖了本文的内容，除了日志压缩；Paxos的讲座涵盖了足够的材料来创建一个等效的复制状态机，包括single-decree Paxos、multi-decree Paxos、重新配置，以及实践中需要的一些优化（如leader选举）。测验测试了对算法的基本理解，还要求学生对角落的情况进行推理。每个学生都看了一个视频，做了相应的测验，看了第二个视频，做了第二个测验。大约一半的参与者先做Paxos部分，另一半先做Raft部分，以便考虑到个人表现的差异和从研究的第一部分获得的经验。我们比较了参与者在每次测验中的得分，以确定参与者是否对Raft表现出更好的理解。 表1：可能存在针对Paxos的偏见的担忧，采取了哪些措施来对抗这种偏见，并提供了哪些额外资料。 我们试图使Paxos和Raft之间的比较尽可能地公平。实验在两个方面对Paxos有利： 43名参与者中的15名报告说以前对Paxos有一些经验，而且Paxos的视频比Raft的视频长14%。正如表1所总结的，我们采取了措施来减少潜在的偏见来源。我们所有的材料都可供查阅[28, 31]。 图14：比较43名参与者在Raft和Paxos测验中的表现的散点图。对角线以上的点（33）代表在Raft上得分较高的参与者。 平均而言，参与者在Raft测验中的得分比Paxos测验高4.9分（在可能的60分中，Raft的平均得分是25.7分，Paxos的平均得分是20.8分）；图14显示了他们的个人得分。成对的t检验表明，在95%的置信度下，Raft分数的真实分布比Paxos分数的真实分布的平均值至少要大2.5分。 我们还创建了一个线性回归模型，根据三个因素预测新学生的测验分数：他们参加了哪次测验，他们之前的Paxos经验程度，以及他们学习算法的顺序。该模型预测，测验的选择会产生有利于Raft的12.5分的差异。这明显高于观察到的4.9分的差异，因为许多实际的学生之前有Paxos的经验，这对Paxos有很大的帮助，而对Raft的帮助则略小。奇怪的是，模型还预测已经参加过Paxos测验的人在Raft上的得分要低6.3分；虽然我们不知道为什么，但这似乎在统计上是有意义的。 图15：使用5分制，参与者被问及（左）他们认为哪种算法更容易在一个有效的、正确的和高效的系统中实现，以及（右）哪种算法更容易向一个CS研究生解释。 我们还在测验后对参与者进行了调查，看看他们认为哪种算法更容易实现或解释；这些结果显示在图15。绝大多数参与者报告说Raft更容易实施和解释（每个问题41人中有33人）。然而，这些自我报告的感受可能不如参与者的测验分数可靠，而且参与者可能因为知道我们的假设–Raft更容易理解而产生偏差。 关于Raft用户研究的详细讨论，可参见[31]。 9.2 正确性我们已经为第5节中描述的共识机制开发了一个正式的规范和安全证明。形式规范[31]使用TLA+规范语言[17]使图2中总结的信息完全精确。它长约400行，作为证明的主题。它本身对实现Raft的人来说也是有用的。我们已经使用TLA证明系统[7]机械地证明了对数完整性属性。然而，这个证明依赖于没有经过机械检查的不变量（例如，我们没有证明规范的类型安全）。此外，我们已经写了一个关于状态机安全属性的非正式证明[31]，它是完整的（它仅仅依赖于规范）和相对精确的（它大约有3500字）。 9.3 性能Raft的性能与其他共识算法（如Paxos）相似。对于性能来说，最重要的情况是当一个已建立的leader在复制新的日志条目时。Raft使用最少的信息数量（从leader到一半集群的单次往返）实现了这一点。进一步提高Raft的性能也是可能的。例如，它很容易支持批处理和管道化请求，以获得更高的吞吐量和更低的延迟。文献中已经为其他算法提出了各种优化方案；其中许多方案可以应用于Raft，但我们将此留给未来的工作。 我们使用我们的Raft实现来衡量Raft的leader选举算法的性能，并回答两个问题。首先，选举过程是否快速收敛？第二，leader崩溃后能实现的最小停机时间是多少？ 图16：检测和替换一个崩溃的leader的时间。上图改变了选举超时的随机性，下图是对最小选举超时的刻度。每条线代表1000次试验（\"150-150ms \"的100次试验除外），并对应于选举超时的特定选择；例如，\"150-155ms \"意味着选举超时是在150ms和155ms之间随机和统一选择的。测试是在一个由五个服务器组成的集群上进行的，广播时间大约为15ms。九个服务器的集群的结果是相似的。 为了测试leader选举性能，我们反复让五个服务器集群的leader崩溃，并计时检测崩溃和选举新leader所需的时间（见图16）。为了产生一个最坏的情况，每次试验中的服务器都有不同的日志长度，所以一些候选人没有资格成为leader。此外，为了鼓励分裂投票，我们的测试脚本在终止其进程之前触发了leader的心跳RPC的同步广播（这接近于leader在崩溃前复制新的日志条目的行为）。领头羊在其心跳间隔内被均匀地随机崩溃，这个间隔是所有测试中最小选举超时的一半。因此，最小的可能停机时间大约是最小选举超时的一半。 图16中的顶部图表显示，选举超时中的少量随机性足以避免选举中的分裂票。在没有随机性的情况下，在我们的测试中，由于许多分裂的选票，领导人选举的时间一直超过10秒。仅仅增加5毫秒的随机性就有很大的帮助，导致中位数停机时间为287毫秒。使用更多的随机性可以改善最坏情况下的行为：使用50ms的随机性，最坏情况下的完成时间（超过1000次试验）是513ms。 图16中的底图显示，通过减少选举超时，可以减少停机时间。在选举超时12-24ms的情况下，平均只需要35ms就能选出一个leader（最长的一次试验用了152ms）。然而，将超时时间降低到超过这个点，就违反了Raft的时间要求：leader很难在其他服务器开始新的选举之前广播心跳。这可能会导致不必要的leader更换，降低系统的整体可用性。我们建议使用保守的选举超时，如150-300ms；这样的超时不太可能导致不必要的leader变更，并且仍然会提供良好的可用性。 十、 相关工作有许多与共识算法有关的出版物，其中许多属于以下类别之一： Lamport对Paxos的原始描述[15]，以及试图更清楚地解释它[16, 20, 21]。 对Paxos的阐述，填补了缺失的细节，修改了算法，为实现提供了更好的基础[26, 39, 13]。 实现共识算法的系统，如Chubby [2, 4], ZooKeeper [11, 12], 和Spanner [6]。Chubby和Spanner的算法还没有被详细公布，尽管两者都声称是基于Paxos。ZooKeeper的算法已经公布了更多的细节，但它与Paxos有很大的不同。 可以应用于Paxos的性能优化[18, 19, 3, 25, 1, 27]。 Oki和Liskov的Viewstamped Replication (VR)，这是一种与Paxos差不多同时开发的共识的替代方法。最初的描述[29]与分布式交易的协议交织在一起，但在最近的更新中[22]，核心共识协议被分离出来。VR使用了一种基于leader的方法，与Raft有许多相似之处。 Raft和Paxos的最大区别是Raft的强leader模式：Raft将leader选举作为共识协议的一个重要部分，它将尽可能多的功能集中在leader身上。这种方法导致了更简单的算法，更容易理解。例如，在Paxos中，leader选举与基本的共识协议是正交的：它只是作为一种性能优化，并不是实现共识的必要条件。然而，这导致了额外的机制： Paxos包括一个两阶段的基本共识协议和一个单独的leader选举机制。相比之下，Raft将leader选举直接纳入共识算法，并将其作为共识的两个阶段中的第一阶段。这导致了比Paxos更少的机制。 与Raft一样，VR和ZooKeeper也是基于leader的，因此与Paxos相比，Raft有许多优势。然而，Raft的机制比VR或ZooKeeper少，因为它将非leader的功能降到最低。例如，Raft中的日志条目只向一个方向流动：从AppendEntries RPCs的leader向外流动。在VR中日志条目是双向流动的（leader可以在选举过程中接收日志条目）；这导致了额外的机制和复杂性。ZooKeeper公布的描述也是将日志条目同时传输给leader和从leader那里传输，但其实现显然更像Raft[35]。 Raft的消息类型比我们所知的任何其他基于共识的日志复制算法都少。例如，我们计算了VR和ZooKeeper用于基本共识和成员变更的消息类型（不包括日志压缩和客户端交互，因为这些几乎是独立于算法的）。VR和ZooKeeper各自定义了10种不同的消息类型，而Raft只有4种消息类型（两个RPC请求和它们的响应）。Raft的消息比其他算法的消息更密集一些，但它们总体上更简单。此外，VR和ZooKeeper的描述是在leader变更期间传输整个日志；为了优化这些机制，将需要额外的消息类型，以便它们能够实用。 Raft的强leader方法简化了算法，但它排除了一些性能优化。例如，Egalitarian Paxos（EPaxos）在某些条件下可以通过无leader的方法获得更高的性能[27]。EPaxos利用了状态机命令的交换性。只要其他同时提出的命令与之换算，任何服务器都可以只用一轮通信来提交一个命令。然而，如果同时提出的命令不能相互交换，EPaxos需要额外的一轮通信。由于任何服务器都可以提交命令，EPaxos可以很好地平衡服务器之间的负载，并能够在广域网环境中实现比Raft更低的延迟。然而，它给Paxos增加了很大的复杂性。 其他工作中已经提出或实施了几种不同的集群成员变化方法，包括Lamport的原始提议[15]、VR[22]和SMART[24]。我们为Raft选择了联合共识的方法，因为它利用了共识协议的其他部分，所以成员资格变更所需的额外机制非常少。Lamport的基于α的方法不是Raft的选择，因为它假设没有leader也能达成共识。与VR和SMART相比，Raft的重新配置算法的优点是，成员变化可以在不限制正常请求的处理的情况下发生；相反，VR在配置变化期间停止所有的正常处理，而SMART对未处理的请求数量施加了类似α的限制。Raft的方法也比VR或SMART增加了更少的机制。 十一、 结论算法的设计通常以正确性、效率和/或简洁性为主要目标。尽管这些都是有价值的目标，但我们认为可理解性也同样重要。在开发者将算法转化为实际的实现之前，其他的目标都无法实现，而实际的实现将不可避免地偏离和扩展公布的形式。除非开发者对算法有深刻的理解，并能对其产生直觉，否则他们将很难在其实现中保留其理想的属性。 在本文中，我们讨论了分布式共识的问题，其中一个被广泛接受但难以理解的算法–Paxos，多年来一直在挑战学生和开发者。我们开发了一种新的算法–Raft，我们已经证明它比Paxos更容易理解。我们还认为Raft为系统建设提供了一个更好的基础。将可理解性作为主要的设计目标，改变了我们设计Raft的方式；随着设计的进展，我们发现自己反复使用了一些技术，例如分解问题和简化状态空间。这些技术不仅提高了Raft的可理解性，而且也使我们更容易相信它的正确性。 References[1] BOLOSKY, W. J., BRADSHAW, D., HAAGENS, R. B., KUSTERS, N. P., AND LI, P. Paxos replicated state machines as the basis of a high-performance data store. In Proc. NSDI’11, USENIX Conference on Networked Systems Design and Implementation (2011), USENIX, pp. 141–154. [2] BURROWS, M. The Chubby lock service for looselycoupled distributed systems. In Proc. OSDI’06, Symposium on Operating Systems Design and Implementation(2006), USENIX, pp. 335–350. [3] CAMARGOS, L. J., SCHMIDT, R. M., AND PEDONE, F. Multicoordinated Paxos. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 316–317. [4] CHANDRA, T. D., GRIESEMER, R., AND REDSTONE, J. Paxos made live: an engineering perspective. In Proc. PODC’07, ACM Symposium on Principles of Distributed Computing (2007), ACM, pp. 398–407. [5] CHANG, F., DEAN, J., GHEMAWAT, S., HSIEH, W. C., WALLACH, D. A., BURROWS, M., CHANDRA, T., FIKES, A., AND GRUBER, R. E. Bigtable: a distributed storage system for structured data. In Proc. OSDI’06, USENIX Symposium on Operating Systems Design and Implementation (2006), USENIX, pp. 205–218. [6] CORBETT, J. C., DEAN, J., EPSTEIN, M., FIKES, A., FROST, C., FURMAN, J. J., GHEMAWAT, S., GUBAREV, A., HEISER, C., HOCHSCHILD, P., HSIEH, W., KANTHAK, S., KOGAN, E., LI, H., LLOYD, A., MELNIK, S., MWAURA, D., NAGLE, D., QUINLAN, S., RAO, R., ROLIG, L., SAITO, Y., SZYMANIAK, M., TAYLOR, C., WANG, R., AND WOODFORD, D. Spanner: Google’s globally-distributed database. In Proc. OSDI’12, USENIX Conference on Operating Systems Design and Implementation (2012), USENIX, pp. 251–264. [7] COUSINEAU, D., DOLIGEZ, D., LAMPORT, L., MERZ, S., RICKETTS, D., AND VANZETTO, H. TLA+ proofs. In Proc. FM’12, Symposium on Formal Methods (2012), D. Giannakopoulou and D. M´ery, Eds., vol. 7436 of Lecture Notes in Computer Science, Springer, pp. 147–154. [8] GHEMAWAT, S., GOBIOFF, H., AND LEUNG, S.-T. The Google file system. In Proc. SOSP’03, ACM Symposium on Operating Systems Principles (2003), ACM, pp. 29–43. [9] GRAY, C., AND CHERITON, D. Leases: An efficient faulttolerant mechanism for distributed file cache consistency. In Proceedings of the 12th ACM Ssymposium on Operating Systems Principles (1989), pp. 202–210. [10] HERLIHY, M. P., AND WING, J. M. Linearizability: a correctness condition for concurrent objects. ACM Transactions on Programming Languages and Systems 12 (July 1990), 463–492. [11] HUNT, P., KONAR, M., JUNQUEIRA, F. P., AND REED, B. ZooKeeper: wait-free coordination for internet-scale systems. In Proc ATC’10, USENIX Annual Technical Conference (2010), USENIX, pp. 145–158. [12] JUNQUEIRA, F. P., REED, B. C., AND SERAFINI, M. Zab: High-performance broadcast for primary-backup systems. In Proc. DSN’11, IEEE/IFIP Int’l Conf. on Dependable Systems &amp; Networks (2011), IEEE Computer Society, pp. 245–256. [13] KIRSCH, J., AND AMIR, Y. Paxos for system builders. Tech. Rep. CNDS-2008-2, Johns Hopkins University, 2008. [14] LAMPORT, L. Time, clocks, and the ordering of events in a distributed system. Commununications of the ACM 21, 7 (July 1978), 558–565. [15] LAMPORT, L. The part-time parliament. ACM Transactions on Computer Systems 16, 2 (May 1998), 133–169. [16] LAMPORT, L. Paxos made simple. ACM SIGACT News 32, 4 (Dec. 2001), 18–25. [17] LAMPORT, L. Specifying Systems, The TLA+ Language and Tools for Hardware and Software Engineers. AddisonWesley, 2002. [18] LAMPORT, L. Generalized consensus and Paxos. Tech. Rep. MSR-TR-2005-33, Microsoft Research, 2005. [19] LAMPORT, L. Fast paxos. Distributed Computing 19, 2 (2006), 79–103. [20] LAMPSON, B. W. How to build a highly available system using consensus. In Distributed Algorithms, O. Baboaglu and K. Marzullo, Eds. Springer-Verlag, 1996, pp. 1–17. [21] LAMPSON, B. W. The ABCD’s of Paxos. In Proc. PODC’01, ACM Symposium on Principles of Distributed Computing (2001), ACM, pp. 13–13. [22] LISKOV, B., AND COWLING, J. Viewstamped replication revisited. Tech. Rep. MIT-CSAIL-TR-2012-021, MIT, July 2012. [23] LogCabin source code. http://github.com/logcabin/logcabin. [24] LORCH, J. R., ADYA, A., BOLOSKY, W. J., CHAIKEN, R., DOUCEUR, J. R., AND HOWELL, J. The SMART way to migrate replicated stateful services. In Proc. EuroSys’06, ACM SIGOPS/EuroSys European Conference on Computer Systems (2006), ACM, pp. 103–115. [25] MAO, Y., JUNQUEIRA, F. P., AND MARZULLO, K.Mencius: building efficient replicated state machines for WANs. In Proc. OSDI’08, USENIX Conference on Operating Systems Design and Implementation (2008), USENIX, pp. 369–384. [26] MAZIERES, D. Paxos made practical. http://www.scs.stanford.edu/˜dm/home/papers/paxos.pdf, Jan. 2007. [27] MORARU, I., ANDERSEN, D. G., AND KAMINSKY, M.There is more consensus in egalitarian parliaments. In Proc. SOSP’13, ACM Symposium on Operating System Principles (2013), ACM. [28] Raft user study. http://ramcloud.stanford.edu/˜ongaro/userstudy/. [29] OKI, B. M., AND LISKOV, B. H. Viewstamped replication: A new primary copy method to support highly-available distributed systems. In Proc. PODC’88, ACM Symposium on Principles of Distributed Computing (1988), ACM, pp. 8–17. [30] O’NEIL, P., CHENG, E., GAWLICK, D., AND ONEIL, E. The log-structured merge-tree (LSM-tree). Acta Informatica 33, 4 (1996), 351–385. [31] ONGARO, D. Consensus: Bridging Theory and Practice.PhD thesis, Stanford University, 2014 (work in progress).http://ramcloud.stanford.edu/˜ongaro/thesis.pdf. [32] ONGARO, D., AND OUSTERHOUT, J. In search of an understandable consensus algorithm. In Proc ATC’14,USENIX Annual Technical Conference (2014), USENIX. [33] OUSTERHOUT, J., AGRAWAL, P., ERICKSON, D., KOZYRAKIS, C., LEVERICH, J., MAZIERES ` , D., MITRA, S., NARAYANAN, A., ONGARO, D., PARULKAR, G., ROSENBLUM, M., RUMBLE, S. M., STRATMANN, E., AND STUTSMAN, R. The case for RAMCloud. Communications of the ACM 54 (July 2011), 121–130. [34] Raft consensus algorithm website.http://raftconsensus.github.io. [35] REED, B. Personal communications, May 17, 2013. [36] ROSENBLUM, M., AND OUSTERHOUT, J. K. The design and implementation of a log-structured file system. ACMTrans. Comput. Syst. 10 (February 1992), 26–52. [37] SCHNEIDER, F. B. Implementing fault-tolerant services using the state machine approach: a tutorial. ACM Computing Surveys 22, 4 (Dec. 1990), 299–319. [38] SHVACHKO, K., KUANG, H., RADIA, S., AND CHANSLER, R. The Hadoop distributed file system. In Proc. MSST’10, Symposium on Mass Storage Systems and Technologies (2010), IEEE Computer Society, pp. 1–10. [39] VAN RENESSE, R. Paxos made moderately complex.Tech. rep., Cornell University, 2012.","link":"/2022/11/30/In-Search-of-an-Understandable-Consensus-Algorithm/"},{"title":"MapReduce: Simplified Data Processing on Large Clusters","text":"MapReduce: 大型集群上的简化数据处理摘要MapReduce是一个设计模型，也是一个处理和产生海量数据的一个相关实现。用户指定一个用于处理一个键值（key-value）对生成一组key/value对形式的中间结果的map函数，以及一个将中间结果键相同的键值对合并到一起的reduce函数。许多现实世界的任务都能满足这个模型，如这篇文章所示。 使用这个功能形式实现的程序能够在大量的普通机器上并行执行。这个运行程序的系统关心下面的这些细节：输入数据的分区、一组机器上调度程序执行、处理机器失败问题，以及管理所需的机器内部的通信。这使没有任何并行处理和分布式系统经验的程序员能够利用这个大型分布式系统的资源。 我们的MapReduce实现运行在一个由普通机器组成的大规模集群上，具有很高的可扩展性：一个典型的MapReduce计算会在几千台机器上处理许多TB的数据。程序员们发现这个系统很容易使用：目前已经实现了几百个MapReduce程序，在Google的集群上，每天有超过一千个的MapReduce工作在运行。 一、 引言在过去的5年中，本文作者和许多Google的程序员已经实现了数百个特定用途的计算程序，处理了海量的原始数据，包括抓取到的文档、网页请求日志等，计算各种衍生出来的数据，如反向索引、网页文档的图形结构的各种表示、每个host下抓取到的页面数量的总计、一个给定日期内的最频繁查询的集合等。大多数这种计算概念明确。然而，输入数据通常都很大，并且计算必须分布到数百或数千台机器上以确保在一个合理的时间内完成。如何并行计算、分布数据、处理错误等问题使这个起初很简单的计算，由于增加了处理这些问题的很多代码而变得十分复杂。 为了解决这个复杂问题，我们设计了一个新的抽象模型，它允许我们将想要执行的计算简单的表示出来，而隐藏其中并行计算、容错、数据分布和负载均衡等很麻烦的细节。我们的抽象概念是受最早出现在lisp和其它结构性语言中的map和reduce启发的。我们认识到，大多数的计算包含对每个在输入数据中的逻辑记录执行一个map操作以获取一组中间key/value对，然后对含有相同key的所有中间值执行一个reduce操作，以此适当的合并之前的衍生数据。由用户指定map和reduce操作的功能模型允许我们能够简单的进行并行海量计算，并使用re-execution作为主要的容错机制。 这项工作的最大贡献是提供了一个简单的、强大的接口，使我们能够自动的进行并行和分布式的大规模计算，通过在由普通PC组成的大规模集群上实现高性能的接口来进行合并。 第二章描述了基本的编程模型，并给出了几个例子。第三章描述了一个为我们的聚类计算环境定制的MapReduce接口实现。第四章描述了我们发现对程序模型很有用的几个优化。第六章探索了MapReduce在Google内部的使用，包括我们在将它作为生产索引系统重写的基础的一些经验。第七章讨论了相关的和未来的工作。 二、 编程模型这个计算输入一个key/value对集合，产生一组输出key/value对。MapReduce库的用户通过两个函数来标识这个计算：Map和Reduce。 Map，由用户编写，接收一个输入对，产生一组中间key/value对。MapReduce库将具有相同中间key I的聚合到一起，然后将它们发送给Reduce函数。 Reduce，也是由用户编写的，接收中间key I和这个key的值的集合，将这些值合并起来，形成一个尽可能小的集合。通常，每个Reduce调用只产生0或1个输出值。这些中间值经过一个迭代器（iterator）提供给用户的reduce函数。这允许我们可以处理由于数据量过大而无法载入内存的值的链表。 2.1 例子考虑一个海量文件集中的每个单词出现次数的问题，用户会写出类似于下面的伪码： 12345678910111213map(String key, String value): // key: document name // value: document contents for each word w in value: EmitIntermediate(w, &quot;1&quot;);reduce(String key, Iterator values): // key: a word // values: a list of counts int result = 0; for each v in values: result += ParseInt(v); Emit(AsString(result)); Map函数对每个单词增加一个相应的出现次数（在这个例子中仅仅为“1”）。Reduce函数将一个指定单词所有的计数加到一起。 此外，用户使用输入和输出文件的名字、可选的调节参数编写代码，来填充一个mapreduce规格对象，然后调用MapReduce函数，并把这个对象传给它。用户的代码与MapReduce库（C++实现）连接到一起。附录A包含了这个例子的整个程序。 2.2 类型尽管之前的伪代码中使用了字符串格式的输入和输出，但是在概念上，用户定义的map和reduce函数需要相关联的类型： 12map (k1, v1) -&gt; list(k2, v2)reduce (k2, list(v2)) -&gt; list(v2) 也就是说，输入的键和值和输出的键和值来自不同的域。此外，中间结果的键和值与输出的键和值有相同的域。 MapReduce的C++实现与用户定义的函数使用字符串类型进行参数传递，将类型转换的工作留给用户的代码来处理。 2.3 更多的例子这里有几个简单有趣的程序，能够使用MapReduce计算简单的表示出来。 分布式字符串查找（Distributed Grep）：map函数将匹配一个模式的行找出来。Reduce函数是一个恒等函数，只是将中间值拷贝到输出上。 URL访问频率计数（Count of URL Access Frequency）：map函数处理web页面请求的日志，并输出&lt;URL, 1&gt;。Reduce函数将相同URL的值累加到一起，生成一个&lt;URL, total count&gt;对。 翻转网页连接图（Reverse Web-Link Graph）：map函数为在一个名为source的页面中指向目标（target）URL的每个链接输出&lt;target, source&gt;对。Reduce函数将一个给定目标URL相关的所有源（source）URLs连接成一个链表，并生成对：&lt;target, list(source)&gt;。 主机关键向量指标（Term-Vector per Host）：一个检索词向量将出现在一个文档或是一组文档中最重要的单词概述为一个&lt;word, frequency&gt;对链表。Map函数为每个输入文档产生一个&lt;hostname, term vector&gt;（hostname来自文档中的URL）。Reduce函数接收一个给定hostname的所有文档检索词向量，它将这些向量累加到一起，将罕见的向量丢掉，然后生成一个最终的&lt;hostname, term vector&gt;对。 倒排索引（Inverted Index）：map函数解析每个文档，并生成一个&lt;word, document ID&gt;序列。Reduce函数接收一个给定单词的所有键值对，所有的输出对形成一个简单的倒排索引。可以通过对计算的修改来保持对单词位置的追踪。 分布式排序（Distributed Sort）：map函数将每个记录的key抽取出来，并生成一个&lt;key, record&gt;对。Reduce函数不会改变任何的键值对。这个计算依赖了在4.1节提到的分区功能和4.2节提到的排序属性。 三、 实现MapReduce接口有很多不同的实现，需要根据环境来做出合适的选择。比如，一个实现可能适用于一个小的共享内存机器，而另一个实现则适合一个大的NUMA多处理器机器，再另一个可能适合一个更大的网络机器集合。 这一章主要描述了针对在Google内部广泛使用的计算环境的一个实现：通过交换以太网将大量的普通PC连接到一起的集群。在我们的环境中： （1） 机器通常是双核x86处理器、运行Linux操作系统、有2-4G的内存。 （2） 使用普通的网络硬件—通常是100Mb/s或者是1Gb/s的机器带宽，但是平均值远小于带宽的一半。 （3） 由数百台或者数千台机器组成的集群，因此机器故障是很平常的事。 （4） 存储是由直接装在不同机器上的便宜的IDE磁盘提供。一个内部的分布式文件系统用来管理存储这些磁盘上的数据。文件系统在不可靠的硬件上使用副本机制提供了可用性和可靠性。 （5） 用户将工作提交给一个调度系统，每个工作由一个任务集组成，通过调度者映射到集群中可用机器的集合上。 3.1 执行概述通过自动的将输入数据分区成M个分片，Map调用被分配到多台机器上运行。数据的分片能够在不同的机器上并行处理。使用分区函数（如，hash(key) mod R）将中间结果的key进行分区成R个分片，Reduce调用也被分配到多台机器上运行。分区的数量（R）和分区函数是由用户指定的。 图1：执行概述 图1中显示了我们实现的一个MapReduce操作的整个流程。当用户程序调用MapReduce函数时，下面一系列的行为将会发生（图1中所使用的数字标识将与下面列表中的相对应）： 用户程序中的MapReduce库会先将输入文件分割成M个通常为16MB-64MB大小的片（用户可以通过可选参数进行控制）。然后它将在一个集群的机器上启动许多程序的拷贝。 这些程序拷贝中的一个是比较特殊的——master。其它的拷贝都是工作进程，是由master来分配工作的。有M个map任务和R个reduce任务被分配。Master挑选出空闲的工作进程，并把一个map任务或reduce任务分配到这个进程上。 一个分配了map任务的工作进程读取相关输入分片的内容，它将从输入数据中解析出key/value对，并将其传递给用户定义的Map函数。Map函数生成的中间key/value对缓存在内存中。 缓存中的键值对周期性的写入到本地磁盘，并通过分区函数分割为R个区域。将这些缓存在磁盘上的键值对的位置信息传回给master，master负责将这些位置信息传输给reduce工作进程。 当一个reduce工作进程接收到master关于位置信息的通知时，它将使用远程调用函数（RPC）从map工作进程的磁盘上读取缓存的数据。当reduce工作进程读取完所有的中间数据后，它将所有的中间数据按中间key进行排序，以保证相同key的数据聚合在一起。这个排序是需要的，因为通常许多不同的key映射到相同的reduce任务上。如果中间数据的总量太大而无法载入到内存中，则需要进行外部排序。 reduce工作进程迭代的访问已排序的中间数据，并且对遇到的每个不同的中间key，它会将key和相关的中间values传递给用户的Reduce函数。Reduce函数的输出追加到当前reduce分区一个最终的输出文件上。 当所有的map任务和reduce任务完成后，master会唤醒用户程序。这时候，用户程序中的MapReduce调用会返回到用户代码上。 在成功完成后，MapReduce操作输出到R个输出文件（每个reduce任务生成一个，文件名是由用户指定的）中的结果是有效的。通常，用户不需要合并这R个输出文件，它们经常会将这些文件作为输入传递给另一个MapReduce调用，或者在另一个处理这些输入分区成多个文件的分布式应用中使用。 3.2 Master数据结构Master保留了几个数据结构。对于每个Map和Reduce任务，它存储了它们的状态（idle、in-progress或者completed），以及工作进程机器的特性（对于非空闲任务）。 Master是中间文件区域的位置信息从map任务传送到reduce任务的一个通道。因此，对于每个完成的map任务来说，master存储了map任务产生的R个中间文件区域的位置信息和大小。在map任务完成时，master会接收到更新这个含有位置信息和大小信息的消息。信息被增量的传输到运行in-progress的reduce任务的工作进程上。 3.3 容错因为MapReduce库是被设计成运行在数百或数千台机器上帮助处理海量数据的，所以这个库必须能够优雅的处理机器故障。 工作进程故障Master周期性的ping每个工作进程，如果在一个特定的时间内没有收到响应，则master会将这个工作进程标记为失效。任何由失效的工作进程完成的map任务都被标记为初始idle状态，因此这个map任务会被重新分配给其它的工作进程。同样的，任何正在处理的map任务或reduce任务也会被置为idle状态，进而可以被重新调度。 在一个失效的节点上完成的map任务会被重新执行，因为它们的输出被存放在失效机器的本地磁盘上，而磁盘不可访问。完成的reduce任务不需要重新执行，因为它们的输出被存储在全局文件系统上。 当一个map任务先被工作进程A执行，然后再被工作进程B执行（因为A失效了），所有执行reduce任务的工作进程都会接收到重新执行的通知，任何没有从工作进程A上读取数据的reduce任务将会从工作进程B上读取数据。 MapReduce对于大规模工作进程失效有足够的弹性。比如，在一个MapReduce操作处理过程中，网络维护造成了80台机器组成的集群几分钟内不可达。MapReduce的master会重新执行那些在不可达机器上完成的工作，并持续推进，最终完成MapReduce操作。 Master故障将上面提到的master数据结构周期性的进行写检查点操作（checkpoint）是比较容易的。如果master任务死掉，一个新的拷贝会从最近的检查点状态上启动。然而，假定只有一个单独的master，它的故障是不大可能的。因此，如果master故障，我们当前的实现是中止MapReduce计算。 当前故障的语义当用户提供的map和reduce操作是输入确定性函数，我们的分布式实现与无故障序列执行整个程序所生成的结果相同。 我们依靠map和reduce任务输出的原子性提交来实现这个属性。每个in-progress任务将它们的输出写入到一个私有的临时文件中。一个reduce任务产生一个这样的文件，一个map任务产生R个这样的文件（每个reduce任务一个）。当一个map任务完成时，它将发送给master一个消息，其中包括R个临时文件的名字。如果master收到一个已经完成的map任务的完成消息，则忽略这个消息。否则，它将这R个文件名记录在master的数据结构中。 当一个reduce任务完成后，reduce的工作进程自动的将临时文件更名为最终的输出文件，如果相同的reduce任务运行在多台机器上，会调用多个重命名操作将这些文件更名为最终的输出文件。 绝大部分的map和reduce操作是确定性的，事实上，在这种情况下我们的语义与一个序列化的执行是相同的，这使程序开发者能够简单的推出他们程序的行为。当map和/或reduce操作是不确定性的时，我们提供较弱但依然合理的语义。在不确定性的操作面前，一个特定的reduce任务R1的输出与一个序列执行的不确定性程序生成的输出相同。然而，一个不同的reduce任务R2的输出可能与一个不同的序列执行的不确定性程序生成的输出可能一致。 考虑map任务M和reduce任务R1和R2。假定e(Ri)是提交的Ri的执行过程（有且仅有这样一个过程）。e(R1)可能从M的一个执行生成的输出中读取数据，e(R2)可能从M的一个不同执行生成的输出中读取数据，则会产生较弱的语义。 3.4 位置在我们的计算环境中，网络带宽是一个相对不足的资源。我们通过将输入数据存放在组成集群的机器的本地磁盘来节省网络带宽。GFS将每个文件分割成64MB大小的块，每个块会在不同的机器上存储几个拷贝（通常为3个）。MapReduce master会考虑文件的位置信息，并试图将一个map任务分配到包含相关输入数据副本的机器上。如果这样做失败，它会试图将map任务调度到一个包含任务输入数据的临近的机器上（例如，与包含输入数据机器在同一个网络下进行交互的一个工作进程）。当在集群的一个有效部分上运行大规模的MapReduce操作时，大多数输入数据都从本地读取，不消耗任何网络带宽。 3.5 任务粒度根据上面所提到的，我们将map阶段细分为M个片，将reduce阶段细分为R个片。理想情况下，M和R应该比工作机器的数量大得多，每个工作进程执行很多不同的任务来促使负载均衡，在一个工作进程失效时也能够快速的恢复：许多完成的map任务可以传播到其它所有的工作机器上。 在我们的实现中，对于取多大的M和R有一个实际的界限，因为如上面提到的那样，master必须进行O(M+R)次调度，在内存中保持O(MR)个状态。（对内存使用的恒定因素影响较小，然而：对由每个map任务/reduce任务对占用大约一个字节所组成的O(MR)片的状态影响较大。） 此外，R经常是由用户约束的，因为每个reduce任务的输出最终放在一个分开的输出文件中。实际中，我们倾向选择M值，以使每一个独立的任务能够处理大约16MB到64MB的输入数据（可以使上面提到的位置优化有更好的效果），把R值设置为我们想使用的工作机器的一个小的倍数。我们经常使用2000个工作机器，设置M=200000和R=5000，来执行MapReduce计算。 3.6 备用任务影响一个MapReduce操作整体执行时间的一个通常因素是“落后者”：一个使用了异常的时间完成了计算中最后几个map任务或reduce任务中的一个的机器。可能有很多因素导致落后者的出现，例如，一个含有损坏磁盘的机器频繁的处理可校正的错误，使它的读取速度从30MB/s下降到了1MB/s。集群调度者可能将其它的任务分配到这个机器上，由于CPU、内存、磁盘或网络带宽的竞争会导致MapReduce代码执行的更慢。我们遇到的最近一个问题是机器初始化代码中的一个bug，它会使处理器的缓存不可用：受到这个问题影响的机器会慢上百倍。 我们使用一个普通的机制来缓解落后者问题。当一个MapReduce操作接近完成时，master调度备用（backup）任务执行剩下的、处于in-process状态的任务。一旦主任务或是备用任务完成，则将这个任务标识为已经完成。我们优化了这个机制，使它通常能够仅仅增加少量的操作所使用的计算资源。我们发现这能有效的减少完成大规模MapReduce操作所需要的时间。作为一个例子，5.3节所描述的那种程序在禁用备用任务机制的情况下，会需要多消耗44%的时间。 四、 细化尽管简单的编写Map和Reduce函数提供的基本功能足够满足大多数需要，但是，我们发现一些扩展是很有用的。这会在本章进行描述。 4.1 分区函数MapReduce的用户指定所希望的reduce任务/输出文件的数量（R）。使用分区函数在中间键上将数据分区到这些任务上。一个默认的分区函数使用hash方法（如“hash(key) mod R”），它能产生相当平衡的分区。然而，在一些情况下，需要使用其它的在key上的分区函数对数据进行分区。为了支持这种情况，MapReduce库的用户能够提供指定的分区函数。例如，使用“hash(Hostname(urlkey)) mod R”作为分区函数，使所有来自同一个host的URL最终放到同一个输出文件中。 4.2 顺序保证我们保证在一个给定的分区内，中间key/value对是根据key值顺序增量处理的。顺序保证可以使它易于生成一个有序的输出文件，这对于输出文件需要支持有效的随机访问，或者输出的用户方便的查找排序的数据很有帮助。 4.3 组合（Combiner）函数在一些情况下，每个map任务产生的中间key会有很多重复，并且用户指定的reduce函数满足结合律和交换律。2.1节中提到的单词技术的例子就是一个很好的例子。因为单词频率倾向于zifp分布，每个map任务都会产生数百或数千个&lt;the, 1&gt;形式的记录。所有这些计数都会通过网络发送给一个单独的reduce任务，然后通过Reduce函数进行累加并产生一个数字。我们允许用户指定一个可选的Combiner函数，它能在数据通过网络发送前先对这些数据进行局部合并。 Combiner函数在每台执行map任务的机器上执行。通常情况下，combiner函数和reduce函数的代码是相同的，两者唯一不同的是MapReduce库如何处理函数的输出。Reduce函数的输出被写入到一个最终的输出文件中，而combiner函数会写入到一个将被发送给reduce函数的中间文件中。 局部合并可以有效的对某类MapReduce操作进行加速。附录A包含了一个使用combiner函数的例子。 4.4 输入和输出类型MapReduce库支持几种不同格式的输入数据。比如，“text”模式的输入可以将每一行看出一个key/value对：key是该行在文件中的偏移量，value是该行的内容。另一中常见的支持格式是根据key进行排序存储一个key/value对的序列。每种输入类型的实现知道如何将自己分割成对map任务处理有意义的区间（例如，text模式区间分割确保区间分割只在行的边界进行）。用户能够通过实现一个简单的读取（reader）接口来增加支持一种新的输入类型，尽管大多数用户仅仅使用了预定义输入类型中的一小部分。 Reader并不是必须从文件中读取数据，比如，我们可以容易的定义一个从数据库中读取记录，或者从内存的数据结构中读取数据的Reader。 类似的，我们提供一组输出类型来产生不同格式的数据，用户也可以简单的通过代码增加对新输出类型的支持。 4.5 副作用在一些情况下，MapReduce的用户发现为它们的map和/或reduce操作的输出生成辅助的文件很方便。我们依靠应用的writer将这个副作用变成原子的和幂等的。通常，应用会将结果写入到一个临时文件，然后在数据完全生成后，原子的重命名这个文件。 如果一个单独任务产生的多个输出文件，我们没有提供两阶段提交的原子操作。因此，产生多个输出文件且对交叉文件有一致性需求的任务应该是确定性的操作。但是在实际工作中，这个限制并不是一个问题。 4.6 跳过损坏的记录有时，在我们的代码中会存在一些bug，它们会导致Map或Reduce函数在处理特定的记录上一定会Crash。这样的bug会阻止MapReduce操作顺利完成。通常的做法是解决这个bug，但有时，这是不可行的；可能是由于第三方的库中的bug，而我们没有这个库的源码。有时，忽略一些记录也是可以接受的，例如，当在海量的数据集上做数据统计时。我们提供了一个可选的运行模式，MapReduce库探测出哪些记录会导致确定性的Crash，并跳过这些记录以继续执行这个程序。 每个工作进程都安装了一个信号处理器，它能捕获段错误和总线错误。在调用用户的Map或Reduce操作之前，MapReduce库将记录的序号存储到全局变量中。如果用户代码产生一个信号，这个信号处理器会向MapReudce master发送一个“临死前”的UDP包，其中包含了这个序号。当master看到对于一个特定的记录有多个失败信号时，在相应的Map或Reduce任务下一次重新执行时，master会通知它跳过这个记录。 4.7 本地执行在Map或Reduce函数中调试问题是很棘手的，因为实际的计算是发生在一个分布式系统上的，通常有几千台机器，并且是由master动态分配的。为了有助于调试、性能分析和小规模测试，我们开发了一个MapReduce库可供选择的实现，它将在本地机器上序列化的执行一个MapReduce的所有工作。这为用户提供了对MapReduce操作的控制，使计算能被限制在一个特定的map任务上。用户使用标记调用他们的程序，并能够简单的使用它们找到的任何调试或测试工具（如，gdb）。 4.8 状态信息Master运行了一个内部的HTTP服务，并显示出状态集页面供人们查看，如，有多少任务已经完成、有多少正在处理、输入的字节数、中间数据的字节数、输出的字节数、处理速率等。这些页面也包含了指向每个任务生成的标准错误和标准输出文件的链接。用户能使用这些数据预测这个计算将要持续多长时间，以及是否应该向这个计算添加更多的资源。这些页面也有助于找出计算比预期执行慢的多的原因。 此外，顶层的状态页显示了哪些工作进程失效，哪些map和reduce任务在处理时失败。这个信息对试图诊断出用户代码中的bug很有用。 4.9 计数器MapReduce库提供了一个计数器，用于统计不同事件的发生次数。比如，用户代码想要统计已经处理了多少单词，或者已经对多少德国的文档建立了索引等。 用户代码可以使用这个计数器创建一个命名的计数器对象，然后在Map和/或Reduce函数中适当的增加这个计数器的计数。例如： 12345678Counter* uppercase;uppercase = GetCounter(&quot;uppercase&quot;);map(String name, String contents): for each word w in contents: if (IsCapitalized(w)): uppercase-&gt;Increment(); EmitIntermediate(w, &quot;1&quot;); 独立的工作机器的计数器值周期性的传送到master（附在ping的响应上）master将从成功的map和reduce任务上获取的计数器值进行汇总，当MapReduce操作完成时，将它们返回给用户的代码。当前的计数器值也被显示在了master的状态页面上，使人们能够看到当前计算的进度。当汇总计数器值时，master通过去掉同一个map或reduce任务的多次执行所造成的影响来防止重复计数。（重复执行可能会在我们使用备用任务和重新执行失败的任务时出现。） 一些计数器的值是由MapReduce库自动维护的，如已处理的输入key/value对的数量和已生成的输出key/value对的数量。 用户发现计数器对检查MapReduce操作的行为很有用处。例如，在一些MapReduce操作中，用户代码可能想要确保生成的输出对的数量是否精确的等于已处理的输入对的数量，或者已处理的德国的文档数量在已处理的所有文档数量中是否被容忍。 五、 性能在这章中，我们测试两个运行在一个大规模集群上的MapReduce计算的性能。一个计算在大约1TB的数据中进行特定的模式匹配，另一个计算对大约1TB的数据进行排序。 这两个程序能够代表实际中大量的由用户编写的MapReduce程序，一类程序将数据从一种表示方式转换成另一种形式；另一类程序是从海里的数据集中抽取一小部分感兴趣的数据。 5.1 集群配置所有的程序运行在一个由将近1800台机器组成的集群上。每个机器有两个2GHz、支持超线程的Intel Xeon处理器、4GB的内存、两个160GB的IDE磁盘和一个1Gbps的以太网链路，这些机器部署在一个两层的树状交换网络中，在根节点处有大约100-200Gbps的带宽。所有的机器都采用相同的部署，因此任意两个机器间的RTT都小于1ms。 在4GB内存里，有接近1-1.5GB用于运行在集群上的其它任务。程序在一个周末的下午开始执行，这时主机的CPU、磁盘和网络基本都是空闲的。 5.2 字符串查找（Grep）这个grep程序扫描了大概1010个100字节大小的记录，查找出现概率相对较小的3个字符的模式（这个模式出现在92337个记录中）。输入被分割成接近64MB的片（M=15000），整个输出被放到一个文件中（R=1）。 图2：数据传输速率 图2显示了计算随时间的进展情况。Y轴显示了输入数据的扫描速率，这个速率会随着MapReduce计算的机器数量的增长而增长，当1764个工作进程参与计算时，总的速率超过30GB/s。随着map任务的完成，速率开始下降，并在计算的大约第80秒变为0，整个计算从开始到结束大约持续了150秒，这包含了大约1分钟的启动时间开销，这个开销是由将程序传播到所有工作机器的时间、等待GFS文件系统打开1000个输入文件集的时间和获取位置优化所需信息的时间造成的。 5.3 排序排序程序对1010个100字节大小的记录（接近1TB的数据）进行排序，这个程序模仿了TeraSort benchmark。 排序程序由不到50行的用户代码组成，一个三行的Map函数从一个文本行中抽取出一个10字节的key，并将这个key和原始的文本行作为中间的key/value对进行输出。我们使用内置的Identity函数作为Reduce操作。这个函数将中间key/value对不做任何修改的输出，最终排序结果输出到两路复制的GFS文件中（如，该程序输出了2TB的数据）。 如前所述，输入数据被分割为64MB大小的片（M=15000），将输出结果分成4000个文件（R=4000）。分区函数使用了key的开头字符将数据分隔到R片中的一个。 这个基准测试的分区函数内置了key的分区信息。在一个普通的排序程序中，我们将增加一个预处理MapReduce操作，它能够对key进行抽样，通过key的抽样分布来计算最终排序处理的分割点。 图3：对于排序程序的不同执行过程随时间的数据传输速率 图3（a）显示了排序程序的正常执行过程。左上方的图显示了输入读取的速率，这个速率峰值大约为13GB/s，因为所有的map任务执行完成，速率也在200秒前下降到了0。注意，这里的输入速率比字符串查找的要小，这是因为排序程序的map任务花费了大约一半的处理时间和I/O带宽将终结结果输出到它们的本地磁盘上，字符串查找相应的中间结果输出几乎可以忽略。 左边中间的图显示了数据通过网络从map任务发往reduce任务的速率。这个缓慢的数据移动在第一个map任务完成时会尽快开始。图中的第一个峰值是启动了第一批大概1700个reduce任务（整个MapReduce被分配到大约1700台机器上，每个机器每次最多只执行一个reduce任务）。这个计算执行大概300秒后，第一批reduce任务中的一些执行完成，我们开始执行剩下的reduce任务进行数据处理。所有的处理在计算开始后的大约600秒后完成。 左边下方的图显示了reduce任务将排序后的数据写到最终的输出文件的速率。在第一个处理周期完成到写入周期开始间有一个延迟，因为机器正在忙于对中间数据进行排序。写入的速率会在2-4GB/s上持续一段时间。所有的写操作会在计算开始后的大约850秒后完成。包括启动的开销，整个计算耗时891秒，这与TeraSort benchmark中的最好记录1057秒相似。 还有一些注意事项：因为我们的位置优化策略，大多数数据从本地磁盘中读取，绕开了网络带宽的显示，所以输入速率比处理速率和输出速率要高。处理速率要高于输出速率，因为输出过程要将排序后的数据写入到两个拷贝中（为了可靠性和可用性，我们将数据写入到两个副本中）。我们将数据写入两个副本，因为我们的底层文件系统为了可靠性和可用性提供了相应的机制。如果底层文件系统使用容错编码（erasure coding）而不是复制，写数据的网络带宽需求会降低。 5.4 备用任务的作用在图3（b）中，我们显示了一个禁用备用任务的排序程序的执行过程。执行的流程与如3（a）中所显示的相似，除了有一个很长的尾巴，在这期间几乎没有写入行为发生。在960秒后，除了5个reduce任务的所有任务都执行完成。然而，这些落后者只到300秒后才执行完成。整个计算任务耗时1283秒，增加了大约44%的时间。 5.5 机器故障在图3（c）中，我们显示了一个排序程序的执行过程，在计算过程开始都的几分钟后，我们故意kill掉了1746个工作进程中的200个。底层的调度者会迅速在这些机器上重启新的工作进程（因为只有进程被杀掉，机器本身运行正常）。 工作进程死掉会出现负的输入速率，因为一些之前已经完成的map工作消失了（因为相应的map工作进程被kill掉了），并且需要重新执行。这个map任务会相当快的重新执行。整个计算过程在933秒后完成，包括了启动开销（仅仅比普通情况多花费了5%的时间）。 六、 经验我们在2003年2月完成了MapReduce库的第一个版本，并在2003年8月做了重大的改进，包括位置优化、任务在工作机器上的动态负载均衡执行等。从那时起，我们惊喜的发现，MapReduce库能够广泛的用于我们工作中的各种问题。它已经被用于Google内部广泛的领域，包括： 大规模机器学习问题 Google新闻和Froogle产品的集群问题 抽取数据用于公众查询的产品报告 从大量新应用和新产品的网页中抽取特性（如，从大量的位置查询页面中抽取地理位置信息） 大规模图形计算 图4：随时间变化的MapReduce实例 图4中显示了在我们的源码管理系统中，随着时间的推移，MapReduce程序的数量有明显的增加，从2003年早期的0增加到2004年9月时的900个独立的实例。MapReduce如此的成功，因为它使利用半个小时编写的一个简单程序能够高效的运行在一千台机器上成为可能，这极大的加快了开发和原型设计的周期。此外，它允许没有分布式和/或并行系统经验的开发者能够利用这些资源开发出分布式应用。 表1： 2004年8月运行的MapReduce任务 在每个工作的最后，MapReduce库统计了工作使用的计算资源。在表1中，我们看到一些2004年8月在Google内部运行的MapReduce工作的一些统计数据。 6.1 大规模索引目前为止，MapReduce最重要的应用之一就是完成了对生产索引系统的重写，它生成了用于Google网页搜索服务的数据结构。索引系统的输入数据是通过我们的爬取系统检索到的海量文档，存储为就一个GFS文件集合。这些文件的原始内容还有超过20TB的数据。索引程序是一个包含了5-10个MapReduce操作的序列。使用MapReduce（代替了之前版本的索引系统中的adhoc分布式处理）有几个优点： 索引程序代码是一个简单、短小、易于理解的代码，因为容错、分布式和并行处理都隐藏在了MapReduce库中。比如，一个计算程序的大小由接近3800行的C++代码减少到使用MapReduce的大约700行的代码。 MapReduce库性能非常好，以至于能够将概念上不相关的计算分开，来代替将这些计算混合在一起进行，避免额外的数据处理。这会使索引程序易于改变。比如，对之前的索引系统做一个改动大概需要几个月时间，而对新的系统则只需要几天时间。 索引程序变得更易于操作，因为大多数由于机器故障、机器处理速度慢和网络的瞬间阻塞等引起的问题都被MapReduce库自动的处理掉，而无需人为的介入。 七、 相关工作许多系统都提供了有限的程序模型，并且对自动的并行计算使用了限制。比如，一个结合函数可以在logN时间内在N个处理器上对一个包含N个元素的数组使用并行前缀计算，来获取所有的前缀[6，9，13]。MapReduce被认为是这些模型中基于我们对大规模工作计算的经验的简化和精华。更为重要的是，我们提供了一个在数千个处理器上的容错实现。相反的，大多数并行处理系统只在较小规模下实现，并将机器故障的处理细节交给了程序开发者。 Bulk Synchronous Programming和一些MPI源于提供了更高层次的抽象使它更易于让开发者编写并行程序。这些系统和MapReduce的一个关键不同点是MapReduce开发了一个有限的程序模型来自动的并行执行用户的程序，并提供了透明的容错机制。 我们的位置优化机制的灵感来自于移动磁盘技术，计算用于处理靠近本地磁盘的数据，减少数据在I/O子系统或网络上传输的次数。我们的系统运行在挂载几个磁盘的普通机器上，而不是在磁盘处理器上运行，但是一般方法是类似的。 我们的备用任务机制与Charlotte系统中采用的eager调度机制类似。简单的Eager调度机制有一个缺点，如果一个给定的任务造成反复的失败，整个计算将以失败告终。我们通过跳过损坏记录的机制，解决了这个问题的一些实例。 MapReduce实现依赖了内部集群管理系统，它负责在一个大规模的共享机器集合中分发和运行用户的任务。尽管不是本篇文章的焦点，但是集群管理系统在本质上与像Condor的其它系统类似。 排序功能是MapReduce库的一部分，与NOW-Sort中的操作类似。源机器（map工作进程）将将要排序的数据分区，并将其发送给R个Reduce工作进程中的一个。每个reduce工作进程在本地对这些数据进行排序（如果可能的话就在内存中进行）。当然NOW-Sort没有使MapReduce库能够广泛使用的用户定义的Map和Reduce函数。 River提供了一个编程模型，处理进程通过在分布式队列上发送数据来进行通信。像MapReduce一样，即使在不均匀的硬件或系统颠簸的情况下，River系统依然试图提供较好的平均性能。River系统通过小心的磁盘和网络传输调度来平衡完成时间。通过限制编程模型，MapReduce框架能够将问题分解成很多细颗粒的任务，这些任务在可用的工作进程上动态的调度，以至于越快的工作进程处理越多的任务。这个受限制的编程模型也允许我们在工作将要结束时调度冗余的任务进行处理，这样可以减少不均匀情况下的完成时间。 BAD-FS与MapReduce有完全不同的编程模型，不像MapReduce，它是用于在广域网下执行工作的。然而，它们有两个基本相似点。（1）两个系统都使用了重新执行的方式来处理因故障而丢失的数据。（2）两个系统都本地有限调度原则来减少网络链路上发送数据的次数。 TASCC是一个用于简化结构的高可用性的网络服务。像MapReduce一样，它依靠重新执行作为一个容错机制。 八、 结论MapReduce编程模型已经成功的应用在Google内部的许多不同的产品上。我们将这个成功归功于几个原因。第一，模型很易用，即使对那些没有并行计算和分布式系统经验的开发者，因为它隐藏了并行处理、容错、本地优化和负载均衡这些处理过程。第二，各种各样的问题都能用MapReduce计算简单的表示出来，例如，MapReduce被Google网页搜索服务用于生成数据、排序、数据挖掘、机器学习和许多其它系统。第三，我们已经实现了扩展到由数千台机器组成的大规模集群上使用的MapReduce。这个实现能够有效的利用这些机器自由，因此适合在Google内部遇到的很多海量计算问题。 我们从这项工作中学到了几样东西。第一，限制程序模型使得并行计算和分布式计算变得容易，也容易实现这样的计算容错。第二，网络带宽是一个稀有的资源，因此我们系统中的很多优化的目标都是为了减少数据网络发送的数据量：局部性优化允许我们从本地磁盘读取数据，在本地磁盘中写单个中间数据的副本同样节约了网络带宽。第三，冗余执行可以用来减少缓慢的机器带俩的影响，并可以用来处理机器故障和数据丢失。","link":"/2022/08/01/MapReduce-Simplified-Data-Processing-on-Large-Clusters/"},{"title":"The Design for a Practical System for Fault-Tolerant Virtual Machines","text":"一种实用的容错虚拟机系统的设计摘要我们通过在另一个服务器上的备份虚拟机复制执行主虚拟机的方法，已经实现了一个支持容错虚拟机的商用企业级系统。我们已经在VMware vSphere 4.0上实现了一个易于使用的完整的系统，这个系统运行在商用服务器上，并且通常会降低实际应用的性能少于10%.我们复制VM执行的方法和Bressoud描述的是相似的，但是我们做了很多重要的设计选择来极大的提高了性能。另外，一个在故障后能自动恢复冗余的易于使用的商用系统需要许多在复制的虚拟机上的额外组件。我们已经设计并实现了这些额外的组件，并且也遇到了一些实际的问题在支持虚拟机运行企业应用的时候。在这篇文章，我们会描述我们的基础设计，并讨论替代的选择和许多实现的细节，也提供了在micro-benchmarks和实际应用上的性能评估。 一、 引言一种常见的实现故障容忍服务器的方法是主备机制，如果主服务器失败，一个备份服务器总是可以进行接管。在任何时间，备份服务器的状态必须和主服务器几乎保持一致，因此当主服务器失败的时候，备份服务器可以立刻接管，此时对于外部客户端而言，故障就相当于被隐藏了起来，并且不会造成数据丢失。在备份服务器上复制状态的一种方法是将主服务器的所有状态，包括CPU、内存、IO设备，连续地送给备份服务器。然而，这种发送状态的方法，尤其是涉及到内存中的变更，其需要的带宽非常大。 另一种可以用更少带宽复制服务器的方法类似于状态机。这种思路是将服务器建模为确定性的状态机，他们从相同的初始状态开始，并且确保以相同的顺序接收相同的输入请求，这样就能保持同步。因为大多数服务器或服务有一些不确定性的操作，因此必须使用额外的协调机制来确保主备同步。然而，需要保持主备一致性的额外信息数目，远远少于正在变更的主服务器上状态（主要是内存更新）的数目。 实现协调机制来确保物理服务器的确定性操作是困难的，尤其随着处理器频率增长。反之，一个运行在虚拟机监视器（hypervisor）上的虚拟机，是一个实现状态机方法的很好的平台。一个虚拟机可以被当作一个定义好的状态机，它的操作是机器被虚拟化的操作（包括它所有的设备）。和物理服务器一样，VM有相同的非确定性操作（例如读取时钟或发送中断），因此为了保持同步，额外的信息必须被发送给备份服务器。虚拟机监视器（hypervisor）有虚拟机的全权控制权利，包括处理所有输入，因此它能够获得所有与主虚拟机上的非确定性操作有关的必要信息，并且能正确地重放这些操作。 因此，这个状态机方法可以通过商业化软件上的虚拟机来实现，它不需要硬件更改，允许在最新的微处理器上立刻实现故障容忍。另外，状态机方法需要的低带宽允许了主备服务器能更好地进行物理分隔。例如，被复制的VM可以运行在横跨一个学校的物理机器上，相比于运行在同一建筑内的虚拟机而言，可以提供更多的可靠性。 我们在VMware vSphere 4.0平台上使用主备机制实现了故障容忍的虚拟机，这个平台以一种高度有效的方式，运行着完全虚拟化的x86虚拟机。因为VMware vSphere实现了一个完整的x86虚拟机，所以我们自动地能够为任何x86操作系统和应用提供故障容忍。这种允许我们记录一个主服务器执行，并确保备份服务器一致执行的基础技术是确定性重放。VMware vSphere Fault Tolerance(FT)是基于确定性重放（Deterministic Replay）的，但是为了建立一个完整的故障容忍系统，还增加了必要的额外协议和功能。除了提供硬件故障容忍，我们的系统在一次失败后，通过在局部集群中任何可接受的服务器上开始一个新的备份虚拟机，进行自动地存储备份。目前确定性重放和VMare FT的产品版本只支持单处理器的虚拟机。多处理器虚拟机的操作记录和重放还在开发中，因为每个共享内存的操作都是一个非确定性的操作，因此还有重要的性能问题待解决。 Bressoud和Schneider描述了一个针对HP PA-RISC平台的故障容忍虚拟机的原型实现。我们的方法是相似的，但是出于性能原因，以及在调查了许多可替代设计后，我们已经做了一些基础性的改变。另外，为了建立一个完整的系统，而这个系统是有效的并且能够被正在运行企业级应用的客户使用，我们已经设计并实现了系统中许多额外的组件，可以处理许多实际问题。与大多数其他实际系统讨论的类似，我们只尝试应付fail-stop类型的故障，这是一种服务器故障，可以在故障服务器造成一次不正确的外部可见行为之前被检测。 文章的剩余部分如下组织。首先，我们描述了我们基础的设计，并且详细说明了我们的基本协议，它能确保在主虚拟机失败后，备份虚拟机能够接替，且不会造成数据丢失。然后，我们描述了许多实际问题的细节，这些问题是为了建立一个健壮的、完整的和自动化系统过程中必须被处理的。我们也描述了几个可用于实现故障容忍虚拟机的设计选择，并且讨论了这些选择的得失。接着，我们给出在一些基准以及实际企业级应用上的性能结果。最后，我们描述相关的工作和结论。 二、 基础的故障容忍设计 图1：基本FT配置。 图1展示了我们系统在故障容忍VM的基本步骤。对于一个给定的VM，我们希望提供故障容忍（主VM），我们在一个完全不同的物理机器上运行一个备份VM，保持和主VM同步并且执行一致，虽然存在短时间的滞后。我们说这两个VM是虚拟锁步的（virtual lock-step）。VM的虚拟磁盘是在一个共享存储中的（例如一个Fibre Channel或者iSCSI磁盘阵列），因此可以接受主备服务器的输入和输出。（我们将在4.1节中讨论带有分隔的非共享虚拟磁盘的主备VM的设计）只有主VM会说明它在网络中的存在，因此所有网络输入都会来到主VM上。相似地，所有其他输入（例如键盘和鼠标）也只会来到主VM上。 所有主VM接收到的输入都会通过名为logging channel的网络连接，被发送到备份VM上。对于几个工作负载而言，主要的输入途径是网络和磁盘。为了保证备份VM和主VM使用相同的方式执行非确定性操作，下面2.1节讨论的额外的信息也需要发送。结果备份VM总是执行和主VM一致的操作。然而，备份VM的输出会被管理程序扔掉，因此只有主VM产生实际输出，并被返回给客户端。和2.2节中描述的一样，为了确保主VM失败后没有数据丢失，主备VM遵循一个具体的协议，包括备份VM明确的确认信息。 为了检测主或备份虚拟机是否失败，我们的系统既使用相关服务器间的心跳机制，同时也监测 logging channel 上的流量。另外，我们我们必须确保只有主或备份VM执行操作，即使存在脑裂（split brain）的场景，在这种场景中主备服务器互相之间会失去通信。 在下面的小节中，我们在几个重要的方面提供更多的细节。在2.1节中，我们给出一些确定性重放技术的细节，保证主备VM通过 logging channel 上的信息保持一致。在2.2节中，我们描述了我们的FT协议中的一个基础规则，保证了主VM失败后没有数据丢失。在2.3节中，我们描述我们的方法，它能够通过正确的方式检测及响应故障。 2.1 确定性重放的实现正如我们已经提到的，复制服务器（或者VM）的操作可以被建模为确定性状态机的复制。如果两个确定性的状态机以相同的初始状态开始，并且以相同的顺序提供确切的输入，它们将经历相同的状态序列并且产生相同的输出。一个虚拟机有很宽泛的输入，包括到来的网络包，磁盘读，以及来自键盘和鼠标的输入。非确定性事件（例如虚拟中断）和非确定性操作（例如处理器的时钟周期计数器）也会影响虚拟机的状态。这显示了对于正在运行任何操作系统和工作负载的任何虚拟机而言，复制执行有三个挑战：（1）为了保证一个备份虚拟机的确定性执行，正确地得到所有输入以及非确定性执行是必要的。（2）正确地将输入与非确定性执行应用到备份虚拟机上。（3）以一种不会降低性能的方式执行。另外，许多在x86处理器上的复杂操作还未被定义，因此会引起非确定性以及副作用。捕获这些未定义的操作并且重放它们产生相同的状态是一个额外的挑战。 针对在VMare vSphere平台上的x86虚拟机，VMware确定性地重放恰好提供了这个功能。确定性重放记录了 VM 的输入以及与 VM执行相关的所有可能的不确定性的日志条目流，这些条目会被写入日志文件。在读取日志文件中的条目后，VM 操作会被精确地重放。 对于非确定性操作，为了允许操作以相同的状态变化和输出再现，需要记录足够的信息。 对于非确定性事件，例如定时器或 IO 完成中断，事件发生的确切指令也会被记录下来。 在重放期间，事件被传递在指令流中的同一位置。 VMware 确定性重放采用各种技术，实现了高效的事件记录和事件传递机制，包括使用AMD和英特尔联合开发的硬件性能计数器。 Bressoud 和 Schneider提到将VM执行切分成不同的时代（epoch），其中非确定性事件，例如中断仅在一个epoch结束时传递。 epoch的概念似乎被用作批处理机制，因为在它发生的确切指令处单独传递每个中断的成本太高。然而，我们的事件传递机制足够高效，以至于 VMware确定性重放不需要使用epochs。 每次中断在发生时被记录，并且在重放时有效地传递到适当的指令处。 2.2 FT协议对于 VMware FT而言，我们使用确定性重放来生成必要的日志条目来记录主VM的执行情况，但是不是将日志条目写入磁盘，而是通过日志通道将它们发送到备份 VM。备份 VM 实时重放日志条目，因此与主 VM 的执行保持一致。 然而，我们必须在日志通道中使用严格的 FT 协议以增强日志条目，从而确保我们实现故障容忍。 我们的基本要求如下： 输出要求：如果备份VM在主VM发生故障后接管，那么备份VM将继续以一种与主虚拟机发送到外部世界的所有输出完全一致的方式执行。 请注意，在发生故障转移后（即备份 VM 需要在主VM故障后接管），备份VM开始执行的方式可能与主 VM 相当不同，因为在执行期间发生了许多非确定性事件。但是，只要备份VM满足输出要求，在故障转移到备份 VM期间没有外部可见状态或数据的丢失，客户端将注意到他们的服务没有中断或不一致。 可以通过延迟任何外部输出（通常是网络数据包）直到备份VM收到所有信息允许它至少执行到该输出操作的节点，来保证上述的输出要求。一个必要条件是备份 VM 必须接收到输出操作之前生成的所有日志条目。这些日志条目将允许它执行到最后一个日志条目的点。但是，假设失败是在主VM执行输出操作后立即发生。备份 VM 必须知道它必须继续重播到输出操作点，并且到那时只能“上线”（停止重播并作为主VM接管，如2.3 节所述）。如果备份将在输出操作之前的最后一个日志条目点上线，一些非确定性事件（例如计时器传递给 VM 的中断）可能会在执行输出操作之前改变其执行路径。 给定上述的限制，强制满足输入要求的最容易的方式是在每个输出操作时创建一个特殊的日志条目。然后，输入要求一定被下面特殊的规则限制： 输出规则：主VM可能不发送一个输出到外部世界，直到备份VM 已收到并确认与产生输出的操作相关的日志条目。 如果备份 VM 已收到所有日志条目，包括生成输出操作的日志条目，然后备份 VM 将能够准确地重现主 VM在输出点的状态，所以如果主VM死了，备份将正确地达到一个与输出一致的状态。相反，如果备份 VM在没有收到所有必要的日志条目的情况下接管，那么它的状态可能会迅速分歧，以至于与主服务器的输出不一致。输出规则在某些方面类似于 [11] 中描述的方法，其中“外部同步” IO 实际上可以被缓存，只要它在下一次外部通信之前确实被写入磁盘了。 请注意，输出规则没有说明关于停止主VM执行的任何事。我们只需要延迟输出发送，但 VM 本身可以继续执行。由于操作系统通过异步中断来指示完成，因此可以执行非阻塞的网络和磁盘输出，VM可以轻松地继续执行并且不一定会立即受到输出延迟的影响。相比之下，以前的工作 [3, 9] 通常必须在执行输出之前完全停止主VM，直到备份 VM 已确认来自主 VM 的所有必要信息。 图2：FT协议。 作为一个例子，我们在图2中展示了 FT 协议的需求。该图显示了一个主VM和备份VM上的事件时间线。从主线到备份线的箭头表示日志条目的传输，从备份线路到主线路的箭头表示确认。有关异步事件、输入和输出操作的信息必须作为日志条目发送到备份VM并确认。如图所示，到外部世界的输出被延迟，直到主VM收到来自备份 VM 的确认，它已经收到与输出操作相关的日志条目。鉴于遵循输出规则，备份VM将能够以这样一种状态接管，即与主VM最后的输出一致。 我们不能保证一旦出现故障转移情况，所有输出都只产生一次。当主VM打算发送输出时，没有使用两阶段提交事务，备份VM无法确定主VM是在发送它的最后一个输出之前还是之后立即崩溃。 幸运的是，网络基础设施（包括常用的TCP）旨在处理丢失的数据包和相同（重复）的数据包。 请注意传入到主VM的数据包也可能在其故障的期间丢失，因此不会被传递给备份VM。 但是，传入的数据包可能会由于与服务器故障无关的任何原因被丢弃，因此网络基础设施、操作系统和应用程序都被写入，以确保他们可以弥补丢失的数据包。 2.3 故障检测与响应如上所述，如果主备VM中的一个显示已经故障，那么另一个必须快速响应。如果备份VM出现故障，主VM将上线，即离开记录模式（因此停止发送条目到日志通道）并开始正常执行。如果主VM失败，备份VM应该同样上线（go live），但过程更为复杂。由于其执行的滞后，备份 VM 可能会有许多它已收到并确认，但尚未消费的日志条目，因为备份 VM 尚未达到执行的适当点。备份VM必须继续重放日志条目，直到它消费了最后一个日志条目。此时，备份 VM 将停止重放模式并开始作为正常VM执行。本质上备份VM被提升为主VM（现在缺少备份VM）。由于它不再是备份 VM，当操作系统执行输出操作时，新的主VM现在将向外部世界生产输出。在过渡到正常模式期间，可能会有一些特定设备的操作需要允许正确地发送输出。特别是，出于联网目的，VMware FT 自动在网络上通告新的主VM的MAC 地址，以便物理网络交换机知道新的主 VM 所在的服务器。此外，新提升的主VM可能需要重做一些磁盘 IO（如第 3.4 节所述）。 有许多可能的方法来尝试检测主备VM的故障。VMware FT在运行容错VMs的服务器之间使用UDP心跳，来检测服务器何时崩溃。此外，VMware FT 监控日志流量，包括从主到备的发送以及从备到主的确认。因为定时器中断，日志流量应该是有规律的，并且永远不会停止。因此，在日志条目或确认流中的中断可能表明VM故障。如果心跳或记录流量已停止超过特定超时时间（大约几秒钟），就可能发生故障了。 但是，任何此类故障检测方法都容易受到脑裂（split brain）问题的影响。如果备份服务器停止接收来自主服务器的心跳，这可能表明主服务器出现故障，或者可能只是意味着所有仍在运行的服务器之间的网络连接丢失。如果备份VM随后上线，而主VM也仍然在运行，对于与VM通信的客户端而言可能会有数据损坏以及其他问题。因此，我们必须确保当检测到故障时，主VM和备份VM只有一个在线。为了避免脑裂问题，我们利用共享存储，来存储VM的虚拟磁盘。 当任一主或备份VM想要上线时，它会在共享存储中执行一个原子性的测试设置操作。 如果操作成功，VM 被允许上线。 如果操作失败，那么另一个 VM 一定已经上线，所以当前虚拟机实际上停止了自己（“自杀”）。 如果尝试执行此原子操作时，VM 无法访问共享存储，然后它只是等待，直到可以访问。 注意如果由于存储网络上的某些故障而无法访问共享存储时，那么虚拟机可能无法做有用的工作，因为虚拟磁盘在同样的共享存储中，因此，为了解决脑裂问题而使用共享存储不会引入任何额外的不可接受性。 这个设计的一个最终方面是一旦故障发生并且一个VM已经上线，VMware FT自动地通过在另一个主机上开始一个新的备份VM，来恢复备份。虽然前面的大部分的工作并未覆盖这个过程，但是它是使故障容忍的VM有效的基础，因此需要谨慎设计。 更多细节将在第 3.1 节中给出。 三、 故障容忍的实际执行第二节描述了我们基础的设计以及FT协议。然而，为了创建一个有用的、健壮的以及自动化的系统，有许多其他组件必须设计实现。 3.1 启动与重启 FT VMs最大的附加组件之一必须被设计为如下机制，即启动备份VM时它将拥有和主VM一样的状态。当故障发生后重启一个备份VM时，这个机制也将变得很有用。因此，这个机制一定可用于一个处于任意状态的正在运行中的主VM。此外，我们希望该机制不会显著地中断主VM的执行，因为这会影响 VM 的任何当前客户端。 对于 VMware FT而言，我们调整了VMware vSphere上现有的 VMotion 功能。 VMware VMotion [10] 允许以最少中断的方式，将正在运行的 VM 从一台服务器迁移到另一台服务器，VM的暂停时间通常不到一秒钟。我们创建了一个VMotion的修改形式，可在远程服务器上创建准确的 VM 运行副本，但不会破坏本地服务器的虚拟机。也就是说，我们修改后的 FT VMotion 将VM克隆到远程主机上而不是迁移它。 FT VMotion还设置了一个日志记录通道，并导致源VM作为主VM进入日志记录模式，而目的VM 作为备份进入重放模式。像平常的VMotion一样，FT VMotion 通常会中断主VM的执行不到一秒。因此，启用 FT在正在运行的 VM 上是一个简单的、无中断的操作。 启动备份 VM 的另一个方面是选择一个运行它的服务器。容错 VM 在服务器集群中运行，可以访问共享存储，因此所有 VM通常可以运行在集群上的任何服务器中。这种灵活性允许VMware vSphere恢复FT冗余，即使一个或多个服务器失效。VMware vSphere 实现了一种集群服务，用于维护管理以及资源信息。 当发生故障并且主VM 现在需要一个新的备份 VM 来重新建立冗余时，主 VM 通知集群服务它需要一个新的备份。 集群服务基于资源利用率以及其他约束，决定运行备份VM最好的服务器，并调用 FT VMotion 以创建新的备份 VM。 结果是 VMware FT通常可以在几分钟内重新建立VM冗余，在一个故障容忍VM的执行上，所有这些都没有任何明显的中断。 3.2 管理日志通道 图3：FT日志缓存和通道。 在管理日志通道上的流量时，有许多有趣的实现细节。在我们的实现中，管理程序为主备 VM 的日志记录条目维持了一个大的缓冲区。当主 VM 执行时，它生成日志条目到缓冲区中，类似地，备份VM从它的日志缓冲区中消耗日志条目。主日志缓冲区的内容会被尽快刷新到日志记录通道，这些日志条目一到日志通道，就会被读取到备份的日志缓冲区。备份每次从网络上读取一些日志条目到它的日志缓冲区时，都会发送确认返回给主VM。这些确认允许 VMware FT 确定一个被输入规则延迟的输出何时可以被发送。图3说明了这个过程。 如果备份 VM 在需要读取下一个日志条目时，遇到空的日志缓冲区，它将停止执行直到有新的日志条目可用。由于备份 VM 是不与外部通信的，此暂停不会影响任何VM 的客户端。同样地，当主VM需要写入一个日志条目时，如果主VM遇到一个完整的日志缓冲区，它必须停止执行，直到可以刷新日志条目。这种执行的停止是一种自然的流控制机制，当主VM生产日志条目太快了，它会减慢主VM。但是，此暂停可能会影响VM的客户端，因为主 VM 将完全停止并且无响应，直到它可以记录其条目并继续执行。因此，我们的实现必须设计为尽量减少主日志缓冲区填满的可能性。 主日志缓冲区可能填满的原因之一是备份 VM 执行速度太慢，因此消耗日志条目太慢。一般来说，备份VM必须能够以与正在记录执行的主VM大致相同的速度重放执行。幸运的是，在 VMware 确定性重放中，记录和重放的开销大致相同。然而，如果由于其他VMs，托管备份 VM 的服务器负载很重（因此过度使用资源），备份VM 可能无法获得足够的 CPU 和内存资源，来与主 VM 一样快地执行，尽管备份管理程序的VM调度器已经尽了最大努力。 如果日志缓冲区填满，除了避免意外暂停，还有另一个原因是我们不希望滞后变得太大。如果主VM出现故障，备份VM 必须通过重放它在上线和开始与外部世界交流之前已经确认的所有日志条目来“赶上”。完成重放的时间基本上是失败点的执行延迟时间，所以备份上线的时间大约等于故障检测时间加上当前执行时差。因此，我们不希望执行滞后时间太大（超过一秒），因为这将显著地增加故障转移时间。 因此，我们有一个额外的机制减慢主VM，以防止备份 VM 获取太滞后了。在我们的发送和确认日志条目的协议中，我们发送附加信息来确定主备VM之间的实时执行滞后。通常执行滞后小于 100 毫秒。如果备份 VM 有一个显著的执行滞后（例如，超过 1 秒），VMware FT 通过通知调度程序给它稍微少一点的CPU（最初只是百分之几）来减慢主 VM。我们使用一个缓慢的反馈循环，这将尝试逐步确定适当的 CPU 限制，将允许主备 VM同步执行。如果备份 VM 继续滞后，我们继续逐步降低主VM的 CPU 限制。反之，如果备份VM赶上，我们逐渐增加主VM的 CPU 限制，直到备份虚拟机恢复轻微的滞后。 请注意，主VM的这种减速很少见，通常只在系统处于低压力时发生。第 5 节的所有性能数都包括任何此类放缓的成本。 3.3 FT VMs上的操作另一个实际问题是处理各种控制操作，它们可以应用于主 VM 。例如，如果主VM明确关闭电源，备份 VM 也应该停止，而不是尝试上线。 再举一个例子，任何主VM上的资源管理更改（例如增加 CPU 份额）应该 也适用于备份。 对于此类操作，为了影响备份进行合适的操作，特殊的控制条目通过日志通道从主发送到备份。 一般来说，VM 上的大部分操作都应该仅在主 VM 上初始化。 VMware FT 然后发送任何必要的控制条目以造成备份VM上适当的更改。 唯一可以独立在主VM和备份VM上完成的操作是 VMotion。 也就是说，主VM和备份VM可以独立被 VMotioned到其他主机。 请注意，VMware FT 确保两个 VM 都不会移动到另一个 VM 所在的服务器，因为这种场景将不再提供故障容忍。 主VM的VMotion增加了比普通VM更多的复杂性，因为备份VM一定会与源主VM失去连接以及在适当的时间重连。备份VM的VMotion有一个相似的问题，但是只增加了一个额外的复杂性。对于一个正常的VMotion而言，我们需要当VMotion上最后的切换发生时，所有的磁盘IO停止（或完成）。对于一个主VM而言，这种停顿可以通过等待，直到物理IO完成并将这些完成信息发送给VM轻易解决。然而，对于一个备份VM而言，没有容易的方式来使得所有IO在任何需要的时刻完成，因为备用VM必须重放主VM的执行过程，并在相同的执行点完成IO。主VM可能正运行在一个工作负载上，在正常执行过程中总是有磁盘IO。VMware FT有一个独一无二的方法来解决这个问题。当一个备份VM是在VMotion最后的切换点时，它需要通过日志通道来告知主VM临时停止所有IO。备份VM的IO将自然地被停止在一个单独的执行点，因为它需要重放主VM的停止操作的过程。 3.4 磁盘IO的实现问题有许多与磁盘IO相关的微小的实现问题。首先，假设磁盘操作是非阻塞的，因此访问相同磁盘位置的并行、同时执行的磁盘操作将引起非确定性。此外，我们的磁盘 IO 实现使用DMA 直接from/to虚拟机的内存，所以同时访问相同内存页的磁盘操作也可能导致不确定性。我们的解决方案是经常检测任何此类 IO 竞争（很少见），以及强制此类竞争磁盘操作在主备VM上按顺序执行。 第二，通过 VM 中的应用程序（或操作系统）时，磁盘操作与内存访问也会存在竞争，因为磁盘操作通过 DMA 直接访问 VM 的内存。例如，如果一个VM 中的应用程序/操作系统正在读取内存块，同时对该块进行磁盘读取。这个情况也不太可能发生，但如果它发生，我们必须检测它并处理它。一种解决方案是临时设置页保护，在作为磁盘操作目标的页面上。如果VM 碰巧访问一个页，同时该页面也是磁盘操作的目标，页保护将导致一个陷阱，VM将暂停直到磁盘操作完成。因为改变页上的MMU 保护是一项昂贵的操作，所以我们选择使用弹跳缓冲区（Bounce Buffer）代替。bounce buffer是临时缓冲区，与正在被磁盘操作访问的内存大小相同。磁盘读取操作被修改为读取指定数据到bounce buffer，并在在IO完成时将数据复制到内存中。相似地，对于磁盘写操作，首先将要发送的数据复制到bounce buffer，磁盘写入修改为向bounce buffer写入数据。bounce buffer的使用会减慢磁盘操作，但我们还没有看到它会导致任何明显的性能损失。 第三，有一些与故障发生并且备份VM接管时，主VM未完成的磁盘 IO 相关的问题。对于新上线的主VM，没有办法确定磁盘IO是有问题的还是成功完成了。另外，由于磁盘IO没有从外部发布到备用VM上，而是通过主备传递，因此对于继续运行的新上任的主VM来说，将没有明确的IO完成信息，最终将导致VM上的操作系统开始中止或者重调度程序。我们能够发送一个错误完成，表示每个IO失败，因为即使IO成功完成了，它可以接受返回一个错误。然而，操作系统可能不能对这些来自本地磁盘的错误有很好的响应。反之，我们在备份VM上线的过程中，重新发送这些挂起的IO。因为我们已经限制了所有的竞争和所有的直接指定内存和磁盘的IO，这些磁盘操作可以被重新发送，即使它们已经成功完成了（即他们是幂等的）。 3.5 网络IO的实现问题VMware vSphere针对VM网络提供了很多性能优化。一些优化是基于管理程序（supervisor）异步更新虚拟机的网络设备状态。例如，当VM正在执行时，接收缓冲区可以由管理程序直接更新。不幸的是这些对 VM 状态的异步更新会增加不确定性。除非我们可以保证所有更新都发生在主备指令流上的同一点，否则备份VM的执行可能与主VM的执行不同。 对于FT而言，网络仿真代码的最大变化是禁用异步网络优化。异步更新带有传入数据包的VM环形缓冲区的代码已被修改，以强制管理程序捕获到操作系统，它可以在其中记录更新然后将它们应用到 VM。同样，异步地将数据包从传输队列中拉出也被修改了，取而代之的是通过管理程序traps来完成传输（如下所述）。 网络设备异步更新的消除结合第 2.2 节中描述的发送数据包的延迟带来了一些网络性能的挑战。我们采取了两种方法在运行 FT 时提高 VM 的网络性能。第一，我们实施了集群优化以减少 VM 的陷阱和中断。当 VM 以足够的比特率流式传输数据时，管理程序可以对每组数据包做一个传输trap，在最好的情况下零trap，因为它可以传输所接收新数据包的一部分数据包。同样地，通过仅对于一组数据包发布中断，管理程序可以将接收包的中断数量减少。 我们对网络的第二个性能优化涉及减少传输数据包的延迟。如前所述，管理程序必须延迟所有发送的包直到它得到备份VM对于某些日志条目的确认。减少发送延迟的关键在于减少发送/接收备份VM信息的所需时间。我们的主要优化包括保证收发信息在无需任何线程上下文切换的情形下就可以被执行。VMware vSphere管理程序允许函数被注册到TCP栈中，只要TCP数据被接收到了，函数就会被一个延期执行的上下文调用（和Linux中的tasklet类似）。这允许我们快速处理备份VM上任何即将到来的日志消息，以及主VM接收的任何确认消息，而不需要任何线程上下文的切换。另外，当主VM有一个包要寄出去时，我们强制一次相关输出日志条目的日志刷出（正如2.2节中所描述的），通过调度一个延迟执行的上下文来执行这次刷出。 四、 可供选择的设计4.1 共享 vs. 非共享磁盘 图4：FT非共享磁盘配置。 在我们默认的设计中，主备VM共享相同的虚拟磁盘。因此，如果一次故障转移发生，共享磁盘的内容自然是正确、可接受的。必要地，对于主备VM来说，共享磁盘被认为是外部的，因此任何共享磁盘的写入被认为是一次与外部世界的沟通。因此，只有主VM做这种实际的磁盘写入，并且为了遵循输出规则，这种写入必须被延迟。 对于主备VM而言，一种可替代的选择是分隔（非共享）的虚拟磁盘。在这种设计中，备份VM要执行所有虚拟磁盘的写入操作。而且这样做的话自然要保持它的虚拟磁盘内容与主VM虚拟磁盘内容一致。图4阐述了这种配置。在非共享磁盘的情况下，虚拟磁盘必须被认为是每个VM的内部状态。因此，依据输出规则，主VM的磁盘写入不必延迟。在共享存储不能被主备VM接受的情况下，非共享的设计是相当有用的。这种情况可能是由于共享存储不可接受或者太昂贵，或者由于运行主备VM的服务器相隔太远（“长距离FT”）。非共享设计的一个缺点是在首次启动故障容错时，虚拟磁盘的两个复制必须以相同的方式进行显示同步。另外，发生故障后磁盘可能会不同步，因此当在一次失败后备份VM重启的时候，他们必须再显式地同步。FT VMotion必须不止同步主备VM的运行状态，还要同步他们的磁盘状态。 在这种非共享磁盘的配置中，他们也能应付脑裂场景。在这种场景中，系统能够使用一些其他的外部决策者，例如所有服务器可以沟通的一个第三方服务。如果服务器是超过两个节点的集群的一部分，这个系统能够基于集群关系使用一种majority算法。在这个例子中，一个VM能够被允许上线，如果它正在一个服务器上运行，这个服务器是包含大多数原始节点的正在通信的子集群的一部分。 4.2 在备份VM上执行磁盘读在我们默认的设计中，备份的VM从不会从它自己的虚拟磁盘上读取（无论共享还是非共享）。因为磁盘读取被认为是一个输入，它是自然地通过日志通道将磁盘读取的结果发送到备份VM上。 一种替代的设计是让备份VM执行磁盘读取，因此消除了磁盘读取的日志。对于大多数时候都做磁盘读取的工作负载而言，这种方法可以很好地降低日志通道上的流量。然而，这种方法有很多小问题。它可能会减慢备份VM的执行速度，因为备份VM必须执行所有的磁盘读取，当到达VM执行中主VM已经完成的位置时，如果备份上的磁盘读取还没完成就必须等待。 同样地，为了处理失败的磁盘读取操作，必须做一些额外的工作。如果一个主VM的磁盘读取成功了，但是相应的备份VM磁盘读取失败了，备份VM的磁盘读取必须重试直到成功。因为备份VM必须获得和主VM一样的数据到内存中。相反地，如果一个主VM的磁盘读取失败了，目标内存的内容必须通过日志通道发送给备份服务器，因此内存的内容将被破坏，不能被备份VM成功的磁盘读取复制。 最后，如果这种磁盘读取被用于共享磁盘配置的话，还有一个小问题。如果主VM做了一次对具体磁盘位置的读取，然后紧跟相同磁盘位置的写入，然后这个磁盘写必须被延迟到备份VM已经执行了第一次磁盘读取。这种依赖可以被检测和正确处理，但是需要增加实现上额外的复杂性。 在5.1节中，对于实际的应用而言，我们给出一些性能结果以表示在备份VM上执行磁盘读取会造成一些轻微的吞吐量减少(1-4%)，因此在日志通道的带宽被限制的情况下，在备份VM上执行磁盘读取可能是有用的。 五、 性能评估在这节中，我们做了一次VMware FT性能的基础评估，针对许多应用负载以及网络基准。为了得到这些结果，我们在一样的服务器上运行主备VM，每个都带9个Intel Xeon 2.8Ghz CPU 和 8Gbytes 的 RAM。服务器间通过10 Gbit/s的交换机连接，但是在所有的例子中都能看到被使用的网络带宽远远少于1Gbit/s。从一个通过标准的4Gbit/s的光纤通道网络连接的EMC Clariion中，服务器可以连接他们的共享虚拟磁盘。客户端通过1 Gbit/s的网络来驱动一些连接服务器的工作负载。 我们评估性能结果的应用如下所示。SPECJbb2005是工业标准的Java应用基准，非常耗费CPU和内存，但是IO非常少。Kernel Compile是一种运行Linux核编译的工作负载。由于许多编译过程的创建和毁灭，这个工作负载做很多磁盘读取和写入，是非常耗费CPU和MMU的。Oracle Swingbench是被Swingbench OLTP工作负载（在线事务处理）驱动的一个Oracle 11g的数据库。这个工作负载做连续的磁盘和网络IO，有80个同时在线的数据库会话。MS-SQL DVD Store是一种工作负载，运行了一个Microsoft SQL Server 2005的数据库，有60个同时在线的客户端。 5.1 基本性能结果 表1：基本性能结果。 表 1 列出了基本的性能结果。对于每个应用程序，第二列给出了应用程序的性能比例，运行服务器工作负载的虚拟机上启用和未启用FT的情况。性能比小于 1 表示带FT的工作负载更慢。显然，这些有代表性的工作负载上启用FT 的开销小于10%。 SPECJbb2005 完全受计算限制，没有空闲时间，但其表现性能良好，因为它具有最小的除定时器中断以外的不确定性事件。另一个工作负载做磁盘 IO 有一些空闲时间，所以一些FT 开销可能被 FT虚拟机的空闲时间更少的真实情况隐藏。然而，一般的结论是VMware FT 能够支持故障容忍VM，并且具备相当低的性能开销。 在表的第三列中，我们给出了当应用程序正在运行时，在日志通道上发送数据的平均带宽。对于这些应用程序，日志带宽相当合理，1 Gbit/s的网络就能满足 。事实上，低带宽要求表明多个 FT 工作负载可以共享相同的 1 Gbit/s网络，同时没有任何负面的性能影响。 对于运行常见操作系统的 VM，例如Linux 和 Windows，我们发现当操作系统空闲时，通常的日志记录带宽为 0.5-1.5 Mbits/sec。”空闲”带宽主要是记录定时器中断发送的结果。对于具有活动中工作负载的 VM而言，日志带宽由网络和必须发送到备份的磁盘输入主导—网络收到的数据包和从磁盘读取的磁盘块。因此，对于非常高的网络接收或者磁盘读取带宽的应用而言，日志带宽高于表1中的测量值。对于这类应用而言，日志通道的带宽可能是瓶颈，特别是日志通道还有其他使用时。 对于许多实际应用程序而言，日志记录所需的带宽相对较低，这使得基于重放的故障容忍对于使用非共享磁盘的长距离配置非常有吸引力。对于远距离配置而言，其主备VM可能相隔1-100公里，光纤可以轻松地支持延迟小于 10 毫秒的100-1000 Mbit/s带宽。对于表 1 中的应用而言，主备之间的额外往返延迟，可能会导致网络和磁盘输出最多延迟 20 毫秒。远距离配置仅适用于这类应用程序：他的客户端可以容忍每个请求的额外延迟。 对于两个最占用磁盘空间的应用程序，我们测量了在备份 VM上执行磁盘读取（如第 4.2 节所述）与通过日志记录通道发送磁盘读取数据相比，对于性能的影响。对于 Oracle Swingbench来说，在备份VM上执行磁盘读取时的吞吐量降低约 4%；对于 MS-SQL DVD 存储，吞吐量约降低 1%。同时，Oracle Swingbench的日志带宽从 12 Mbits/sec 降低到 3 Mbits/sec，MS-SQL DVD 存储从 18 Mbits/sec 降低到 8 Mbits/sec。显然，对于具有更大磁盘读取带宽的应用程序，带宽可能会节省很多。如第 4.2 节所述，预计在备份 VM 上执行磁盘读取时，性能可能会更差。但是，对于日志通道的带宽是有限的（例如，远程配置）情况下，在备份 VM 上执行磁盘读取可能有用。 5.2 网络基准测试 表2：日志通道对网络传输性能的影响。 出于多种原因。网络基准测试对我们的系统来说非常具有挑战性。第一，高速网络会有一个非常高的中断率，这需要以非常高的速度记录和重放异步事件。 第二，以高速率接收数据包的基准将导致高速率的日志流量，因为所有这些数据包必须通过日志通道发送到备份。第三，发送数据包的基准测试将受制于输出规则，延迟网络数据包的发送直到已收到来自备份VM的确认。 此延迟会增加对客户端测量的延迟。这种延迟还可能会降低到客户端的网络带宽，因为网络协议（如 TCP）由于往返延迟增加，可能不得不降低网络传输速率。 表 2 给出了我们通过标准的netperf 基准测试，多次测量的结果。在所有这些测量中，客户端 VM 和主 VM 通过 1 Gbit/s 网络连接。前两行给出了主备主机间通过1 Gbit/s 的日志通道连接时，发送和接收的性能。第三行和第四行给出当主备服务器通过10 Gbit/s的日志通道连接时，发送和接收的性能，不仅带宽更高，延迟也低于 1 Gbit/s。作为一个粗略的测量，在1 Gbit/s 网络连接的管理程序之间， ping 时间约为 150 微秒，而对于 10 Gbit/s 连接，ping时间大约需要 90 微秒。 未启用 FT 时，主 VM 对于接收和发送，可以实现接近 (940 Mbit/s) 1 Gbit/s 的线路传输速率。当为接收工作负载启用 FT 时，日志记录带宽非常大，因为所有传入的网络数据包必须在日志通道上发送。因此，日志记录通道可能成为瓶颈，正如1 Gbit/s 日志网络的结果。对于 10 Gbit/s 的日志网络，影响则小了很多。当为上传工作负载启用 FT 时，上传数据包的数据不会记录，但仍必须记录网络中断。日志带宽要低得多，因此可实现的网络上传带宽高于网络接收带宽。总的来说，我们看到 FT 在非常高的上传和接收速率情况下，可以显著地限制网络带宽，但仍然可以实现很高的速率。 六、 相关工作Bressoud 和 Schneider [3] 描述了实施的最初想法，通过完全包含在管理程序级别的软件对虚拟机进行故障容忍。他们展示了保持一个备份虚拟机的可行性，该备份通过配备 HP PA-RISC 处理器的服务器原型与主虚拟机同步。但是，由于PA-RISC 架构的限制，他们无法实现完全安全、隔离的虚拟机。此外，他们没有实现任何故障检测方法，也没有尝试解决第 3 节中描述的任何实际问题。更重要的是，他们对他们的 FT 协议提出的很多限制是不必要的。首先，他们强加了epoch的概念，其中异步事件被延迟到设定的时间间隔结束。一个epoch的概念是不必要的—他们可能强加了它，因为他们无法足够有效地重放单个异步事件。其次，他们要求主虚拟机基本上停止执行，直到备份收到并且确认所有以前的日志条目。然而，只有输出本身（例如网络数据包）必须延迟 –主 VM 本身可能会继续执行。 Bressoud [4] 描述了一个在操作系统（Unixware）中实现故障容忍的系统，因此为在该操作系统上运行的所有应用程序提供容错。系统调用接口变成了必须确定性地复制的一组操作。这项工作与基于管理程序的工作有着相似的限制与设计选择。 纳珀等人 [9] 以及 Friedman 和 Kama [7] 描述了故障容忍 Java 虚拟机的设计。他们在日志通道中发送输入与非确定性操作时遵循与我们类似的设计。像 Bressoud 一样，他们似乎并不专注于检测故障并在故障后重新建立容错。此外，它们的实现仅限于对在 Java 虚拟机中运行的应用程序提供故障容忍。这些系统试图处理多线程Java 应用程序的问题，但要求所有数据都正确地受锁保护或强制执行序列化到共享内存。 邓拉普等 [6] 描述了确定性重放的实现，主要针对在半虚拟化系统上调试应用软件。我们的工作支持在虚拟机内运行的任何操作系统并实现了对这些 VM 的容错支持，这需要更高水平的稳定性和性能。 库利等人[5] 描述了一种支持故障容忍VMs的替代方法，并且在一个名为Remus的项目里实现了。通过这种方法，在执行期间主VM的状态被反复检查，并且被发送到备份服务器，该服务器收集检查点信息。检查点必须非常频繁地执行（每秒多次），因为外部输出必须延迟到下一个检查点被发送和确认。这种方法的优点是它同样适用于单处理器和多处理器 VM。 这种方法的主要问题是有非常高的网络带宽需要，以将每个检查点内存状态的增量更改发送出去。 Remus 的结果[5] 显示，对于发送内存状态的改变，当使用一个1 Gbit/s 网络连接尝试每秒做40个检查点时，内核编译与SPECweb 基准测试减速 100% 到 225%。有许多优化可能有助于减少所需的网络带宽，但不清楚1 Gbit/s 连接是否可以实现合理的性能。相比之下，我们基于确定性重放的方法可以实现低于 10% 的开销，在几个真实应用中主备主机所需的带宽远远少于 20 Mbit/s。 七、 结论及未来工作我们在VMware vSphere 中设计并实施了一个高效完整的系统(FT) ，用于为服务器上运行的虚拟机提供容错。我们的设计基于复制主VM中的执行，再通过另一台主机上的备份VM执行VMware确定性重放。如果运行主 VM的服务器出现故障，备份 VM 能立即接管且不会中断或丢失数据。 总体而言，在商业硬件上运行VMware FT时，故障容错VM的性能非常出色，并且对于某些典型应用程序，其开销低于 10%。大多数 VMware FT 的性能成本来自于使用 VMware 确定性重放来保持主备VM同步。因此，VMware FT 的低开销源自 VMware 确定性重放的效率。此外，保持主备同步所需的日志带宽非常小，通常小于 20 Mbit/s。因为日志带宽在大多数情况下很小，主备相隔很长的距离（1-100公里）似乎也是可行的实施配置。因此，VMware FT 可用于这种场景：可以防止整个站点发生故障的灾难。值得注意的是，日志流通常是可压缩的，因此简单的压缩技术可以显著地减少日志带宽，虽然有少量额外的 CPU 开销。 我们对 VMware FT 的结果表明，一个高效的故障容错VM的实现可以建立在确定性重放的基础上。这样的系统可以透明地为运行任何操作系统和应用的虚拟机提供容错能力，仅会带来极小的开销。然而，对客户有用的故障容错VM系统而言，它必须还具有强大、易于使用和高度自动化的特点。一个可用的系统除了复制虚拟机执行之外，还需要许多其他组件。特别是VMware FT 故障后自动地恢复冗余，通过在本地集群中找到合适的服务器并在其上创建一个新的备份VM。通过解决所有必要的问题，我们已经展示了一个在客户的数据中心可用于实际应用的系统。 通过确定性重放实现容错的权衡之一是当前确定性重放仅针对单处理器VM 。然而，单处理器虚拟机足够应付各种各样的工作负载，特别是因为物理处理器不断变得更加强大。此外，许多工作负载可以通过使用许多单处理器的虚拟机来扩展，而不是通过使用一个更大的多处理器虚拟机来扩展。多处理器 VM 的高性能重放是一种活跃的研究领域，并且可以潜在地被微处理器中的一些额外硬件支持。一个有趣的方向可能是扩展事务内存模型以促进多处理器重放。 将来，我们也有兴趣扩展我们的系统处理部分硬件故障。通过部分硬件故障，我们的意思是服务器上功能或冗余的部分丢失，不会导致损坏或丢失数据。一个例子是到 VM所有网络连接的丢失，或在物理服务器中备用电源丢失。如果在运行主 VM 的服务器上发生部分硬件故障，在许多情况下（但不是all) 故障转移到备份 VM 将是有利的。这样的故障转移对于关键VM而言，可以立即恢复完整服务，并确保虚拟机从可能不可靠的服务器上快速地移出。 References[1] Alsberg, P., and Day, J. A Principle for Resilient Sharing of Distributed Resources. In Proceedings of the Second International Conference on Software Engineering (1976), pp. 627–644. [2] AMD Corporation. AMD64 Architecture Programmer’s Manual. Sunnyvale, CA. [3] Bressoud, T., and Schneider, F. Hypervisor-based Fault Tolerance. In Proceedings of SOSP 15 (Dec. 1995). [4] Bressoud, T. C. TFT: A Software System for Application-Transparent Fault Tolerance. In Proceedings of the Twenty-Eighth Annual International Symposium on FaultTolerance Computing (June 1998), pp. 128–137. [5] Cully, B., Lefebvre, G., Meyer, D., Feeley, M., Hutchison, N., and Warfield, A. Remus: High Availability via Asynchronous Virtual Machine Replication. In Proceedings of the Fifth USENIX Symposium on Networked Systems Design and Implementation (Apr. 2008), pp. 161–174. [6] Dunlap, G. W., King, S. T., Cinar, S., Basrai, M., and Chen, P. M. ReVirt: Enabling Intrusion Analysis through Virtual Machine Logging and Replay. In Proceedings of the 2002 Symposium on Operating Systems Design and Implementation (Dec. 2002). [7] Friedman, R., and Kama, A. Transparent Fault-Tolerant Java Virtual Machine. In Proceedings of Reliable Distributed System (Oct. 2003), pp. 319–328. [8] Intel Corporation. IntelAˆR 64 and IA-32 Architectures Software Developer’s Manuals. Santa Clara, CA. [9] Napper, J., Alvisi, L., and Vin, H. A Fault-Tolerant Java Virtual Machine. In Proceedings of the International Conference on Dependable Systems and Networks (June 2002), pp. 425–434. [10] Nelson, M., Lim, B.-H., and Hutchins, G. Fast Transparent Migration for Virtual Machines. In Proceedings of the 2005 Annual USENIX Technical Conference (Apr. 2005). [11] Nightingale, E. B., Veeraraghavan, K., Chen, P. M., and Flinn, J. Rethink the Sync. In Proceedings of the 2006 Symposium on Operating Systems Design andImplementation (Nov. 2002). [12] Schlicting, R., and Schneider, F. B. Fail-stop Processors: An Approach to Designing Fault-tolerant Computing Systems. ACM Computing Surveys 1, 3 (Aug.1983), 222–238. [13] Schneider, F. B. Implementing fault-tolerance services using the state machine approach: A tutorial. ACM Computing Surveys 22, 4 (Dec. 1990), 299–319. [14] Stratus Technologies. Benefit from Stratus Continuing Processing Technology: Automatic 99.999% Uptime for Microsoft Windows Server Environments. At http://www.stratus.com/pdf/whitepapers/continuous-processing-for-windows.pdf, June 2009. [15] Xu, M., Malyugin, V., Sheldon, J., Venkitachalam, G., and Weissman, B. ReTrace: Collecting Execution Traces with Virtual Machine Deterministic Replay. InProceedings of the 2007 Workshop on Modeling, Benchmarking, and Simulation (June 2007).","link":"/2022/09/30/The-Design-of-a-Practical-System-for-Fault-Tolerant-Virtual-Machines/"},{"title":"The Go Memory Model","text":"Go内存模型Version of June 6, 2022 介绍Go 内存模型指定了以下条件,来保证在某一个 goroutine 中对一个变量进行读操作时,可以观察到在不同的 goroutine 中对该变量进行写操作产生的值。 建议程序中改变的数据会被多个 goroutine 同时访问时,必须序列化此类访问。 若要序列化访问,请使用通道操作或其他同步原语保护数据,例如 sync 和 sync/atomic 中的那些。 如果你必须阅读本文档的其余部分才能了解你的程序的行为, 那你可太聪明了。 不要自作聪明。 非正式概述Go 以与语言其他部分大致相同的方式处理其内存模型, 旨在保持语义简单、易懂和有用。 本节概述了该方法,对于大多数程序员来说应该足够了。 内存模型将在下一节中更正式地阐明。 数据竞争定义为在对内存某一位置写入时,同时发生了对同一位置的另一个读取或写入,除非涉及的所有访问都是 sync/atomic 包提供的原子数据访问。 如前所述,强烈建议程序员使用适当的同步 以避免数据竞争。 在没有数据竞争的情况下,Go 程序的行为就像所有的 goroutine 被多路复用到单个处理器上。 虽然程序员应该编写没有数据竞争的 Go 程序,Go 实现在响应数据竞争时可以做什么是有限制的。 实现可能始终通过报告争用和终止程序来响应数据争用。否则,每次读取单个字大小或子字大小的内存位置 必须观察实际写入该位置的值（可能是由并发执行的 goroutine） 并且尚未重写。 这些实现约束使 Go 更像 Java 或 JavaScript, 由于大多数竞争的结果有限, 不像 C 和 C++,其中任何带有竞争的程序的含义 完全未定义,编译器可能做出任何事情。 Go的方法旨在使错误的程序更可靠,更易于调试, 同时仍然坚持认为竞争是错误的,工具可以诊断和报告它们。 内存模型以下 Go 内存模型的正式定义沿袭了 Hans-J. Boehm 和 Sarita V. Adve 发表于 PLDI 2008 的 “Foundations of the C++ Concurrency Memory Model”。data-race-free 程序的定义和race-free程序顺序一致性的保证与他们的这一作品中相同。 内存模型描述了程序执行的要求, 程序执行由 goroutine 执行组成, goroutine 执行又由内存操作组成。 内存操作由四个细节建模： 它的种类,表明它是普通数据读取,还是普通数据写入,或同步操作,例如原子数据访问, 互斥操作或通道操作, 它在程序中的位置, 正在访问的内存位置或变量,以及 该操作读取或写入的值。某些内存操作是类似读取的，包括读取、原子读取、互斥锁和通道接收。 其他内存操作类似写入，包括写入、原子写入、互斥解锁、通道发送和通道关闭。 有些，例如原子比较和交换，既是类似读的，也是类似写的。 goroutine 执行被建模为由单个 goroutine 执行的一组内存操作。 要求 1： 每个 goroutine 中的内存操作必须对应于该 goroutine 的正确顺序执行，给定从内存中读出和写入的值。该执行必须符合先序关系，定义为Go语言规范中Go的控制流结构所规定的部分顺序要求，以及表达式的评估顺序。 要求2：对于一个给定的程序执行，映射W，当限于同步操作时，必须可以用一些隐含的同步操作的总顺序来解释，该顺序与排序和这些操作所读写的值一致。 如果一个同步的读类存储器操作r观察到一个同步的写类存储器操作w（也就是说，如果W(r)=w），那么w就同步于r。 发生在前的关系被定义为顺序在前和同步在前关系的联合的反式闭合。 要求3：对于一个内存位置x上的普通（非同步）数据读r，W(r)必须是一个对r可见的写w，其中可见意味着以下两个条件都成立。 w发生在r之前。 w不会发生在r之前发生的任何其他写w’（对x）之前。 内存位置x上的读写数据竞争由x上的读类内存操作r和x上的写类内存操作w组成，其中至少有一个是非同步的，这两个操作在发生之前是无序的（也就是说，r发生在w之前，w也没有发生在r之前）。 内存位置x上的写-写数据竞争由x上的两个类似写的内存操作w和w’组成，其中至少有一个是不同步的，它们在发生之前是无序的。 请注意，如果内存位置x上没有读写或写数据竞争，那么x上的任何读r只有一个可能的W(r)：在发生之前的顺序中紧接在它之前的单一w。 更一般地说，可以证明任何Go程序是 data-race-free的，也就是说，它没有存在读写数据竞争的程序执行，其结果只能由一些顺序一致的goroutine执行交错来解释。(证明方法与上文引用的Boehm和Adve的论文第7节相同。)这个属性被称为DRF-SC。 正式定义的意图是与其他语言（包括C、C++、Java、JavaScript、Rust和Swift）为race-free程序提供的DRF-SC保证相匹配。 Go语言的某些操作，如 goroutine 创建和内存分配，都是同步操作。这些操作对synchronized-before部分顺序的影响在下面的 “同步”部分有记录。各个包负责为自己的操作提供类似的文档。 包含数据竞争的程序的实现限制上一节给出了data-race-free程序执行的正式定义。本节非正式地描述了实现必须为包含竞争的程序提供的语义。 首先，任何实现都可以在检测到数据竞争时，报告该竞争并停止程序的执行。使用ThreadSanitizer（通过 “go build -race “访问）的实现正是这样做的。 否则，对不大于一个机器字的内存位置x的读取必须观察到一些写w，使得r在w之前不会发生，并且没有写w’，使得w在w’之前发生，w’在r之前发生。 此外，不允许观察无因的和 “凭空 “的写入。 大于一个机器字的内存位置的读取被鼓励，但不需要满足与字大小的内存位置相同的语义，观察一个允许的写w。由于性能的原因，实现可以将更大的操作作为一组单独的机器字大小的操作，以一个未指定的顺序。这意味着多字数据结构上的竞争可能导致不一致的值，而不是对应于单一的写。当这些值依赖于内部（指针、长度）或（指针、类型）对的一致性时，如大多数Go实现中的接口值、map、切片和字符串，这种竞争又会导致任意的内存损坏。 下面的 “不正确的同步 “部分给出了不正确的同步的例子。 对实现的限制的例子在下面的 “不正确的编译”一节中给出。 同步初始化程序初始化在一个 goroutine 中运行，但这个 goroutine 可以创建其他 goroutine ，这些 goroutine 同时运行。 如果一个包p导入了包q，那么q的init函数的完成会在p的任何一个函数开始之前发生。 所有init函数的完成在函数main.main开始之前是同步的。 创建Goroutine启动一个新的goroutine的go语句在goroutine的执行开始之前就已经同步了。 例如，在这个程序中 12345678910var a stringfunc f() { print(a)}func hello() { a = &quot;hello, world&quot; go f()} 调用hello方法会在未来某个时间打印“hello world”（可能是在hello方法返回）。 销毁Goroutine一个 goroutine的 退出并不保证在程序中的任何事件之前同步进行。例如，在这个程序中。 123456var a stringfunc hello() { go func() { a = &quot;hello&quot; }() print(a)} 对a的赋值后面没有任何同步事件，所以它不能保证被其他goroutine观察到。事实上，一个积极的编译器可能会删除整个go语句。 如果一个goroutine的效果必须被另一个goroutine观察到，请使用同步机制，如锁或通道通信来建立一个相对顺序。 通道通信通道通信是goroutine之间同步的主要方法。在一个特定通道上的每一个发送都与来自该通道的相应接收相匹配，通常是在一个不同的goroutine中。 在一个通道上的发送是在该通道的相应接收完成之前同步的。 这个程序: 12345678910111213var c = make(chan int, 10)var a stringfunc f() { a = &quot;hello, world&quot; c &lt;- 0}func main() { go f() &lt;-c print(a)} 保证打印 “hello, world”。对a的写排在了c的发送之前，c的发送在相应的接收完成之前进行同步，接收排在了打印之前。 关闭一个通道是在接收之前同步进行的，因为该通道已经关闭，所以返回一个零值。 在前面的例子中，用close(c)代替c &lt;- 0，可以得到一个具有相同保证行为的程序。 从一个无缓冲通道的接收在该通道的相应发送完成之前被同步。 这个程序（同上，但发送和接收语句互换，并使用一个无缓冲的通道）： 12345678910111213var c = make(chan int)var a stringfunc f() { a = &quot;hello, world&quot; &lt;-c}func main() { go f() c &lt;- 0 print(a)} 也保证了打印 “hello, world”。对a的写排在了c的发送之前，c的接收在相应的发送完成之前进行同步，而发送则排在了打印之前。 如果通道是缓冲的（例如，c = make(chan int, 1)），那么程序将不能保证打印 “hello, world”。(它可能会打印出空字符串，崩溃，或做其他事情）。 在一个容量为C的通道上的第k个接收是在该通道的第k+C个发送完成之前同步进行的。 这条规则将前面的规则概括为缓冲通道的规则。它允许用缓冲信道来模拟计数信号：信道中的项目数对应于有效使用的数量，信道的容量对应于同时使用的最大数量，发送一个项目会获得信号，而接收一个项目会释放信号。这是一个限制并发性的常见用法。 这个程序为工作列表中的每一个条目都启动了一个goroutine，但是这些goroutine使用限制通道进行协调，以确保一次最多有三个工作函数在运行。 123456789101112var limit = make(chan int, 3)func main() { for _, w := range work { go func(w func()) { limit &lt;- 1 w() &lt;-limit }(w) } select{}} 锁sync包实现了两种锁的数据类型， sync.Mutex 和 sync.RWMutex. 对任何sync.Mutex 或 sync.RWMutex 变量 l 来说， n &lt; m 时 ,在调用m次l.Lock() 返回前调用n次l.Unlock()总是同步的。 这个程序： 1234567891011121314var l sync.Mutexvar a stringfunc f() { a = &quot;hello, world&quot; l.Unlock()}func main() { l.Lock() go f() l.Lock() print(a)} 保证了打印 “hello, world”。对l.Unlock()的第一次调用（在f中）在对l.Lock()的第二次调用（在main中）返回之前是同步的，这是在打印之前的排序。 对于任何对sync.RWMutex变量l的调用l.RLock，有一个n，使得对l.Unlock的第n次调用在l.RLock的返回之前被同步，而对l.RUnlock的匹配调用在对l.Lock的调用n+1返回之前被同步。 对l.TryLock（或l.TryRLock）的成功调用等同于对l.Lock（或l.RLock）的调用。一个不成功的调用完全没有同步的效果。就内存模型而言，l.TryLock（或l.TryRLock）可以被认为是能够返回false的，即使是在互斥区l被解锁的情况下。 OnceSync包通过使用Once类型为存在多个goroutines的情况下的初始化提供了一种安全机制。多个线程可以为一个特定的f执行once.Do(f)，但只有一个线程会运行f()，其他调用会阻塞，直到f()返回。 从once.Do(f)中调用一次f()的完成是在任何调用once.Do(f)的返回之前同步进行的。 在这个程序中: 12345678910111213141516var a stringvar once sync.Oncefunc setup() { a = &quot;hello, world&quot;}func doprint() { once.Do(setup) print(a)}func twoprint() { go doprint() go doprint()} 调用twoprint将精确地调用setup一次。setup函数将在调用print之前完成。结果是，”hello, world “将被打印两次。 原子值sync/atomic包中的API统称为 “原子操作”，可用于同步不同goroutines的执行。如果一个原子操作A的效果被原子操作B观察到，那么A就会在B之前被同步。一个程序中执行的所有原子操作的行为就像以某种顺序一致的方式执行。 前面的定义与C++的顺序一致的原子操作和Java的volatile变量的语义相同。 终结器runtime包提供了一个SetFinalizer函数，该函数添加了一个终结器，当程序不再能到达某个特定对象时，就会调用这个终结器。对SetFinalizer(x, f)的调用在最终化调用f(x)之前是同步的。 额外机制sync包提供了额外的同步抽象，包括条件变量、无锁映射、分配池和等待组。每个包的文档都说明了它对同步的保证。 其他提供同步抽象的包也应该在文档中记录它们的保证。 不正确的同步有竞争的程序是不正确的，可以表现出非顺序一致的执行。特别是要注意的是，一个读r可能会观察到与r同时执行的任何写w所写的值。即使发生这种情况，也不意味着在r之后发生的读会观察到w之前发生的写。 在这个程序中： 12345678910111213141516var a, b intfunc f() { a = 1 b = 2}func g() { print(b) print(a)}func main() { go f() g()} 有可能发生g打印出2，然后又打印出0。 这一事实使一些常见的用法失效了。 双重检查锁是为了避免同步化的开销。例如，twoprint程序可能被错误地写成： 12345678910111213141516171819var a stringvar done boolfunc setup() { a = &quot;hello, world&quot; done = true}func doprint() { if !done { once.Do(setup) } print(a)}func twoprint() { go doprint() go doprint()} 但不能保证在doprint中，观察到对done的写意味着观察到对a的写。这个版本可以（不正确地）打印一个空字符串，而不是 “hello, world”。 另一个不正确的常见用法是忙于等待一个值，如： 1234567891011121314var a stringvar done boolfunc setup() { a = &quot;hello, world&quot; done = true}func main() { go setup() for !done { } print(a)} 如前所述，在main中，不能保证观察到写给done的内容就意味着观察到写给a的内容，所以这个程序也可以打印一个空字符串。更糟糕的是，由于两个线程之间没有同步事件，所以无法保证写给done的数据会被main观察到。main中的循环不能保证完成。 在这个主题上还有一些更微妙的变体，比如这个程序： 123456789101112131415161718type T struct { msg string}var g *Tfunc setup() { t := new(T) t.msg = &quot;hello, world&quot; g = t}func main() { go setup() for g == nil { } print(g.msg)} 即使main观察到g != nil并退出其循环，也不能保证它能观察到g.msg的初始化值。 在所有这些例子中，解决方案都是一样的：使用显式同步。 不正确的编译Go的内存模型对编译器优化的限制和对Go程序的限制一样多。一些在单线程程序中有效的编译器优化在所有Go程序中都是无效的。特别是，编译器不能引入原程序中不存在的写，不能允许一个读来观察多个值，也不能允许一个写来写多个值。 下面所有的例子都假设*p和*q指的是多个goroutine可以访问的内存位置。 不在race-free程序中引入数据竞争意味着不将写操作从条件语句中移出，因为它们出现在条件语句中。例如，编译器不能在这个程序中倒置条件。 1234*p = 1if cond { *p = 2} 也就是说， 编译器不得将该程序改写成这个程序： 1234*p = 2if !cond { *p = 1} 如果cond值为false，另一个goroutine正在读取p，那么在原来的程序中，另一个goroutine只能观察到p的任何先前值和1。 不引入数据竞争也意味着不假定循环会终止。例如，一般来说，编译器不能把对p或q的访问移到这个程序中的循环前面： 123456n := 0for e := list; e != nil; e = e.next { n++}i := *p*q = 1 如果list指向一个循环列表，那么原程序永远不会访问p或q，但重写的程序会。(如果编译器能证明*p不会恐慌，那么将*q提前是安全的；将*q提前也需要编译器证明没有其他goroutine可以访问*q)。 不引入数据竞争也意味着不假设被调用的函数总是返回或没有同步操作。例如，编译器不能在这个程序中把对p或q的访问移到函数调用之前（至少在不直接了解f的精确行为的情况下不能这样做）： 123f()i := *p*q = 1 如果该调用从未返回，那么原程序将再次无法访问p或q，但重写的程序可以。而如果调用包含同步操作，那么原程序可以在访问p和q之前的边上建立发生，但重写的程序不会。 不允许一次读取观察多个值意味着不允许从共享内存中重新加载局部变量。例如，在这个程序中，编译器不能丢弃i并第二次从*p重新加载它： 1234567i := *pif i &lt; 0 || i &gt;= len(funcs) { panic(&quot;invalid function index&quot;)}... complex code ...// compiler must NOT reload i = *p herefuncs[i]() 如果复杂的代码需要很多寄存器，单线程程序的编译器可以丢弃i而不保存副本，然后在funcs[i]()之前重新加载i = p。Go编译器则不能这样做，因为p的值可能已经改变。(相反，编译器可以将i溢出到堆栈中）。 不允许一次写入多个值也意味着在写入前不使用将写入局部变量的内存作为临时存储。例如，在这个程序中，编译器不得使用*p作为临时存储。 1*p = i + *p/2 也就是说， 编译器不得将该程序改写成这个程序： 12*p /= 2*p += i 如果i和p开始等于2，原代码做p=3，所以一个竞赛线程只能从p中读取2或3。重写的代码先做p=1，然后再做*p=3，这样一个竞赛线程也可以读到1。 请注意，所有这些优化在C/C++编译器中都是允许的：与C/C++编译器共享后端的Go编译器必须注意禁用那些对Go无效的优化。 请注意，如果编译器能够证明数据竞争不会影响目标平台上的正确执行，那么禁止引入数据竞争的规定就不适用。例如，在基本上所有的CPU上，改写以下内容是有效的： 1234n := 0for i := 0; i &lt; m; i++ { n += *shared} 改写成： 12345n := 0local := *sharedfor i := 0; i &lt; m; i++ { n += local} 只要能证明*shared在访问时不会出错，因为可能增加的读不会影响任何现有的并发读或写。另一方面，这种重写在源到源的翻译器中是无效的。 结论编写data-race-free的Go程序员可以依赖这些程序的顺序一致的执行，就像在基本上所有其他现代编程语言中一样。 当涉及到有竞争的程序时，程序员和编译器都应该记住一个建议：不要耍小聪明。","link":"/2022/10/31/The-GO-Memory-Model/"},{"title":"The Google File System","text":"Google文件系统摘要GFS（Google File System）是由我们设计并实现的为大规模分布式数据密集型应用程序设计的可伸缩的分布式文件系统。GFS为在廉价商用设备上运行提供了容错能力，并可以在有大量客户端的情况下提供较高的整体性能。 GFS的设计来自于我们对我们的应用负载与技术环境的观察。虽然GFS与过去的分布式文件系统有着共同的目标，但是根据我们的观察，我们的应用负载和技术环境与过去的分布式系统所做的假设有明显的不同。这让我们重新审视了传统的选择并去探索完全不同的设计。 GFS很好地满足了我们的存储需求。GFS在Google被广泛地作为存储平台部署，用于生成、处理我们服务所使用的数据或用于需要大规模数据集的研发工作。到目前为止，最大的GFS集群有上千台机器、上千块磁盘，并提供了上百TB的存储能力。 在本文中，我们介绍了为支持分布式应用程序而设计的文件系统接口的扩展，还从多方面讨论了我们的设计，并给出了微型基准测试与在现实场景中的使用表现。 一、 引言为了满足Google快速增长的数据处理需求，我们设计并实现了GFS。GFS与过去的分布式系统有着很多相同的目标，如性能、可伸缩性、可靠性和可用性。但是我们的设计来自于我们对我们的应用负载与技术环境的观察。这些观察反映了与过去的分布式系统所做的假设明显不同的结果。因此，我们重新审视的传统的选择并探索了完全不同的设计。 首先，我们认为设备故障经常发生。GFS由成百上千台由廉价设备组成的存储节点组成，并被与其数量相当的客户端访问。设备的数量和质量决定了几乎在任何时间都会有部分设备无法正常工作，甚至部分设备无法从当前故障中分恢复。我们遇到过的问题包括：应用程序bug、操作系统bug、人为错误和硬盘、内存、插头、网络、电源等设备故障。因此，系统必须具有持续监控、错误检测、容错与自动恢复的能力。 第二，文件比传统标准更大。数GB大小的文件是十分常见的。每个文件一般包含很多引用程序使用的对象，如Web文档等。因为我们的数据集由数十亿个总计数TB的对象组成，且这个数字还在快速增长，所以管理数十亿个几KB大小的文件是非常不明智的，即使操作系统支持这种操作。因此，我们需要重新考虑像I/O操作和chunk大小等设计和参数。 第三，大部分文件会以“追加”的方式变更，而非“覆写”。在实际场景中，几乎不存在对文件的随机写入。文件一旦被写入，即为只读的，且通常仅被顺序读取。很多数据都有这样的特征。如数据分析程序扫描的大型数据集、流式程序持续生成的数据、归档数据、由一台机器生产并同时或稍后在另一台机器上处理的数据等。鉴于这种对大文件的访问模式，追加成了为了性能优化和原子性保证的重点关注目标，而客户端中对chunk数据的缓存则不再重要。 第四，同时设计应用程序和文件系统API便于提高整个系统的灵活性。例如，我们放宽了GFS的一致性协议，从而大幅简化了系统，减少了应用程序的负担。我们还引入了一种在不需要额外同步操作的条件下允许多个客户端并发将数据追加到同一个文件的原子性操作。我们将在后文中讨论更多的细节。 目前，我们部署了多个GFS集群并用于不同的目的。其中最大的集群有超过1000个存储节点、超过300TB的磁盘存储，并被数百台客户端连续不断地访问。 二、 设计概述2.1 假设在设计能够满足我们需求的文件系统时，我们提出并遵循了一些挑战与机遇并存的假设。之前我们已经提到了一些，现在我们将更详细地阐述我们的假设。 系统有许多可能经常发生故障的廉价的商用设备组成。它必须具有持续监控自身并检测故障、容错、及时从设备故障中恢复的能力。 系统存储一定数量的大文件。我们的期望是能够存储几百万个大小为100MB左右或更大的文件。系统中经常有几GB的文件，且这些文件需要被高效管理。系统同样必须支持小文件，但是不需要对其进行优化。 系统负载主要来自两种读操作：大规模的流式读取和小规模的随机读取。在大规模的流式读取中，每次读取通常会读几百KB、1MB或更多。来自同一个客户端的连续的读操作通常会连续读文件的一个区域。小规模的随机读取通常会在文件的某个任意偏移位置读几KB。性能敏感的应用程序通常会将排序并批量进行小规模的随机读取，这样可以顺序遍历文件而不是来回遍历。 系统负载还来自很多对文件的大规模追加写入。一般来说，写入的规模与读取的规模相似。文件一旦被写入就几乎不会被再次修改。系统同样支持小规模随机写入，但并不需要高效执行。 系统必须良好地定义并实现多个客户端并发向同一个文件追加数据的语义。我们的文件通常在生产者-消费者队列中或多路归并中使用。来自不同机器的数百个生产者会并发地向同一个文件追加写入数据。因此，最小化原子性需要的同步开销是非常重要的。文件在被生产后可能同时或稍后被消费者读取。 持续的高吞吐比低延迟更重要。我们的大多数应用程序更重视告诉处理大量数据，而很少有应用程序对单个读写操作有严格的响应时间的需求。 2.2 接口GFS提供了一个熟悉的文件系统接口，尽管它没有实现POSIX等标准API。文件以目录的形式分层组织，并由路径名标识。我们支持创建、删除、打开、关闭、读取和写入文件的常用操作。 此外，GFS具有快照和记录追加（record append）操作。快照以较低的成本创建文件或目录树的副本。记录追加允许多个客户端同时向同一个文件追加数据，同时保证每个客户端追加的原子性。它对于实现多路归并结果和生产者消费者队列非常有用，许多客户端可以同时附加到这些队列中，而不需要额外的锁。我们发现这些类型的文件在构建大型分布式应用程序时非常有用。快照和记录追加分别在第3.4节和3.3节中进一步讨论。 2.3 架构一个GFS集群由单个主服务器master和多个块服务器chunkserver组成，由多个client客户端访问，如图1所示。它们通常都是一台运行用户级服务器进程的商用Linux机器。在同一台机器上同时运行一个chunkserver和一个client是很容易的，只要机器资源允许，并且运行可能不可靠的应用程序代码所导致的低可靠性是可以接受的. 图1：GFS架构。 文件被分成固定大小的chunk块。每个chunk都由一个不可变的、全局唯一的64位chunk handle块句柄来标识，该句柄是在chunk创建时由master分配的。chunkserver将chunk以Linux文件的形式存储在本地磁盘上，并通过chunk handle和字节范围来读写chunk数据。为了提高可靠性，每个chunk都被复制到多个chunkserver上。默认情况下，我们存储三个副本，但是用户可以为文件命名空间的不同区域指定不同的复制级别。 Master维护所有文件系统元数据。这包括命名空间、访问控制信息、从文件到chunk的映射以及chunk的当前位置。它还控制系统范围的活动，如chunk租约的管理、孤立块的垃圾收集和chunkserver之间的块迁移。master通过HeartBeat消息定期与每个chunkserver通信，给它指令并收集它的状态。 链接到每个应用程序中的GFS client代码实现了文件系统API，并与master和chunkserver通信，以代表应用程序读写数据。client与master进行元数据操作，但所有承载数据的通信都直接到chunkserver。我们不提供POSIX API，因此不需要挂接到Linux vnode层 client和chunkserver都不会缓存文件数据。client缓存几乎没有什么好处，因为大部分应用程序需要流式地处理大文件，或者数据集过大以至于无法缓存。不使用它们可以消除缓存一致性问题，从而简化client和整个系统。(然而，client缓存元数据。)chunkserver不需要缓存文件数据，因为chunk存储为本地文件，所以Linux的缓冲缓存已经将频繁访问的数据保存在内存中。 2.4 单Master单master大大的简化了我们的设计，单master能够放心使用全局策略执行复杂的chunk布置、制定复制决策等。然而，我们必须在读写过程中尽量减少对它的依赖，它才不会成为一个瓶颈。client从不通过master读写文件，它只会询问master自己应该访问哪个chunkserver。client会缓存这个信息一段时间，随后的很多操作即可以复用此缓存，与chunkserver直接交互。 我们利用图1来展示一个简单读操作的交互过程。首先，使用固定的chunk size，client将应用程序指定的文件名和字节偏移量翻译为一个GFS文件及内部chunk序号，随后将它们作为参数，发送请求到master。master找到对应的chunk handle和副本位置，回复给client。client缓存这些信息，使用GFS文件名+chunk序号作为key。 client然后发送一个读请求到其中一个副本，很可能是最近的那个。请求中指定了chunk handle以及在此chunk中读取的字节范围。后面对相同chunk的读不再与master交互，直到client缓存信息过期或者文件被重新打开。事实上，client通常会在一个与master的请求中顺带多索要一些其他chunk的信息，而且master也可能将客户端索要chunk后面紧跟的其他chunk信息主动回复回去。这些额外的信息避免了未来可能发生的一些client-master交互，几乎不会导致额外的花费。 2.5 chunk块大小chunk的大小是一个设计的关键参数。我们选择这个大小为64M，远远大于典型的文件系统的block大小。每一个chunk的副本都是作为在chunkserver上的Linux文件格式存放的，并且只有当需要的情况下才会增长。可以通过惰性空间分配的机制来避免由于内部碎片造成的空间浪费，对于这样大的chunksize来说，内部碎片可能是一个最大的缺陷了。 选择一个很大的chunk大小提供了一些重要的好处。首先，它减少了client和master的交互，因为在同一个chunk内的读写操作之需要client初始询问一次master关于chunk位置信息就可以了。这个减少访问量对于我们的系统来说是很显著的，因为我们的应用大部分是顺序读写超大文件的。即使是对小范围的随机读，client可以很容易缓存一个好几个TB数据文件的所有的位置信息。其次，由于是使用一个大的chunk，client可以在一个chunk上完成更多的操作，它可以通过维持一个到chunkserver的TCP长连接来减少网络管理量。第三，它减少了元数据在master上的大小。这个使得我们可以把元数据保存在内存，这样带来一些其他的好处，详细请见2.6.1节。 在另一方面，选择一个大型的chunk，就算是采用惰性空间分配的模式，也有它的不好的地方。小型文件包含较少数量的chunk，也许只有一个chunk。保存这些文件的chunkserver就会在大量client访问的时候就会成为hot spots。在实践中，hot spots问题不太重要，因为我们的应用大部分都是顺序读取包含很多chunk的大文件。 不过，随着batch-queue系统开始使用GFS系统的时候，hot spots问题就显现出来了：一个可执行的程序在GFS上保存成为一个单chunk的文件，并且在数百台机器上一起启动的时候就出现hot spots问题。只有两三个chunkserver保存这个可执行的文件，但是有好几百台机器一起请求加载这个文件导致系统局部过载。我们通过把这样的执行文件保存份数增加，以及错开batch-queue系统的各worker启动时间来解决这样的问题。一劳永逸的解决方法是让client能够互相读取数据，这样才是解决之道。 2.6 元数据master存储了三种主要类型的元数据：文件和chunk的命名空间，文件到chunk的映射，以及每个chunk副本的位置。所有的元数据都保留在master的内存中。前两个类型（命名空间和文件到chunk的映射）通过将操作记录存储在本地磁盘上的日志文件中得以永久保存，并在远程的机器上进行日志备份。使用日志使我们能够简单可靠的更新master状态，并且不用担心由于master崩溃而造成的不一致性。master不会永久的保存chunk的位置信息，相反，master会在启动时，以及有新的chunkserver加入集群时，询问每个chunkserver的chunk信息。 2.6.1 内存数据结构由于元数据存放在内存中，所以master的操作非常快。此外，它也使master能够周期性的在后台简单有效的浏览整个系统的状态。这个周期性的浏览操作用于实现chunk的垃圾回收，chunkserver出错后的重复制，以及均衡负载和磁盘空间使用的块迁移。4.3和4.4节会深入的讨论这些行为。 对于这种内存存储的方法有一个潜在的问题，块的数量和将来整个系统的容量受到master的内存大小限制。在实际中，这不是一个严重的问题，主节点为每个64MB大小的块保留不到64字节的元数据。大多数块都是满的，因为大多数文件都包含了多个块，只有最后一个块才可能被部分使用。相似的，每个文件命名空间数据通常也不到64字节，因为它使用前缀压缩来简洁的存储文件名。 即使是要支持更大的文件系统，为master增加额外的内存的花费，比起将元数据存放在内存中所带来的简单性、可靠性、有效性和扩展性来说，也是相当值得的。 2.6.2 chunk位置master并不持久化保存chunkserver上保存的chunk的记录。它只是在启动的时候简单的从chunkserver取得这些信息。master可以在启动之后一直保持自己的这些信息是最新的，因为它控制所有的chunk的位置，并且使用普通心跳信息监视chunkserver的状态。 我们最开始尝试想把chunk位置信息持久化保存在master上，但是我们后来发现如果在启动时候，以及定期性从chunkserver上读取chunk位置信息会使得设计简化很多。因为这样可以消除master和chunkserver之间进行chunk信息的同步问题，当chunkserver加入和离开集群，更改名字，失效，重新启动等等时候，如果master上要求保存chunk信息，那么就会存在信息同步的问题。在一个数百台机器的组成的集群中，这样的发生chunserver的变动实在是太平常了。 此外，不在master上保存chunk位置信息的一个重要原因是因为只有chunkserver对于chunk到底在不在自己机器上有着最后的话语权。另外，在master上保存这个信息也是没有必要的，因为有很多原因可以导致chunserver可能忽然就丢失了这个chunk（比如磁盘坏掉了等等），或者chunkserver忽然改了名字，那么master上保存这个资料啥用处也没有。 2.6.3 操作日志操作日志包含重要的元数据变更的历史记录。这是GFS的核心。它不仅是元数据中唯一被持久化的记录，还充当了定义并发操作顺序的逻辑时间线。带有版本号的文件和chunk都在他们被创建时由逻辑时间唯一、永久地确定。 操作日志是GFS至关重要的部分，其必须被可靠存储，且在元数据的变更被持久化前不能让client对变更可见。否则当故障发生时，即使chunk本身没有故障，但是整个文件系统或者client最近的操作会损坏。我们将操作日志备份到多台远程主机上，且只有当当前操作记录条目被本地和远程机器均写入到了磁盘后才能向client发出响应。master会在操作记录被写入前批量合并一些记录日志来减少写入和备份操作对整个系统吞吐量的影响。 master通过重放操作日志来恢复其文件系统的状态。操作日志要尽可能小以减少启动时间。当日志超过一定大小时，master会对其状态创建一个检查点（checkpoint），这样master就可以从磁盘加载最后一个检查点并重放该检查点后的日志来恢复状态。检查点的结构为一个紧凑的B树，这样它就可以在内存中被直接映射，且在查找命名空间时不需要进行额外的解析。这进一步提高了恢复速度，并增强了系统的可用性。 因为创建一个检查点需要一段时间，所以master被设计为可以在不推迟新到来的变更的情况下创建检查点。创建检查点时，master会切换到一个新的日志文件并在一个独立的线程中创建检查点。这个新的检查点包含了在切换前的所有变更。一个有着几百万个文件的集群可以再一分钟左右创建一个检查点。当检查点被创建完成后，它会被写入master本地和远程主机的磁盘中。 恢复仅需要最后一个完整的检查点和后续的日志文件。旧的检查点和日志文件可以随意删除，不过我们会保留一段时间以容灾。创建检查点时发生错误不会影响日志的正确性，因为恢复代码会检测并跳过不完整的检查点。 2.7 一致性模型GFS采用宽松的一致性模型，能很好的支持高分布式应用，但同时保持相对简单，易于实现。我们现在讨论GFS的保障机制以及对于应用的意义。我们也着重描述了GFS如何维持这些保障机制，但将一些细节留在了其它章节。 2.7.1 GFS的保证文件命名空间变化（比如文件创建）是原子的，只有master能处理此种操作：master中提供了命名空间的锁机制，保证了原子性和正确性（章节4.1）；master的操作日志为这些操作定义了一个全局统一的顺序（章节2.6.3） 表1：变更后文件区域状态。 各种数据变更在不断发生，被它们改变的文件区域处于什么状态？这取决于变更是否成功了、有没有并发变更等各种因素。表1列出了所有可能的结果。对于文件区域A，如果所有客户端从任何副本上读到的数据都是相同的，那A就是consistent。如果A是consistent，并且客户端可以看到变更写入的完整数据，那A就是defined。当一个变更成功了、没有受到并发写的干扰，它写入的区域将会是defined（也是一致的）：所有客户端都能看到这个变更写入的完整数据。对同个区域的多个并发变更成功写入，此区域是consistent，但可能是undefined：所有客户端看到相同的数据，但是它可能不会反应任何一个变更写的东西，可能是多个变更混杂的碎片。一个失败的变更导致区域不一致（也是undefined）：不同客户端可能看到不同的数据在不同的时间点。下面描述我们的应用程序如何区分defined区域和undefined区域。 数据变更可能是写操作或者记录追加。写操作导致数据被写入一个用户指定的文件偏移。而记录追加导致数据（“记录”）被原子的写入GFS选择的某个偏移（正常情况下是文件末尾，见章节3.3），GFS选择的偏移被返回给客户端，其代表了此record所在的defined区域的起始偏移量。另外，某些异常情况可能会导致GFS在区域之间插入了padding或者重复的record。他们占据的区域可认为是不一致的，不过数据量不大。 如果一系列变更都成功写入了，GFS保证发生变更的文件区域是defined的，并完整的包含最后一个变更。GFS通过两点来实现：(a) chunk的所有副本按相同的顺序来实施变更（章节3.1）；(b) 使用chunk版本数来侦测任何旧副本，副本变旧可能是因为它发生过故障、错过了变更（章节4.5）。执行变更过程时将跳过旧的副本，客户端调用master获取chunk位置时也不会返回旧副本。GFS会尽早的通过垃圾回收处理掉旧的副本。 因为客户端缓存了chunk位置，所以它们可能向旧副本发起读请求。不过缓存项有超时机制，文件重新打开时也会更新。而且，我们大部分的文件是append-only的，这种情况下旧副本最坏只是无法返回数据（append-only意味着只增不减也不改，版本旧只意味着会丢数据、少数据），而不会返回过期的、错误的数据。一旦客户端与master联系，它将立刻得到最新的chunk位置（不包含旧副本）。 在一个变更成功写入很久之后，组件的故障仍然可能腐化、破坏数据。GFS中，master和所有chunkserver之间会持续handshake通讯并交换信息，借此master可以识别故障的chunkserver并且通过检查checksum侦测数据损坏（章节5.2）。一旦发现此问题，会尽快执行一个restore，从合法的副本复制合法数据替代损坏副本（章节4.3）。一个chunk也可能发生不可逆的丢失，那就是在GFS反应过来采取措施之前，所有副本都被丢失。通常GFS在分钟内就能反应。即使出现这种天灾，chunk也只是变得不可用，而不会损坏：应用收到清晰的错误而不是错误的数据。 2.7.2 对应用程序可能的影响GFS 应用程序可以利用一些简单技术适应这个宽松的一致性模型，这些技术已经满足了其他目的的需要：依赖追加而不是覆写，检查点，自验证，自标识的记录。 实际中，我们所有的应用通过追加而不是覆写的方式变更文件。一种典型的应用中，写入程序从头到尾地生成一个文件。写完所有数据之后，程序原子性地将文件重命名为一个永久的文件名，或者定期地对成功写入了多少数据设置检查点。检查点也可以包含程序级别的检验和。Readers仅校验并处理上一个检查点之后的文件域，也就是人们知道的已定义状态。不管一致性和并发问题的话，该方法对我们很适合。追加比随机写更有效率，对程序失败有更弹性。检查点允许Writer递增地重启，并且防止Reader成功处理从应用程序的角度看来并未完成的写入的文件数据。 在另一种典型应用中。许多Writer为了合并结果或者作为生产者-消费者队列并发地向一个文件追加数据。记录追加的“至少追加一次”的语义维持了每个Writer的输出。Reader使用下面的方法来处理偶然的填充和重复。Writer准备的每条记录中都包含了类似检验和的额外信息，以便用来验证它的有效性。Reader可以用检验和识别和丢弃额外的填充数据和记录片段。如果偶尔的重复内容是不能容忍的(比如，如果这些重复数据将要触发非幂等操作)，可以用记录的唯一标识来过滤它们，这些标识符也通常用于命名相应程序实体，例如web文档。这些记录I/O功能（除了剔除重复数据）都包含在我们程序共享的代码库（library code）中，并且适用于Google内部其它的文件接口实现。这样，记录的相同序列，加上些许重复数据，总是被分发到记录Reader中。 三、 系统交互3.1 租约和变更顺序变更是改变块内容或者块元数据的操作，比如写操作或者追加操作。每次变更在块所有的副本上执行。我们使用租约（lease）来维护副本间的一致性变更顺序。Master向其中一个副本授权一个块租约，我们把这个副本叫做主副本。主副本为对块的所有变更选择一个序列。应用变更的时候所有副本都遵照这个顺序。这样，全局变更顺序首先由master选择的租约授权顺序规定，然后在租约内部由主副本分配的序列号规定。 设计租约机制的目的是为了最小化master的管理开销。租约的初始过期时间为60秒。然而，只要块正在变更，主副本就可以请求并且通常会得到master无限期的延长。这些延长请求和批准信息附在master和所有Chunkserver之间的定期交换的心跳消息中。Master有时可能试图在到期前取消租约（例如，当master想令一个在一个重命名的文件上进行的修改失效）。即使master和主副本失去联系，它仍然可以安全地在旧的租约到期后和向另外一个副本授权新的租约。 图2：写控制和数据流。 在图2 中，我们根据写操作的控制流程通过这些标号步骤图示说明了这一过程。 1．client询问master哪一个chunkserver持有该块当前的租约，以及其它副本的位置。如果没有chunkserver持有租约，master将租约授权给它选择的副本（没有展示）。 2．master将主副本的标识符以及其它副本（次级副本）的位置返回给client。client为将来的变更缓存这些数据。只有在主副本不可达，或者其回应它已不再持有租约的时候，client才需要再一次联系master。 3．client将数据推送到所有副本。client可以以任意的顺序推送数据。Chunkserver将数据存储在内部LRU 缓存中，直到数据被使用或者过期。通过将数据流和控制流解耦，我们可以基于网络拓扑而不管哪个chunksever上有主副本，通过调度昂贵的数据流来提高系统性能。3.2章节会作进一步讨论。 4．当所有的副本都确认接收到了数据，client对主副本发送写请求。这个请求标识了早前推送到所有副本的数据。主副本为接收到的所有变更分配连续的序列号，由于变更可能来自多个client，这就提供了必要的序列化。它以序列号的顺序把变更应用到它自己的本地状态中。 5．主副本将写请求转发(forward)到所有的次级副本。每个次级副本依照主副本分配的序列顺序应用变更。 6．所有次级副本回复主副本并标明它们已经完成了操作。 7．主副本回复client。任何副本遇到的任何错误都报告给client。出错的情况下，写操作可能在主副本和次级副本的任意子集上执行成功。（如果在主副本失败，就不会分配序列号和转发。）client请求被认定为失败，被修改的域处于不一致的状态。我们的client代码通过重试失败的变更来处理这样的错误。在退到从头开始重试之前，client会将从步骤（3）到步骤（7）做几次尝试。 如果应用程序一次的写入量很大，或者跨越了多个块的范围，GFS clientS代码把它分成多个写操作。它们都遵照上面描述的控制流程，但是可能会被来自其它client的并发操作造成交错或者重写。因此，共享文件域可能以包含来自不同client的片段结尾，尽管如此，由于这些单个的操作在所有的副本上都以相同的顺序完成，副本仍然会是完全相同的。这使文件域处于2.7节提出的一致但是未定义的状态。 3.2 数据流为了高效地利用网络，我们对数据流与控制流进行了解耦。在控制流从client向primary再向所有secondary推送的同时，数据流沿着一条精心挑选的chunkserver链以流水线的方式线性推送。我们的目标是充分利用每台机器的网络带宽，避免网络瓶颈和高延迟的链路，并最小化推送完所有数据的时延。 为了充分利用机器的网络带宽，数据会沿着chunkserver链线性地推送，而不是通过其他拓扑结构（如树等）分配发送。因此，每台机器全部的出口带宽都被用来尽可能快地传输数据，而不是非给多个接受者。 为了尽可能地避免网络瓶颈和高延迟的数据链路（例如，交换机间链路（inter-switch）经常同时成为网络瓶颈和高延迟链路），每台机器会将数据传递给在网络拓扑中最近的的且还没有收到数据的机器。假设client正准备将数据推送给S1S4。client会将数据发送给最近的chunkserver，比如S1。S1会将数据传递给S2S4中离它最近的chunkserver，比如S2。同样，S2会将数据传递给S3~S4中离它最近的chunkserver，以此类推。由于我们的网络拓扑非常简单，所以可以通过IP地址来准确地估算出网络拓扑中的“距离”。 最后，我们通过流水线的方式通过TCP连接传输数据，以最小化时延。当chunkserver收到一部分数据时，它会立刻开始将数据传递给其他chunkserver。因为我们使用全双工的交换网络，所以流水线可以大幅减少时延。发送数据不会减少接受数据的速度。如果没有网络拥塞，理论上将 B 个字节传输给 R 个副本所需的时间为 B/T+RL ，其中 T 是网络的吞吐量， L 是两台机器间的传输时延。通常，我们的网络连接吐吞量 T 为 100Mbps ，传输时延 L 远小于 1ms 。 3.3 原子性记录追加GFS提供了一种叫做记录追加的原子追加操作。传统的写操作中，客户程序指定写入数据的偏移量。对同一个域的并行写不是串行的：域可能以包含来自多个client的数据片段结尾。在记录追加中，然而，client只需指定数据。GFS将其原子地追加到文件中至少一次（例如，作为一个连续的byte序列），数据追加到GFS选择的偏移位置，然后将这个偏移量返回给给client。这类似于在Unix中，对以O_APPEND模式打开的文件，多个并发写操作在没有竞态条件时对文件的写入。 记录追加在我们的分布应用中经常使用，其中很多在不同机器上的客户程序并发对同一文件追加。如果我们采用传统写方式处理，client将需要额外的复杂、昂贵的同步机制，例如通过一个分布式锁管理器。在我们的工作中，这样的文件通常用于多生产者/单消费者队列，或者是合并来自多个client的结果。 记录追加是一种变更，遵循3.1节的控制流，只主副本有些额外的控制逻辑。client把数据推送给文件最后一个块的所有副本，然后向主副本发送请求。主副本会检查如果追加这条记录会不会导致块超过最大尺寸（64MB）。如果超过，将快填充到最大尺寸，通知次级副本做同样的操作，然后回复client指出操作应该在下一个块重试。（记录追加限制在至多块最大尺寸的1/4，这样保证最坏情况下数据碎片的数量仍然在可控的范围。）如果记录在最大尺寸以内，这也是通常情况，主副本服务器将数据追加到自己的副本，通知次级副本将数据写在它准确的位移上，最后回复client操作成功。 如果记录追加在任何副本上失败，客户端重试操作。结果，同一个块的副本可能包含不同的数据，可能包括一个记录的全部或者部分重复。GFS并不保证所有副本在字节级别完全相同。它只保证数据作为一个原子单元的至少被写入一次。这个特性可以很容易地从简单观察中推断出来：操作如果要报告成功，数据一定已经写入到了一些块的所有副本的相同偏移上。并且，至此以后，所有副本至少都和记录尾部一样长，并且将来的记录会被分配到更高的偏移，或者不同的块，即之后一个不同的副本成为了主副本。就我们的一致性保障而言，记录追加操作成功写入数据的域是已定义的（因此也是一致的），然而中间域则是不一致的（因此也就是未定义的）。我们的程序可以像我们在2.7.2节讨论的那样处理不一致的域。 3.4 快照快照操作几乎会在瞬间对一个文件或一个目录树（被称为源）完成拷贝，同时能够尽可能少地打断正在执行的变更。我们的用户使用快照操作来快速地对一个庞大的数据集的一个分支进行拷贝（或对其拷贝再进行拷贝等等），或者在实验前对当前状态创建检查点，这样就可以在试验后轻松地提交或回滚变更。 我们使用类似AFS的标准的写入时复制技术来实现快照。当master收到快照请求的时候，它首先会撤销快照涉及到的文件的chunk上所有未完成的租约。这确保了对这些chunk在后续的写入时都需要与master交互以查找租约的持有者。这会给master优先拷贝这些chunk的机会。 在租约被收回或过期后，master会将快照操作记录到日志中，并写入到磁盘。随后，master会通过在内存中创建一个源文件或源目录树的元数据的副本的方式来进行快照操作。新创建的快照文件与源文件指向相同的chunk。 在快照操作后，首次想要对chunk C 进行write操作的client会向master发送一个请求以找到当前的租约持有者。master会检测到chunk C 的引用数超过1个。master会推迟对client的响应，并选取一个新的chunk handler C’ 。接着，master请求每个当前持有chunk C 副本的chunkserver去创建一个新chunk C’ 。通过在与源chunk相同的chunkserver上创建新chunk，可以保证数据只在本地拷贝，而不会通过网络拷贝（我们的磁盘大概比 100Mb 的以太网连接快3倍左右）。在这之后，请求的处理逻辑就与处理任何其他chunk的请求一样了：master向新chunk C’ 的一个副本授权租约并将其响应client的请求。这样，client就可以像平常一样对chunk进行write操作，且client并不知道这个chunk是刚刚从一个已有的chunk创建来的。 四、 Master的操作Master执行所有的命名空间操作。另外，它管理整个系统里的chunk副本：它制定部署策略，创建新的chunk也就是副本，协调各种系统级活动以保证chunk全面备份，在所有Chunkserver间平衡负载，回收闲置的存储空间。本节我们分别讨论这些主题。 4.1 命名空间管理和锁许多master操作会花费很长时间：比如，快照操作必须取消被快照覆盖的所有块上的chunkserver租约。我们不想它们运行的时候耽搁其它master操作。因此，我们允许多个操作活跃，使用命名空间域上的锁来保证正确的串行化。 不同于许多传统文件系统，GFS没有能够列出目录下所有文件的每目录数据结构。也不支持同一文件或者目录的别名（例如，Unix语境中的硬链接或者符号链接）。GFS将其命名空间逻辑上表现为全路径到元数据映射的查找表。利用前缀压缩，这个表可以在内存中高效展现。命名空间树中的每个节点（绝对文件名或绝对目录名）都有一个关联的读写锁。 每个master操作在运行之前都获得一组锁。通常情况下，如果它涉及/d1/d2/…/dn/leaf，它将获得目录名/d1，/d1/d2，…，/d1/d2/…/dn上的读锁，以及全路径/d1/d2/…/dn/leaf上的读锁或者写锁。注意，根据操作的不同，leaf可能是文件或者目录。 现在我们演示一下在/home/user被快照到/save/user的时候，锁机制如何防止创建文件/home/user/foo。快照操作获得/home和/save上的读锁，以及/home/user 和/save/user上的写锁。文件创建操作获得/home和/home/user的读锁，以及/home/user/foo的写锁。这两个操作将准确地串行，因为它们试图获取/home/user 上的冲突锁。文件创建不需要父目录的写锁，因为这里没有“目录”，或者类似内部节点的数据结构需要防止修改。文件名的读锁足以防止父目录被删除。 这种锁机制的一个良好特性是支持对同一目录的并发变更。比如，可以在同一个目录下同时创建多个文件：每个都获得一个目录名的上的读锁和文件名上的写锁。目录名的读取锁足以防止目录被删除、改名以及被快照。文件名的写入锁序列化地尝试用同一个名字两次创建文件。 因为命名空间可以有许多节点，读写锁对象采用惰性分配，一旦不再使用立刻被删除。同样，锁在一个一致性的全局顺序中获取来避免死锁：首先按命名空间树中的层次排序，同层按字典顺序排序。 4.2 副本的部署GFS集群在多个层级上都高度分布。GFS通常有数百个跨多个机架的chunkserver。这些chunkserver可能会被来自相同或不同机架上的数百个clienet访问。在不同机架上的两台机器的通信可能会跨一个或多个交换机。另外，一个机架的出入带宽可能小于这个机架上所有机器的出入带宽之和。多层级的分布为数据的可伸缩性、可靠性和可用性带来了特有的挑战。 chunk副本分配策略有两个目标：最大化数据可靠性和可用性、最大化网络带宽的利用。对于这两个目标，仅将副本分散在所有机器上是不够的，这样做只保证了容忍磁盘或机器故障且只充分利用了每台机器的网络带宽。我们必须在机架间分散chunk的副本。这样可以保证在一整个机架都被损坏或离线时（例如，由交换机、电源电路等共享资源问题引起的故障），chunk的一些副本仍存在并保持可用状态。除此之外，这样还使对chunk的流量（特别是读流量）能够充分利用多个机架的总带宽。而另一方面，写流量必须流经多个机架，这是我们资源做出的权衡。 4.3 chunk创建、重新备份、重均衡chunk副本的创建可能由三个原因引起：chunk创建、重新备份和重均衡。 当master创建一个chunk的时候，它会选择初始化空副本的位置。位置的选择会参考很多因素：（1）我们希望在磁盘利用率低于平均值的chunkserver上放置副本。随着时间推移，这样将平衡chunkserver间的磁盘利用率（2）我们希望限制每台chunkserver上最近创建的chunk的数量。尽管创建chunk本身开销很小，但是由于chunk时写入时创建的，且在我们的一次追加多次读取（append-once-read-many）的负载下chunk在写入完成后经常是只读的，所以master还要会可靠的预测即将到来的大量的写入流量。（3）对于以上讨论的因素，我们希望将chunk的副本跨机架分散。 当chunk可用的副本数少于用户设定的目标值时，master会重新备份。chunk副本数减少可能有很多种原因，比如：chunkserver可能变得不可用、chunkserver报告其副本被损坏、chunkserver的磁盘因为错误变得不可用、或者目标副本数增加。每个需要重新备份的chunk会参考一些因素按照优先级排序。这些因素之一是当前chunk副本数与目标副本数之差。例如，我们给失去两个副本的chunk比仅失去一个副本的chunk更高的优先级。另外，我们更倾向于优先为还存在的文件的chunk重新备份，而不是优先为最近被删除的文件（见章节4.4）重做。最后，为了最小化故障对正在运行的应用程序的影响，我们提高了所有正在阻塞client进程的chunk的优先级。 master选取优先级最高的chunk，并通过命令若干chunkserver直接从一个存在且合法的副本拷贝的方式来克隆这个chunk。新副本位置的选取与创建新chunk时位置选取的目标类似：均衡磁盘空间利用率、限制在单个chunkserver上活动的克隆操作数、在机架间分散副本。为了防止克隆操作的流量远高于client流量的情况发生，master需要对整个集群中活动的克隆操作数和每个chunkserver上活动的克隆操作数进行限制。除此之外，在克隆操作中，每个chunkserver还会限制对源chunkserver的读请求，以限制每个克隆操作占用的总带宽。 最后，每隔一段时间master会对副本进行重均衡：master会检测当前的副本分布并移动副本位置，使磁盘空间和负载更加均衡。同样，在这个过程中，master会逐渐填充一个新的chunkserver，而不会立刻让来自新chunk的高负荷的写入流量压垮新的chunkserver。新副本放置位置的选择方法与我们上文中讨论过的类似。此外，master必须删除一个已有副本。通常，master会选择删除空闲磁盘空间低于平均的chunkserver上的副本，以均衡磁盘空间的使用。 4.4 垃圾回收GFS在文件删除后不会立刻回收可用的物理空间。这只在常规的文件及块级垃圾回收期间懒惰地进行。我们发现这个方法使系统更简单、更可靠。 4.4.1 机制当一个文件被应用程序删除时，master象对待其它变更一样立刻把删除操作录入日志。然而，与立即回收资源相反，文件只是被重命名为包含删除时间戳的隐藏的名字。在master对文件系统命名空间常规扫描期间，它会删除所有这种超过了三天的隐藏文件（这个时间间隔是可设置的）。在这之前，文件仍旧可以用新的特殊的名字读取，也可以通过重命名为普通文件名的方式恢复。当隐藏文件被从命名空间中删除，它的内存元数据被擦除。这有效地割断了它与所有块的连接。 在类似的块命名空间常规扫描中，master识别孤立块（也就是对任何文件不可达的那些）并擦除那些块的元数据。在与master定期交换的心跳信息中，每个Chunkserver报告它拥有的块的子集，master回复识别出的已经不在master元数据中显示的所有块。Chunkserver可以任意删除这种块的副本。 4.4.2 讨论虽然分布式垃圾回收在编程语言环境中是一个需要复杂的解决方案的难题，这在GFS中是相当简单的。我们可以轻易地识别块的所有引用：它们在master专有维护的文件-块映射中。我们也可以轻松识别所有块的副本：它们是每个Chunkserver指定目录下的Linux文件。所有这种master不识别的副本都是“垃圾”。 垃圾回收方法对于空间回收相比迫切删除提供了一些优势。首先，在组件失效是常态的大规模分布式系统中其既简单又可靠。块创建可能在某些Chunkserver上成功在另一些上失败，留下了master不知道其存在的副本。副本删除消息可能丢失，master不得不记得重新发送失败的删除消息，不仅是自身的还有Chunkserver的。垃圾回收提供了一个统一的、可靠的清除无用副本的方法。第二，它将空间的回收合并到master常规后台活动中，比如，命名空间的常规扫描和与Chunkserver的握手等。因此，它批量执行，成本也被摊销。并且，它只在master相对空闲的时候进行。Master可以更快速地响应需要及时关注的client请求。第三，延缓空间回收为意外的、不可逆转的删除提供了安全网。 根据我们的经验，主要缺点是，延迟有时会阻碍用户在存储空间紧缺时对微调使用做出的努力。重复创建和删除临时文件的应用程序不能立刻重用释放的空间。如果一个已删除的文件再次被明确删除，我们通过加速空间回收的方式解决这些问题。我们允许用户对命名空间的不同部分应用不同的备份和回收策略。例如，用户可以指定某些目录树内的文件中的所有块不备份存储，任何已删除的文件立刻不可恢复地从文件系统状态中移除。 4.5 过期副本检测如果Chunkserver失效或者漏掉它失效期间对块的变更，块副本可能成为过期副本。对每个chunk，master维护一个块版本号来区分最新副本和过期副本。 每当master在一个块授权一个新的租约，它就增加chunk的版本号，然后通知最新的副本。Master和这些副本都把这个新的版本号记录在它们持久化状态中。这发生在任何client得到通知以前，因此也在对块开始写之前。如果另一个副本当前不可用，它的块版本号就不会被增长。当Chunkserver重新启动，并且向master报告它拥有的块子集以及它们相联系的版本号的时候，master会探测该Chunkserver是否有过期的副本。如果master看到一个比它记录的版本号更高的版本号，master假定它在授权租约的时候失败了，因此选择更高的版本号作为最新的。 master在常规垃圾搜集中移除过期副本。在此之前，master在回复client的块信息请求的时候实际上认为过期副本根本不存在。作为另一种保护措施，在通知client哪个Chunkserver持有一个块的租约、或者在一个克隆操作中命令一个Chunkserver从另一个Chunkserver读取块时，master都包含了块的版本号。client或者Chunkserver在执行操作时都会验证版本号，因此它总是访问最新数据。 五、 容错和诊断在我们设计GFS时，最大的挑战之一就是处理经常发生的设备故障。设备的质量和数量让故障发生不再是异常事件，而是经常发生的事。我们既无法完全信任机器，也无法完全信任磁盘。设备故障可能导致系统不可用，甚至会导致数据损坏。我们将讨论我们是如何应对这些挑战的，以及系统内建的用来诊断系统中发生的不可避免的问题的工具。 5.1 高可用在GFS 集群的数百个服务器之中，在任意给定时间有些服务器必定不可用。我们用两条简单但是有效的策略保证整个系统的高可用性：快速恢复和副本。 5.1.1 快速恢复在master和chunkserver的设计中，它们都会保存各自的状态，且无论它们以哪种方式终止运行，都可以在数秒内重新启动。事实上，我们并不区分正常终止和非正常的终止。通常，服务会直接被通过杀死进程的方式终止。当client和其他服务器的请求超时时，它们会在发生一个时间很短的故障，并随后重新连接到重启后的服务器并重试该请求。6.2.2节中有启动时间相关的报告。 5.1.2 chunk副本正如之前讨论的，每个chunk会在不同机架的多个chunkserver上存有副本。用户可以为不同命名空间的文件制定不同的副本级别。副本级别默认为3。当有chunkserver下线或通过checksum校验和（见章节5.2）检测到损坏的副本时，master根据需求克隆现有的副本以保证每个chunk的副本数都是饱和的。尽管副本策略可以很好地满足我们的需求，我们还是探索了其他形式的跨服务器的冗余策略以满足我们日益増长的只读数据存储需求，如：奇偶校验码（parity code）或擦除码（erasure code）。因为我们的流量主要来自append和读操作，而不是小规模的随机写操作，所以我们希望在松散耦合的系统中，既有挑战性又要可管理地去实现这些复杂的冗余策略。 5.1.3 master副本为了保证可靠性，master的状态同样有副本。master的操作日志和检查点被在多台机器上复制。只有当变更在被日志记录并被写入，master本地和所有master副本的磁盘中后，这个变更才被认为是已提交的。为了简单起见，一个master进程既要负责处理所有变更又要负责处理后台活动，如垃圾回收等从内部改变系统的活动。当master故障时，其几乎可以立刻重启。如果运行master进程的机器故障或其磁盘故障，在GFS之外的负责监控的基础架构会在其它持有master的操作日志副本的机器上启动一个新的master进程。client仅通过一个规范的命名来访问master结点（例如gfs-test），这个规范的命名是一个DNS别名，其可以在master重新被分配到另一台机器时被修改为目标机器。 此外，“影子”master节点（“shadow” master）可以提供只读的文件系统访问，即使在主master结点脱机时它们也可以提供服务。因为这些服务器可能稍稍滞后于主master服务器（通常滞后几分之一秒），所以这些服务器是影子服务器而非镜像服务器。这些影子master服务器增强了那些非正在被变更的文件和不介意读到稍旧数据的应用程序的可用性。实际上，由于文件内容是从chunkserver上读取的，所以应用程序不会读取到陈旧的文件内容。能够在一个很短的时间窗口内被读取到的陈旧的数据只有文件元数据，如目录内容和访问控制信息。 为了让自己的元数据跟随主master变化，影子master服务器会持续读取不断增长的操作日志副本，并像主master一样按照相同的顺序对其数据结构应用变更。像主master一样，影子master服务器也会在启动时从chunkserver拉取数据来获取chunk副本的位置（启动后便很少拉取数据），并频繁地与chunkserver交换握手信息来监控它们的状态。只有因主master决定创建或删除副本时，影子master服务器上的副本位置才取决于主master服务器。 5.2 数据完整性每个chunkserver都使用校验和来检测存储的数据是否损坏。由于GFS集群通常在数百台机器上有数千chunk磁盘，所以集群中经常会出现磁盘故障，从而导致数据损坏或丢失（第七章中介绍了一个诱因）。我们可以通过chunk的其他副本来修复损坏的chunk，但不能通过比较chunkserver间的副本来检测chunk是否损坏。除此之外，即使内容不同的副本中的数据也可能都是合法的：GFS中变更的语义，特别是前文中讨论过的记录追加，不会保证副本完全相同。因此，每个chunkserver必须能够通过维护校验和的方式独立的验证副本中数据的完整性。 一个chunk被划分为64KB的block。每个block有其对应的32位校验和。就像其他元数据一样，校验和也在内存中保存且会被通过日志的方式持久化存储。校验和与用户数据是分开存储的。 对于读取操作，无论请求来自client还是其他chunkserver，chunkserver都会在返回任何数据前校验所有包含待读取数据的block的校验和。因此，chunkserver不会将损坏的数据传给其他机器。如果一个block中数据和记录中低的校验和不匹配，那么chunkserver会给请求者返回一个错误，并向master报告校验和不匹配。随后，请求者会从其他副本读取数据，而master会从该chunk的其他副本克隆这个chunk。当该chunk新的合法的副本被安置后，master会通知报告了校验和不匹配的chunkserver删除那份损坏的副本。 校验和对读取性能的影响很小。因为我们的大部分读操作至少会读跨几个block的内容，我们只需要读取并校验相对少量的额外数据。GFS客户端代码通过尝试将读取的数据与需要校验的block边界对其的方式，进一步地减小了校验开销。除此之外，chunkserver上校验和的查找与比较不需要I/O操作，且校验和计算操作经常与其他操作在I/O上重叠，因此几乎不存在额外的I/O开销。 因为向chunk末尾append数据的操作在我们的工作负载中占主要地位，所以我们对这种写入场景的校验和计算做了大量优化。在append操作时，我们仅增量更新上一个block剩余部分的校验和，并为append的新block计算新校验和。即使最后一个block已经损坏且目前没被检测到，增量更新后的该block的新校验和也不会与block中存储的数据匹配。在下一次读取该block时，GFS会像往常一样检测到数据损坏。 相反，如果write操作覆盖了一个chunk已存在的范围，那么我们必须读取并验证这个范围的头一个和最后一个block，再执行write操作，最后计算并记录新的校验和。如果我们没有在写入前校验头一个和最后一个block，新的校验和可能会掩盖这两个block中没被覆写的区域中存在的数据损坏问题。 chunkserver可以在空闲期间扫描并验证非活动的chunk的内容。这样可以让我们检测到很少被读取的chunk中的数据损坏。一旦检测到数据损坏，master可以创建一个新的未损坏的副本并删除损坏的副本。这样可以防止master将chunk的非活动的但是已损坏的副本识别成数据合法的副本。 5.3 诊断工具全面且详细的诊断日志以极小的开销为问题定位、调试和性能分析提供了很大的帮助。如果没有日志，理解机器间短暂且不重复的交互将变得非常困难。GFS服务器会生成用来记录重要事件（如chunkserver上线或离线）和所有RPC请求与响应的诊断日志。这些诊断日志可以随意删除，不会影响到系统正确性。不过，如果磁盘空间允许，我们将尽可能地保持这些日志。 RPC日志包括通过网络收发的请求和响应中除读写的文件数据之外的详细内容。在诊断问题时，我们可以通过整合不同机器中的日志并将请求与响应匹配的方式，重建整个交互历史。同样，这些日志也可用来跟踪压力测试、性能分析等情况。 因为日志是顺序且异步写入的，因此日志对性能的影响非常小，并带来了很大的好处。其中最近的事件也会在内存中保存，以便在持续的在线监控中使用。 六、 测量在这一节中我们展示了一些微型基准测试，以说明在GFS架构和实现中的瓶颈。我们还将展示一些Google在现实场景中的集群使用时的一些指标。 6.1 微型基准测试我们在一个由1个master、2个master副本、16个chunkserver和16个client组成的GFS集群中测量性能表现。该配置的选择仅为了便于测试。通常一个GFS集群会由数百个chunkserver和数百个client组成。 所有的机器都采用双核1.4GHz的奔腾III处理器、2GB内存、两块5400转的80GB磁盘和100Mbpc全双工以太网，并连接到一台HP2524交换机。其中所有的19台GFS服务器都连接到同一台交换机，所有的16台client机器都连接到另一台交换机。这两个交换机之间通过1Gbps连接。 6.1.1 读取N个client同时从GFS读取数据。每个client从320GB的数据集中随机选取4MB的区域读取。读操作将重复256次，即每个client最终将读取1GB的数据。chunkserver总计有32GB内存，因此我们预测读操作中最多10%命中Linux缓冲区缓存。我们的测试结果应该接近冷缓存的结果。 图3：总吞吐量。上面的曲线表示在网络拓扑中的理论极限。下面的曲线表示测量到的吞吐量。测量结果曲线显示了95%置信区间的误差柱，在一些情况下，由于测量值的方差很低，置信区间在图中难以辨认。 图3(a)展示了N个client的总读取速率和理论速率上限。整体的理论速率在125MB/s时达到峰值，此时两个交换机间的1Gbps的链路达到饱和；或者每个client的理论速率在12.5MB/s时达到峰值，此时它的100Mbps的网络接口达到饱和。当仅有一台client在读取时，我们观测到其读取速率为10MB/s，在每台client理论上限的80%。当16个client一起读取时，总读取速率达到了94MB/s，大致达到了理论上限125MB/s的75%，平均每个client的读取速率为6MB/s。因为reader的数量增加导致多个reader从同一个chunkserver读取的概率增加，所以读取速率从理论值的80%下降到了75%。 6.1.2 写入N个client同时向N个不同的文件写入。每个client通过一系列的1MB的写操作向一个新文件写入总计1GB数据。图3(b)展示了整体的写入速率和理论速率上限。整体的理论写入速率上限为67MB/s，因为我们需要将每个字节写入16个chunkserver中的三个，每个chunkserver的连接输入速率为12.5MB/s。 每个client的写入速率为6.3MB/s，大概是上限的一半。网络栈是造成这一现象的罪魁祸首。在我们使用流水线的方式将数据推送到chunk副本时，网络栈的表现不是很好。数据从一个副本传递给另一个副本的时延降低了整体的写入速率。 16个client的整体写入速率达到了35MB/s，大概是理论上限的一半。在读取的情况下，当client的数量增加时，更有可能出现多个client并发地向同一个chunkserver写入的情况。此外，因为write操作需要向3份不同的副本写入，所以16个writer比16个reader更有可能出现碰撞的情况。 write操作比我们预想的要慢。但是在实际环境中，这并不是主要问题。即使它增加了单个client的时延，但是在有大量client的情况下它并没有显著影响系统的整体写入带宽。 6.1.3 记录追加图3(c)展示了记录追加操作的性能表现。N个client同时向同一个文件追加数据。其性能受存储该文件最后一个chunk的chunkserver的网络带宽限制，与client的数量无关。当仅有1个client时，记录追加的速率为6.0MB/s，当client的数量增加到16个时，速率下降到4.8MB/s。网络拥塞和不同client的网络传输速率不同是造成记录追加速率下降的主要原因。 在实际环境中，我们的应用程序往往会并发地向多个这样的文件追加数据。换句话说，即N个client同时地向M个共享的文件追加数据，其中N与M均为数十或数百。因此，实验中出现的chunkserver的网络拥塞在实际环境中并不是大问题，因个client可以在chunkserver忙着处理一个文件时向另一个文件写入数据。 6.2 现实场景中的集群现在我们来考察在Google中使用的两个集群，它们代表了其他类似的集群。集群A是数百个工程师常用来研究或开发的集群。其中典型的任务由人启动并运行几个小时。这些任务会读几MB到几TB的数据，对其分析处理，并将结果写回到集群中。集群B主要用于生产数据的处理。其中的任务持续时间更长，会不断地生成数TB的数据集，且偶尔才会有人工干预。在这两种情况中，每个任务都由许多进程组成，这些进程包括许多机器对许多文件同时的读写操作。 表2：两个GFS集群的特征。 6.2.1 存储正如表中前5个条目所示，两个集群都有数百个chunkserver，有数TB的磁盘存储空间，且大部分存储空间都被使用，但还没满。其中“已使用空间”包括所有chunk的副本占用的空间。几乎所有文件都以3份副本存储。因此，集群分别存储了18TB和52TB的数据。 这两个集群中的文件数相似，但集群B中停用文件（dead file）比例更大。停用文件即为被删除或被新副本替换后还未被回收其存储空间的文件。同样，集群B中chunk数量更多，因为其中文件一般更大。 6.2.2 元数据在chunkserver中，总共存储了数十GB的元数据，其中大部分是用户数据的每64KB大小的block的校验和。除此之外，chunkserver中的保存元数据只有4.5节中讨论的chunk版本号。 保存在master上的元数据小的多，只有数十MB，或者说平均每个文件100 字节。这和我们设想的是一样的，实际中master的内存大小并不限制系统容量。大部分的文件元数据是文件名，我们对其采用前缀压缩的形式存储。其他的文件元数据包括文件所有权和权限、文件到chunk的映射、每个chunk当前的版本号。除此之外，我们还存储了chunk当前的副本位置和chunk的引用计数（以实现写入时拷贝等）。 无论是chunkserver还是master，每个服务器中仅有50MB到100MB元数据。因此，服务器恢复的速度很快。服务器只需要几秒钟的时间从磁盘读取元数据，随后就能应答查询请求。然而，master的恢复稍微有些慢，其通常需要30到60秒才能恢复，因为master需要从所有的chunkserver获取chunk的位置信息。 6.2.3 读写速率表3展示了不同时间段的读写速率。两个集群在测量开始后均运行了大概一周的时间。（集群最近已因升级到新版本的GFS而重启过。） 表3：两个GFS集群的性能指标。 从重启后，集群的平均写入速率小于30MB/s。当我们测量时，集群B正在执行以大概100MB/s写入生成的数据的活动，因为需要将数据传递给三份副本，该活动造成了300MB/s的网络负载。 读操作的速率比写操作的速率要高得多。正如我们假设的那样，整体负载主要有读操作组成而非写操作。在测量时两个集群都在执行高负荷的读操作。实际上，集群A已经维持580MB/s的读操作一周了。集群A的网络配置能够支持750MB/s的读操作，所以集群A在高效利用其资源。集群B能够支持峰值在1300MB/s的读操作，但集群B的应用程序仅使用了380MB/s。 6.2.4 master负载表3中还展示了向master发送操作指令的速率，该速率大概在每秒200到500次左右。master可以在该速率下轻松地工作，因此这不会成为负载的瓶颈。 GFS在早期版本中，在某些负载场景下，master偶尔会成为瓶颈。当时master会消耗大量的时间来扫描包含成百上千个文件的目录以寻找指定文件。在那之后，我们修改了master中的数据结构，允许其通过二分查找的方式高效地搜索命名空间。目前，master已经可以轻松地支持每秒上千次的文件访问。如果有必要，我们还可以通过在命名空间数据结构前放置名称缓存的方式进一步加快速度。 6.2.5 恢复时间当chunkserver故障后，一些chunk的副本数会变得不饱和，系统必须克隆这些块的副本以使副本数重新饱和。恢复所有chunk需要的时间取决于资源的数量。在一次实验中，我们杀掉集群B中的一个chunkserver。该chunkserver上有大概15000个chunk，总计约600GB的数据。为了限制重分配副本对正在运行的应用程序的影响并提供更灵活的调度策略，我们的默认参数限制了集群中只能有91个并发的克隆操作（该值为集群中chunkserver数量的40%）。其中，每个克隆操作的速率上限为6.25MB/s（50Mbps）。所有的chunk在23.2分钟内完成恢复，有效地复制速率为440MB/s。 在另一个实验中，我们杀掉了两台均包含16000个chunk和660GB数据的chunkserver。这两个chunkserver的故障导致了266个chunk仅剩一分副本。这266个块在克隆时有着更高的优先级，在2分钟内即恢复到至少两份副本的状态，此时可以保证集群中即使再有一台chunkserver故障也不会发生数据丢失。 6.3 工作负载分解在本节中，我们将详细介绍两个GFS集群中的工作负载。这两个集群与6.2节中的类似但并不完全相同。集群X用来研究和开发，集群Y用来处理生产数据。 6.3.1 方法论及注意事项这些实验结果仅包含来自client的请求，因此结果反映了我们的应用程序为整个文件系统带来的负载情况。结果中不包括用来处理client请求的内部请求和内部的后台活动，如chunkserver间传递write数据和副本重分配等。 I/O操作的统计数据来源于GFS通过RPC请求日志启发式重构得到的信息。例如，GFS的client代码可能将一个read操作分解为多个RPC请求以提高并行性，通过日志启发式重构后，我们可以推断出原read操作。因为我们的访问模式是高度一致化的，所以我们期望的错误都在数据噪声中。应用程序中显式的日志可能会提供更加准确的数据，但是重新编译并重启上千个正在运行的client是不现实的，且从上千台机器上采集数据结果也非常困难。 需要注意的一点是，不要过度地概括我们的负载情况。因为Google对GFS和它的应用程序具有绝对的控制权，所以应用程序会面向GFS优化，而GFS也正是为这些应用程序设计的。虽然这种应用程序与文件系统间的互相影响在一般情况下也存在，但是这种影响在我们的例子中可能会更明显。 6.3.2 chunkserver的工作负载表4展示了各种大小的操作占比。读操作的大小呈双峰分布。小规模read（64KB以下）来自client从大文件查找小片数据的seek密集操作。大规模read（超过512KB）来自读取整个文件的线性读取。 表4：各种大小的操作占比（%）。** 对于read操作，数据大小为实际读取和传输的数据大小，而非请求读取的总大小。 在集群Y中，大量的read没有返回任何数据。在我们的应用程序中（特别是生产系统中的应用程序），经常将文件作为生产者-消费者队列使用。在多个生产者并发地向同一个文件支架数据的同时，会有一个消费者读末尾的数据。偶尔当消费者超过生产者时，read即不会返回数据。集群X中这种情况出现的较少，因为在集群X中的应用程序通常为短期运行的数据分析程序，而非长期运行的分布式应用程序。 write也呈同样的双峰分布。大规模write（超过256KB）通常是由writer中的大量的缓冲区造成的。小规模write（小于64KB）通常来自于那些缓冲区小、创建检查点操作或者同步操作更频繁、或者是仅生成少量数据的writer。 对于记录追加操作，集群Y中大规模的记录追加操作比集群X中要高很多。因为我们的生产系统使用了集群Y，生产系统的应用程序会更激进地面向GFS优化。 表5中展示了不同大小的操作中传输数据的总量的占比。对于所有类型的操作，大规模操作（超过256KB）通常都是字节传输导致的。小规模read（小于64KB）操作通常来自seek操作，这些读操作传输了很小但很重要的数据。 表5：各种大小的操作字节传输量占比（%）。** 对于read操作，数据大小为实际读取和传输的数据大小，而非请求读取的总大小。二者的区别为，读取请求可能会试图读取超过文件末尾的内容。在我们的设计中，这不是常见的负载。 6.3.3 追加 VS 写入记录追加操作尤其在我们生产系统中使用频繁。对于集群X，按照传输的字节数写操作和记录追加的比率是108:1，按照操作次数比是8:1。对于用于我们生产系统的集群Y，比率分别是3.7:1和2.5:1。并且，这一比率说明对于这两个集群，记录追加倾向于比写操作大。然而对于集群X，在测量期间记录追加的整体使用相当低，因此结果可能可能被一两个使用特定大小的缓冲区选择的应用程序造成偏移。 不出所料，我们的数据变更负载主要被记录追加占据而不是重写。我们测量了在主副本上的重写数据量。这近似于一个client故意重写之前写过的数据，而不是增加新的数据。对于集群X，重写的量低于字节变更的0.0001%，低于变更操作的0.0003%。对于集群Y，这两个比率都是0.05%。虽然这很微小，但是仍然高于我们的预期。这证明了，大多数重写来自由于错误或超时引起的客户端重试。这在不算工作负载本身的一部分，而是重试机制的结果。 6.3.4 master的工作负载表6展示了对master的各种类型的请求占比。其中，大部分请求来自read操作询问chunk位置的请求（FindLocation）和数据变更操作询问租约持有者（FindLeaseLocker）。 集群X与集群Y中Delete请求量差异很大，因为集群Y存储被经常重新生成或者移动的生产数据。一些Delete请求量的差异还隐藏在Open请求中，因为打开并从头写入文件时（Unix中以“w”模式打开文件），会隐式地删除旧版本的文件。 FindMatchingFiles是用来支持“ls”或类似文件系统操作的模式匹配请求。不像给master的其他请求，FindMatchingFiles请求可能处理很大一部分命名空间，因此这种请求开销很高。在集群Y中，这种请求更加频繁，因为自动化的数据处理任务常通过检查部分文件系统的方式来了解应用程序的全局状态。相反，使用集群X的应用程序会被用户更明确地控制，通常会提交知道所需的文件名。 表6：master请求类型占比（%）。 七、 经验在构建和部署GFS 的过程中，我们经历了各种各样的问题，有些是操作上的，有些是技术上的。 起初，GFS 被设想为我们的生产系统的后端文件系统。随着时间推移，使用涉及了研究和开发任务。开始对许可和配额这类工作有很少的支持，但是现在包含了这些工作的基本形式。虽然生产系统是条理可控的，用户有时却不是。需要更多的基础设施来防止用户互相干扰。 我们最大的问题是磁盘以及和Linux相关的问题。很多磁盘声称拥有支持某个范围内的IDE协议版本的Linux驱动，但是实际中反映出，只可靠地支持最新的。因为协议版本非常类似，这些磁盘大都可用，但是偶尔失配会导致驱动和内核对于驱动状态意见不一致。这会导致因为内核中的问题而默默地损坏数据。这个问题激励了我们使用checksum来探测数据损坏，然而同时我们修改内核来处理这些协议失配。 早期我们在用Linux 2.2内核时有些问题，起因于fsync()的开销。它的开销与文件的大小而不是文件修改部分的大小成比例。这对我们的大型操作日志来说是一个问题，尤其是在我们实现检查点之前。我们花了不少时间用同步写来解决这个问题，但是最后还是移植到了Linux2.4内核上。 另一个和Linux问题是单个读写锁问题，在一个地址空间的任意线程在从磁盘读进页（读锁）的时候都必须持有锁，或者在mmap()调用（写锁）的时候修改地址空间。在轻负载下的系统中我们发现短暂超时，然后卖力寻找资源瓶颈或者零星硬件错误。最终我们发现在磁盘线程置换之前映射数据的页时，单独锁阻塞了主网络线程把新数据映射到内存。因为我们主要受限于网络接口而不是内存复制带宽，我们以多一次复制为代价，用pread()替代mmap()的方式来解决这个问题。 除了偶然的问题，Linux代码的可用性为我们节省了时间，并且再一次探究和理解系统的行为。适当的时候，我们改进内核并且和开源代码社区共享这些改动。 八、 相关工作类似诸如AFS的其它大型分布式文件系统，GFS提供了一个与位置无关的命名空间，这使得数据可以为均衡负载或者容错透明地移动数据。不同于AFS 的是，GFS把文件数据分布到存储服务器，一种更类似Xfs和Swift的方式，这是为了实现整体性能和提高容错能力。 由于磁盘相对便宜，并且复制比更复杂的RAID方法简单的多，GFS当前只使用备份进行冗余，因此要比xFS或者Swift花费更多的原始数据存储。 与AFS、xFS、Frangipani以及Intermezzo系统相比，GFS并没有在文件系统层面提供任何缓存机制。我们的目标工作负载在单个应用程序运行内部几乎不会重复使用，因为它们或者是流式的读取一个大型数据集，要么是在其中随机搜索，每次读取少量的数据。 某些分布式文件系统，比如Frangipani、xFS、Minnesota’s GFS、GPFS，去掉了中心服务器，依赖分布式算法保证一致性和可管理性。我们选择中心化的方法，目的是简化设计，增加可靠性，获得灵活性。特别的是，由于master已经拥有大多数相关信息，并且控制着它的改变，中心master使实现复杂的块部署和备份策略更简单。我们通过保持小型的master状态和在其它机器上对状态全复制的方式处理容错。扩展性和高可用性（对于读取）当前通过我们的影子master机制提供。对master状态的更新通过向预写日志追加的方式持久化。因此，我们可以适应类似Harp中主复制机制，从而提供比我们当前机制更强一致性保证的高可用性。 我们在对大量用户实现整体性能方面类似于Lustre处理问题。然而，我们通过关注我们应用的需求，而不是建立一个兼容POSIX的文件系统的方式，显著地简化了这个问题。此外，GFS假定了大量不可靠组件，因此容错是我们设计的核心。 GFS很类似NASD架构。虽然NASD架构是基于网络附属磁盘驱动的，GFS使用日常机器作为chunkserver，就像NASD原形中做的那样。与NASD工作不同的是，我们的Chunkserver使用惰性分配固定大小的块，而不是分配变长对象。此外，GFS实现了诸如重新平衡负载、备份、恢复等在生产环境中需要的特性。 不同于与Minnesota’s GFS和NASD，我们并不谋求改变存储设备模型。我们关注使用现存日常组件的复杂分布式系统的日常数据处理需求。 原子记录追加实现的生产者-消费者队列解决了类似River中分布式队列的问题。River使用跨机器分布、基于内存的队列，小心的数据流控制；然而GFS 使用可以被许多生产者并发追加记录的持久化文件。River模型支持m到n 的分布式队列，但是缺少伴随持久化存储的容错机制，然而GFS只高效地支持m到1的队列。多个消费者可以读取同一个文件，但是它们必须调整划分将来的负载。 九、 结论Google文件系统展示了在日常硬件上支持大规模数据处理工作负载必需的品质。虽然一些设计决策是针对独特设置指定的，许多决策可能应用到相似数量级和成本意识的数据处理任务中。 首先，我们根据我们当前和预期的工作负载和技术环境重新检查传统文件系统的假设。我们的观测在设计领域导致了根本不同的观点。我们将组件失效看作是常态而不是例外，优化通常先被追加（可能并发）然后再读取（通常序列化读取）的大文件，以及既扩展又放松标准文件系统接口来改进整个系统。 我们的系统通过持续监控，备份关键数据，快速和自动恢复的方式容错。chunk副本使得我们可以容忍chunkserver失效。这些失效的频率激发了一种新奇的在线修复机制，定期透明地修复受损数据，尽快补偿丢失副本。此外，我们使用检验和在磁盘或者IDE子系统级别探测数据损坏，考虑到系统中磁盘的数量，这些情况是很常见的。 我们的设计对大量并发的执行各种任务的reader和writer实现了高合计吞吐量。我们通过将文件系统控制与数据传输分离实现这个目标，控制经过master，数据传输直接在chunkserver和客户机之间穿行。Master与一般操作的牵连被大块尺寸和块租约最小化，块租约对主副本进行数据变更授权。这使得一个简单、中心化的master不变成瓶颈有了可能。我们相信在网络协议栈上的改进可以提升个别客户端经历的写吞吐量限制。 GFS成功满足了我们的存储需求，并且在Google内部作为存储平台，无论是用于研究和开发，还是作为生产数据处理，都得到了广泛应用。它是使我们持续创新和解决整个WEB范围内的难题的一个重要工具。","link":"/2022/08/31/The-Google-File-System/"},{"title":"ZooKeeper: Wait-free coordination for Internet-scale systems","text":"ZooKeeper: 互联网级系统的无等待协调摘要在本文中，我们描述了ZooKeeper，一个用于协调分布式应用程序进程的服务。由于ZooKeeper是关键基础设施的一部分，ZooKeeper的目的是为在客户端建立更复杂的协调原语提供一个简单和高性能的内核。它在一个复制的集中式服务中整合了群组消息传递、共享寄存器和分布式锁服务的元素。ZooKeeper提供的接口具有共享寄存器的免等待功能，以及类似于分布式文件系统的缓存失效的事件驱动机制，以提供一个简单而强大的协调服务。 ZooKeeper接口能够实现高性能的服务。除了无等待的特性外，ZooKeeper还为每个客户端提供了以下保证：请求的FIFO执行和所有改变ZooKeeper状态请求的可线性化。这些设计决定使高性能的处理管道得以实现，读取请求由本地服务器满足读取请求。我们展示了对于目标工作负载，即2:1到100:1的读写比，ZooKeeper可以每秒处理几万到几十万的事务。这种性能使ZooKeeper可以被客户端应用程序广泛使用。 一、 引言大规模的分布式应用需要不同形式的协调。配置是协调的最基本形式之一。在最简单的形式中，配置只是系统进程的操作参数列表，而更复杂的系统则有动态配置参数。在分布式系统中，群组成员和领导者的选举也很常见：经常有进程需要知道哪些其他进程是活的，这些进程负责什么。锁构成了一个强大的协调原语，实现了对关键资源的互斥访问。 协调的方法之一是为每个不同的协调需求开发服务。例如，Amazon Simple Queue Service[3]专门关注队列问题。还有的服务是专门为领导者选举[25]和配置[27]开发的。已经实现了更强大的原语的服务可以用来实现弱一些的原语。例如 例如，Chubby[6]是一个锁定服务，具有强大的同步保证。然后，锁可以被用来实现领导者选举、组成员资格等。 在设计我们的协调服务时，我们放弃了在服务器端实现特定的原语，而是选择了暴露一个API，使应用开发者能够实现他们自己的原语。这样的选择导致了协调内核的实现，它可以在不需要改变服务核心的情况下实现新的原语。这种方法使多种形式的协调适应应用程序的要求，而不是将开发者限制在一套固定的原语中。 在设计ZooKeeper的API时，我们摒弃了阻塞原语，如锁。协调服务的阻塞原语会导致，除其他问题外，缓慢或有问题的客户端会对快速客户端的性能产生负面影响。如果处理请求依赖于其他客户端的响应和故障检测，那么服务的实现本身就会变得更加复杂。因此，我们的系统Zookeeper实现了一个操作简单的无等待数据对象的API，像文件系统一样分层组织。事实上，ZooKeeper的API类似于其他文件系统的API，只看API的签名，ZooKeeper似乎是没有锁方法、打开和关闭的Chubby。然而，实现无等待的数据对象使ZooKeeper与基于阻塞原语的系统（如锁）有明显的区别。 尽管无等待属性对性能和容错很重要，但它对协调来说是不够的。我们还必须为操作提供顺序保证。特别是，我们发现，保证所有操作的先进先出（FIFO）客户端顺序和可线性化的写入可以有效地实现服务，这足以实现我们应用所关心的协调原语。事实上，我们可以用我们的API为任何数量的进程实现共识，根据Herlihy的层次结构，ZooKeeper实现了一个通用对象[14]。 ZooKeeper服务包括一个使用复制来实现高可用性和性能的服务器集合。它的高性能使包括大量进程的应用程序能够使用这样一个协调内核来管理协调的所有方面。我们能够使用一个简单的流水线架构来实现ZooKeeper，该架构允许我们有成百上千的请求未处理，同时仍然实现低延迟。这样的流水线自然能够以先进先出的顺序执行来自单个客户端的操作。保证先进先出的客户端顺序使客户端能够异步地提交操作。有了异步操作，一个客户端就能在同一时间有多个未完成的操作。这个功能是可取的，例如，当一个新的客户端成为领导者，它必须操作元数据并相应地更新它。如果没有多个未完成操作的可能性，初始化的时间可能是几秒钟，而不是亚秒级的。 为了保证更新操作满足线性化，我们实现了一个基于领导的原子广播协议[23]，称为Zab[24]。然而，ZooKeeper应用程序的典型工作负载是由读操作主导的，因此扩展读吞吐量是可取的。在ZooKeeper中，服务器在本地处理读取操作，我们不使用Zab来对其进行完全排序。 在客户端缓存数据是提高读取性能的一项重要技术。例如，对于一个进程来说，缓存当前领导者的标识符是非常有用的，而不是在每次需要知道领导者的时候去探测ZooKeeper。ZooKeeper使用观察机制，使客户端能够在不直接管理客户端缓存的情形下存储数据。通过这种机制，客户端可以监视某个数据对象的更新，并在更新时收到通知。Chubby直接管理客户端的缓冲区。它阻止更新，使所有缓存被改变的数据的客户端的缓存失效。在这种设计下，如果这些客户端中的任何一个速度慢或有问题，更新就会被延迟。Chubby使用租约来防止一个有问题的客户无限期地阻塞系统。然而，租约只限制了慢的或有问题的客户端的影响，而ZooKeeper的观察机制完全避免了这个问题。 在本文中，我们讨论了我们对ZooKeeper的设计和实现。通过ZooKeeper，我们能够实现我们的应用程序所需要的所有协调原语，尽管只有写是可线性化的。为了验证我们的方法，我们展示了我们如何用ZooKeeper实现一些协调原语。总而言之，本文的主要贡献是： 协调内核：我们提出了一种具有宽松的一致性保证的无等待协调服务，用于分布式系统中。特别是，我们描述了我们对协调内核的设计和实现，我们已经在许多关键的应用中使用它来实现各种协调技术。 协调的秘诀：我们展示了如何使用ZooKeeper来构建更高层次的协调原语，甚至是分布式应用中经常使用的阻塞和强一致性原语。 协调的经验：我们分享一些我们使用ZooKeeper的方法，并评估其性能。 二、 Zookeeper服务客户端通过客户端API使用ZooKeeper客户端库向ZooKeeper提交请求。除了通过客户端API公开ZooKeeper服务接口外，客户端库还负责管理客户端和ZooKeeper服务器之间的网络连接。 在这一节中，我们先从高层次了解ZooKeeper服务。然后我们讨论客户端用来与ZooKeeper交互的API。 术语：在本文中，我们用客户端表示ZooKeeper服务的用户，用服务器表示提供ZooKeeper服务的进程，用znode表示ZooKeeper数据中的一个内存数据节点，它被组织在一个被称为数据树的分层命名空间中。我们还使用update和write来指代任何修改数据树状态的操作。客户端在连接到ZooKeeper时建立一个会话，并获得一个会话句柄，通过它发出请求。 2.1 服务概述ZooKeeper为其客户端提供一组数据节点（znodes）的抽象，这些节点按照分层的名称空间组织。这个层次结构中的znodes是客户通过ZooKeeper API操作的数据对象。分层名称空间通常在文件系统中使用。这是一种理想的数据对象组织方式，因为用户已经习惯了这种抽象，它可以更好地组织应用程序的元数据。为了指代一个给定的znode，我们使用文件系统路径的标准UNIX符号。例如，我们使用 /A/B/C 来表示到Znode C 的路径，其中 C 有 B 作为它的父节点， B 有 A 作为它的父节点。所有的节点都存储数据，并且除了临时节点外的所有的节点，都可以有子节点。 图1：Zookeeper分层命名空间图解。 客户端可以创建两种类型的znodes： 普通的：客户端通过明确地创建和删除它们来操纵常规的znodes。 临时的：客户端创建这样的znodes，他们或者明确地删除它们，或者让系统在创建它们的会话终止时（故意的或由于失败）自动删除它们。 此外，当创建一个新的znode时，客户可以设置一个顺序标志。在设置了顺序标志的情况下创建的节点有一个单调增加的计数器的值附加在它的名字上。如果 n 是新的znode， p 是父znode，那么 n 的序列值永远不会小于 p 下曾经创建的任何其他序列znode的名字中的值。 ZooKeeper实现了观察功能，允许客户端及时收到变化的通知，而不需要轮询。当客户端发出一个设置了watch标志的读操作时，除了服务器承诺在返回的信息发生变化时通知客户端外，操作会正常完成。观察是与会话相关的一次性触发器；一旦被触发或会话关闭，它们就不再被注册。观察器表明发生了变化，但不提供具体变化信息。例如，如果客户端在”/foo”被改变两次之前发出 getData(‘’/foo’’, true) ，客户端只会得到一个观察事件，告诉客户端”/foo “的数据已经改变。会话事件，如连接丢失事件，也会被发送到观察回调，以便客户端知道观察事件可能会被延迟。 数据模型：ZooKeeper的数据模型本质上是一个简化了API的文件系统，只有全数据的读写，或者是一个带有分层键的键/值表。分层的命名空间对于为不同应用程序的名字空间分配子树和设置这些子树的访问权限是很有用的。我们还利用客户端的目录概念来构建更高层次的原语，我们将在第2.4节看到。 与文件系统中的文件不同，znodes不是为一般的数据存储而设计的。相反，znodes映射到客户端应用程序的抽象，通常对应于用于协调目的的元数据。为了说明问题，在图1中我们有两个子树，一个是应用1（/app1），另一个是应用2（/app2）。应用1的子树实现了一个简单的组成员协议：每个客户进程 pi 在/app1下创建一个znode p_i，只要该进程在运行，它就会一直存在。 尽管znodes并不是为一般的数据存储而设计的，但ZooKeeper确实允许客户端存储一些信息，这些信息可以用于分布式计算中的元数据或配置。例如，在基于领导者的应用中，对于刚刚开始的应用服务器来说，了解其他哪台服务器目前是领导者是很有用的。为了实现这个目标，我们可以让当前的领导者在znode空间的一个已知位置上写下这个信息。Znode也有相关的元数据，有时间戳和版本计数器，允许客户端跟踪Znode的变化，并根据Znode的版本执行条件更新。 会话：客户端连接到ZooKeeper并启动一个会话。会话有一个相关的超时时间。如果客户端在超过该超时时间内没有收到任何来自其会话的信息，ZooKeeper就认为它是有问题的。当客户端明确关闭会话句柄或ZooKeeper检测到客户端有问题时，会话就会结束。在一个会话中，客户端观察到的是反映其操作执行情况的一系列状态变化。会话使客户端能够在ZooKeeper集合中透明地从一个服务器移动到另一个服务器，并因此在ZooKeeper服务器之间持续存在。 2.2 客户端API我们在下面介绍ZooKeeper API的相关子集，并讨论每个请求的语义。 create(path, data, flags):创建一个具有路径名称 path 的znode，在其中存储 data[] ，并返回新znode的名称。 flags 使客户端能够选择znode的类型：普通的、临时的，并设置顺序的标识; delete(path, version):如果该znode处于预期的版本中，删除 path 代表的znode; exists(path, watch):如果路径名称为 path 的znode存在，则返回true，否则返回false。 watch 标志使客户端能够在znode上设置一个监视; getData(path, watch):返回与znode相关的数据和元数据，例如版本信息。 watch 标志的工作方式与 exists() 相同，只是如果znode不存在，ZooKeeper不会设置watch; setData(path, data, version):如果版本号是 znode的当前版本 ,写入 data[] 到 znode path; getChildren(path, watch):返回子节点的名称集合; sync(path):等待所有在操作开始时挂起的更新传播到客户端连接的服务器上。path目前被忽略。 所有的方法都有一个同步和异步的版本，可以通过API使用。当一个应用程序需要执行一个单一的ZooKeeper操作，并且没有并发的任务要执行时，它就会使用同步API，所以它进行必要的ZooKeeper调用和阻塞。然而，异步API使一个应用程序能够同时拥有多个未完成的ZooKeeper操作和其他并行执行的任务。ZooKeeper客户端保证每个操作的相应回调被依次调用。 需要注意的是，ZooKeeper不使用句柄来访问znode。每个请求都包括被操作的znode的完整路径。这种选择不仅简化了API（没有open()或close()方法），而且还消除了服务器需要维护的额外状态。 每个更新方法都需要一个预期的版本号，这使得有条件的更新得以实现。如果znode的实际版本号与预期的版本号不一致，则更新失败，出现意外的版本错误。如果版本号为-1，则不执行版本检查。 2.3 ZooKeeper的保证ZooKeeper有两个基本的排序保证： 可线性化的写入：所有更新ZooKeeper状态的请求都是可序列化的，并尊重优先权； FIFO的客户端顺序：来自特定客户端的所有请求都按照客户端发送的顺序执行。 请注意，我们对可线性化的定义与Herlihy[15]最初提出的定义不同，我们称之为A-线性化（异步线性化）。在Herlihy对线性化的最初定义中，一个客户端在同一时间只能有一个未完成的操作（一个客户端就是一个线程）。在我们的定义中，我们允许一个客户端有多个未完成的操作，因此我们可以选择保证同一客户端的未完成操作没有特定的顺序，或者保证先进先出的顺序。我们选择后者作为我们的属性。需要注意的是，所有对可线性化对象成立的结果也对A-线性化对象成立，因为一个满足A-线性化的系统也满足线性化。因为只有更新请求是可线性化的，ZooKeeper在每个副本中都会本地处理读取请求。这使得服务可以随着服务器的增加而线性地扩展到系统中。 为了了解这两种保证是如何相互作用的，请考虑以下情况。一个由若干进程组成的系统选出一个领导者来命令worker进程。当一个新的领导者负责该系统时，它必须改变大量的配置参数，并在完成后通知其他进程。这样我们必须满足如下两个要求： 当新的领导者开始进行改变时，我们不希望其他进程开始使用正在被改变的配置； 如果新的领导者在配置完全更新之前死亡，我们不希望进程使用这个部分配置。 请注意，分布式锁，如Chubby提供的锁，将有助于满足第一个要求，但不足以满足第二个要求。在ZooKeeper中，新的领导者可以指定一个路径作为ready znode；其他进程只有在该znode存在时才会使用配置。新的领导者通过删除ready，更新各种配置的znode，并创建ready来进行配置的改变。所有这些变化都可以通过流水线和异步发布来快速更新配置状态。虽然改变操作的延迟是2毫秒的量级，但如果一个新的领导者必须更新5000个不同的znodes，如果请求是一个接一个地发出，将需要10秒钟；通过异步发出请求，请求将花费不到一秒钟。由于排序的保证，如果一个进程看到了ready znode，它也必须看到新的领导者所做的所有配置改变。如果新的领导者在ready znode创建之前就死亡，其他进程知道配置还没有最终完成，就不会使用它。 上述方案仍有一个问题：如果一个进程在新的领导者开始进行变更之前看到ready存在，然后在变更进行时开始读取配置，会发生什么情况。这个问题通过对通知的排序保证得到了解决：如果一个客户端正在监听变更，客户端将在看到变更后系统的新状态之前看到通知事件。因此，如果读取ready znode的进程要求被通知该znode的变更，它将在读取任何新的配置之前看到通知，告知客户端的变更。 当客户端在ZooKeeper之外还有自己的通信通道时，会出现另一个问题。例如，考虑到两个客户端A和B在ZooKeeper中拥有一个共享配置，并通过一个共享通信通道进行通信。如果A改变了ZooKeeper中的共享配置，并通过共享通信通道告诉B这个改变，B就会在重新读取配置时看到这个改变。如果B的ZooKeeper副本稍微落后于A的，它可能看不到新的配置。利用上述保证，B可以通过在重新读取配置之前发出一个写入指令来确保它看到最新的信息。为了更有效地处理这种情况，ZooKeeper提供了sync请求：当紧随其后的是读，就构成了慢速读。sync使服务器在处理读之前应用所有悬而未决的写请求，而不需要完全写的开销。这个原语与ISIS[5]的flush原语的想法相似。 ZooKeeper还具有以下两个有效性和持久性保证：如果大多数ZooKeeper服务器处于活动状态并进行通信，那么服务将是可用的；如果ZooKeeper服务成功地响应了一个变更请求，那么只要有一组 quorum 服务器最终能够恢复，该变更在任何数量的故障中都会持续。 2.4 原语的例子在本节中，我们将展示如何使用ZooKeeper的API来实现更强大的原语。ZooKeeper服务对这些更强大的原语一无所知，因为它们完全是在客户端使用ZooKeeper客户端API实现的。一些常见的原语，如群组成员和配置管理也是无需等待的。对于其他的原语，如rendezvous，客户端需要等待一个事件。尽管ZooKeeper是无等待的，但我们可以用ZooKeeper实现高效的阻塞原语。ZooKeeper的排序保证允许对系统状态进行有效的推理，而观察机制允许有效的等待。 配置管理 ZooKeeper可用于在分布式应用程序中实现动态配置。在其最简单的形式中，配置被存储在一个znode中，即 zc 。进程启动时使用 zc 的完整路径名。启动进程通过读取 zc 来获得他们的配置，并将watch标志设置为true。如果 zc 中的配置被更新，进程会被通知并读取新的配置，并再次将watch标志设为true。 请注意，在这个方案中，和其他大多数使用观察的方案一样，观察被用来确保进程拥有最新的信息。例如，如果一个观察 zc 的进程被通知了 zc 的变化，而在它发出对 zc 的读取之前， zc 又有三个变化，那么这个进程就不会再收到三个通知事件。这并不影响进程的行为，因为这三个事件只是通知进程它已经知道的事情：它拥有的关于 zc 的信息是过时的。 会和（Rendezvous） 有时候在分布式系统中并不能清晰的预知系统的最终配置会是什么情形。例如，某个客户端可能会希望启动一个master进程和几个worker进程，不过由于节点的启动是由某个调度器执行，客户端并不能事先知道某些需要的信息，例如工作节点需要连接的主节点的地址和端口号。这个问题可以由客户端通过 ZooKeeper 创建一个 rendezvous 节点 zr 来解决。客户端将 zr 的全路径作为启动参数传给master和worker进程。当master启动后，它就将自己的地址和端口号写入到 zr 中。当woker启动后，它就能从 zr 中读取，并设置watch标识味true。如果 zr 仍未被填满，那么worker就会等待 zr 更新的通知。如果 zr 是临时节点，master和worker就能观察到 zr 被删除通知，并在完成资源清理后退出。 群组成员 我们可以利用临时节点的特性来实现群组成员关系管理。具体来说，我们利用了临时节点允许观测创建该节点的 session 状态这一特性。首先创建一个节点 zg 来表示群组。当群组中的某个进程启动时，它会在 zg 下创建一个临时的子节点。如果每个进程都有唯一的命名或标识，那么这个命名或标识就可以作为 ZooKeeper 节点的名称；否则就可以在创建节点时设置 SEQUENTIAL 标记让 ZooKeeper 自动在节点名称后追加一个单调递增的数字，以保证名称的唯一性。各进程可以将进程相关的信息放到子节点中，例如当前进程的地址和端口号。 在节点 zg 下创建完子节点后，进程就可以正常启动。它不需要做其他任何事。如果这个进程失败或者结束，那么它所创建的 zg 下的子节点也会自动被删除。 各进程可以简单的通过查询 zg 的所有子节点来获取当前群组成员的信息。如果某个进程想要监控群组成员的变化，那么它可以设置watch标记为true（总是设置watch标记为true），当它收到变更通知时，就可以刷新群组信息。 简单锁 ZooKeeper虽然不是锁服务，但是可以用来实现锁。 使用 ZooKeeper 的应用程序通常使用根据其需要定制的同步原语，例如上面显示的那些。 这里我们展示如何使用 ZooKeeper 实现锁，以表明它可以实现各种通用同步原语。 最简单的锁实现借助于 lock files。使用一个 znode 来表示一把锁。为了获取锁，客户端会尝试以 EPHEMERAL 标记创建指定的 znode。如果创建成功，那么这个客户端就获得了锁。否则，客户端就会去读取这个 znode 并设置watch标识，从而当这个领导者被删除时能收到通知。当持有锁的客户端发生异常或者主动删除该节点时，则代表释放了锁。其他监听的客户端就会收到通知并尝试重新创建临时节点来获取锁。 虽然这个简单的锁定协议管用，但它确实存在一些问题。 首先，它受到惊群效应的影响。 如果有很多客户端在等待获取锁，即使只有一个客户端可以获取锁，当锁被释放时，它们也会争抢。 其次，它只实现了排它锁。以下两个原语显示了如何克服这两个问题。 无惊群效应的简单锁 我们定义一个锁znode l 来实现这样的锁。直观上，我们排列所有请求锁的客户端，每个客户端都按请求到达的顺序获得锁。因此，希望获得锁的客户端会有如下操作: 12345678910Lock1 n = create(l + “/lock-”, EPHEMERAL|SEQUENTIAL)2 C = getChildren(l, false)3 if n is lowest znode in C, exit4 p = znode in C ordered just before n5 if exists(p, true) wait for watch event6 goto 2Unlock1 delete(n) 第1行 SEQUENTIAL 的标记用来将所有希望获取锁的客户端进行排序。每个客户端首先在节点 l 下创建一个临时顺序的子节点，然后获取 l 的所有子节点。之后在第3行判断自己创建的节点是否在所有子节点中有着最小的序号，如果是，则表示当前客户端获得了锁。如果不是，说明有其他序号更小的子节点存在，当前客户端需要排在这之后获取锁。然后客户端会尝试判断排在当前序号前的子节点是否存在，如果存在则设置监听状态等待前一个节点删除的通知，如果不存在，则继续回到第2行执行。每个客户端只监听排在自己前面的子节点避免了惊群效应，因为任何一个子节点删除的通知只会发给其中的一个客户端。每当客户端收到前面节点删除的通知时，需要再次获取 l 的所有子节点来判断自己是否是最小子节点。（因为排在前面的子节点并不一定持有锁，可能是更前面的子节点持有锁。） 释放锁就是简单的删除对应的临时节点 _n_。而通过 EPHEMERAL 标记创建节点能保证进程异常时自动释放锁或者放弃对锁的获取请求。 这种锁实现有以下几个优势： 一个节点的删除只会唤醒一个客户端，因为每个节点都只会被一个客户端监听，所以也不会有惊群效应。 锁的获取和释放不依赖轮询或超时。 使用这种方式创建锁使得可以通过查看 ZooKeeper 中的数据来监测锁竞争的数量，以及调试锁相关的问题。 读写锁 为了实现读写锁，我们稍微改变了锁相关的程序代码，并有独立的读锁和写锁过程。释放过程与普通锁的情况相同。 123456789101112131415Write Lock1 n = create(l + “/write-”, EPHEMERAL|SEQUENTIAL)2 C = getChildren(l, false)3 if n is lowest znode in C, exit4 p = znode in C ordered just before n5 if exists(p, true) wait for event6 goto 2Read Lock1 n = create(l + “/read-”, EPHEMERAL|SEQUENTIAL)2 C = getChildren(l, false)3 if no write znodes lower than n in C, exit4 p = write znode in C ordered just before n5 if exists(p, true) wait for event6 goto 3 该锁的过程与之前的锁略有不同。 写锁仅在命名上有所不同。 由于读锁可能是共享的，第3行和第4行略有不同，因为只有较早的写锁 znode 会阻止客户端获得读锁。 当有多个客户端等待读锁并且收到删除具有较低序列号的“写”znode的通知时，我们可能会出现“惊群效应”； 事实上，这种行为是我们锁期望的，所有那些读的客户端都应该被释放，因为它们现在可能已经持有了该锁。 双重屏障 双重屏障使客户端能够同步一个计算的开始和结束。当由屏障阈值定义的足够多的进程加入屏障时，进程开始计算，并在完成后离开屏障。 我们用 Znode 来表示ZooKeeper的屏障，称为 _b_。 每一个进程 p 都在 b 通过创建一个 Znode 作为 b 的子节点来注册，在它准备好离开时取消注册。 当 b 的子 Znode 的数目超过屏障阈值时，进程可以进入屏障。 进程可以在所有的进程都已移除其子节点时离开屏障。 我们使用监视器高效地等待进入和退出条件得到满足。 如果要进入屏障的话，进程需要监视 b 的 ready 状态的子节点，当子节点数量超过屏障阈值时，该进程才可以进入。 如果要离开屏障，进程监视某个特定的子节点消失，并且只有在这个特定的 Znode 被删除后才检查退出条件。 三、 Zookeeper应用我们下面会描述一些使用了ZooKeeper的应用程序，并简要说明它们是怎样使用的。我们用粗体显示每个例子的原语。 The Fetching Service 爬虫是搜索引擎的重要组成部分，Yahoo!抓取数十亿个Web文档。The Fetching Service（FS）是Yahoo！爬虫的一部分，并且已经投入生产。从本质上讲，它由主进程向页面爬取进程主进程发出命令。主进程为爬取进程提供配置，爬取进程回写通知其状态和运行状况。在FS中使用 ZooKeeper 的主要优点是从主节点的故障中恢复，在发生故障时保证可用性，以及将客户端与服务器分离，允许它们通过从 ZooKeeper 读取其状态来将请求定向到健康的服务器。因此，FS 主要使用 ZooKeeper 来管理配置元数据，尽管它也使用 ZooKeeper 来选举主节点（领导者选举）。 图2：FS中Zookeeper的工作负载。每个点代表了一秒的采样。 图2显示了 FS 使用的 ZooKeeper 服务器在三天内的读写流量。为了生成此图，我们统计了这段时间内每秒的操作数，每个点对应一秒。我们观察到读取流量比写入流量高得多。 在速率高于每秒 1, 000 次操作期间，读：写比率在 10:1 和 100:1 之间变化。 此工作负载中的读取操作是 _getData()、_getChildren() 和 _exists()_，按普遍程度递增的顺序排列。 Katta Katta [17] 是一个使用 ZooKeeper 进行协调的分布式索引器，它是非 Yahoo! 应用。 Katta 使用分片来划分索引的工作。 主服务器将分片分配给从属服务器并跟踪进度。 从站可能会发生故障，因此主站必须在从站来来去去时重新分配负载。 主服务器也可能发生故障，因此其他服务器必须准备好在发生故障时接管。 Katta 使用 ZooKeeper 来跟踪从服务器和主服务器的状态（群组成员），并处理主服务器故障转移（领导者选举）。 Katta 还使用 ZooKeeper 来跟踪和传播分片分配给从属（配置管理）。 Yahoo! Message Broker Yahoo! Message Broker (YMB) 是一个分布式发布-订阅系统。 该系统管理着数千个主题，客户端可以向这些主题发布消息或从中接收消息。 主题分布在一组服务器中以提供可伸缩性。 每个主题都使用主备方案进行复制，该方案确保将消息复制到两台机器以确保可靠的消息传递。 构成 YMB 的服务器使用无共享分布式架构，这使得协调对于正确操作至关重要。 YMB 使用 ZooKeeper 来管理主题的分发（配置元数据），处理系统中机器的故障（故障检测和群组成员），以及控制系统运行。 图3：YMB在Zookeeper中的架构布局。 图 3 显示了 YMB 的部分 znode 数据布局。 每个broker域都有一个称为 nodes 的 znode，它为组成 YMB 服务的每个活动服务器提供一个临时 znode。 每个 YMB 服务器在 nodes 下创建一个临时 znode，其中包含负载和状态信息，通过 ZooKeeper 提供组成员资格和状态信息。 shutdown 和 migration_prohibited 等节点由构成服务的所有服务器监控，并允许对 YMB 进行集中控制。 对于 YMB 管理的每个主题，topics 目录都有一个子 znode。 这些主题 znode 具有子 znode，它们指示每个主题的主服务器和备用服务器以及该主题的订阅者。 primary 和 backup 服务器 znode 不仅可以让服务器发现负责某个主题的服务器，还可以管理领导者选举和服务器崩溃。 四、 Zookeeper实现ZooKeeper通过在组成服务的每个服务器上复制ZooKeeper数据来提供高可用性。我们假设服务器会因崩溃而失败，而这些有问题的服务器后来可能会恢复。图4显示了ZooKeeper服务的高级组件。在收到一个请求后，一个服务器准备执行它（请求处理器）。如果这样的请求需要服务器之间的协调（写请求），那么他们就会使用一个协议（原子广播的实现），最后服务器将变化提交给ZooKeeper数据库，并在该集合的所有服务器上完全复制。在读请求的情况下，服务器只需读取本地数据库的状态并生成对请求的响应。 图4：Zookeeper服务组件。 复制的数据库是一个包含整个数据树的内存数据库。树上的每个节点默认存储最大1MB的数据，但这个最大值是一个配置参数，可以在特定情况下改变。为了保证可恢复性，我们有效地将更新记录到磁盘上，并强制写在磁盘介质上，然后再应用到内存数据库上。事实上，就像Chubby[8]一样，我们保留了一个已提交操作的重放日志（在我们的例子中，是一个写前日志），并定期生成内存数据库的快照。 每个ZooKeeper服务器都为客户提供服务。客户端正好连接到一个服务器来提交其请求。如前所述，读取请求是由每个服务器数据库的本地副本提供服务。改变服务状态的请求，即写请求，由一个一致性协议处理。 作为一致性协议的一部分，写请求被转发到一个单独的服务器，称为领导者1。其余的ZooKeeper服务器，称为追随者，从领导者那里接收由状态变化组成的消息提议，并同意状态变化。 4.1 请求处理器由于消息传递层是原子的，我们保证本地副本永远不会出现分歧，尽管在任何时候，一些服务器可能比其他服务器应用了更多的事务。与客户发送的请求不同，事务是幂等的。当领导者收到一个写请求时，它计算当写被应用时系统的状态是什么，并将其转换为一个捕捉这个新状态的事务。未来的状态必须被计算出来，因为可能有未完成的事务还没有被应用到数据库中。例如，如果一个客户做了一个附带条件的 setData ，并且请求中的版本号与被更新的节点的未来版本号相匹配，服务会生成一个 setDataTXN ，其中包含新数据、新版本号和更新的时间戳。如果发生错误，例如版本号不匹配或被更新的节点不存在，则会生成一个 errorTXN 。 4.2 原子广播所有更新ZooKeeper状态的请求都被转发给领导者。领导者执行请求，并通过Zab[24]这个原子广播协议广播ZooKeeper状态的变化。收到客户端请求的服务器在传递相应的状态变化时对客户端进行响应。Zab默认使用简单的多数 quorums 机制来决定提案，所以Zab以及ZooKeeper只有在大多数服务器都正确的情况下才能工作（也就是说，在2f + 1个服务器的情况下，我们可以容忍f个失败）。 为了实现高吞吐量，ZooKeeper试图让请求处理管道保持满员。它可能在处理管道的不同部分有成千上万的请求。因为状态的改变取决于先前状态改变的应用，Zab提供了比常规原子广播更强的顺序保证。更具体地说，Zab保证领导者广播的变化按照它们被发送的顺序传递，并且在它广播自己的变化之前，所有来自先前领导者的变化都被传递给已建立的领导者。 有一些实现细节简化了我们的实现，并给我们带来了出色的性能。我们使用TCP进行传输，所以消息的顺序由网络来维持，这使我们能够简化我们的实现。我们使用Zab选择的领导者作为ZooKeeper的领导者，这样，创建事务的进程也会提议事务。我们使用日志来跟踪提议，作为内存数据库的写前日志，这样我们就不必将信息两次写入磁盘。 在正常的操作中，Zab确实按顺序准确地传递了所有的消息，但是由于Zab没有持久地记录每条传递的消息的ID，Zab可能在恢复期间重新传递一条消息。因为我们使用幂等型事务，所以只要是按顺序传递的，多次传递是可以接受的。事实上，ZooKeeper要求Zab至少重新交付所有在最后一次快照开始后交付的消息。 4.3 复制数据库每个副本在内存中都有一份ZooKeeper状态的副本。当ZooKeeper服务器从崩溃中恢复时，它需要恢复这个内部状态。在服务器运行一段时间后，重放所有传递的消息来恢复状态会花费太多时间，所以ZooKeeper使用定期快照，只要求重新传递快照开始后的消息。我们称ZooKeeper快照为模糊快照（_fuzzy snapshot_），因为我们不锁定ZooKeeper状态来进行快照；相反，我们对树进行深度扫描，原子式地读取每个znode的数据和元数据，并将它们写入磁盘。由于产生的模糊快照可能已经应用了快照生成过程中的一些状态变化子集，所以结果可能不符合ZooKeeper在任何时间点的状态。然而，由于状态变化是等价的，只要我们按顺序应用状态变化，我们就可以接受它们两次。 例如 例如，假设在 ZooKeeper 数据树中，两个节点 /foo 和 /goo 的值分别为 f1 和 _g1_，并且在模糊快照开始时都处于版本 1，并且以下状态更改流到达时具有以下形式 &lt;transactionType, path, value, new-version&gt;: 123&lt;SetDataTXN, /foo, f2, 2&gt;&lt;SetDataTXN, /goo, g2, 2&gt;&lt;SetDataTXN, /foo, f3, 3&gt; 处理完这些状态更改后，_/foo_ 和 /goo 的值分别为版本 3 和版本 2 的 f3 和 _g2_。 然而，模糊快照可能记录了 /foo 和 /goo 分别具有版本 3 和 1 的值 f3 和 _g1_，这不是 ZooKeeper 数据树的有效状态。 如果服务器崩溃并使用此快照恢复并且 Zab 重新交付状态更改，则结果状态对应于崩溃前的服务状态。 4.4 客户端-服务端交互当一个服务器处理一个写请求时，它也会发送和清除与该更新相对应的任何观察的通知。服务器按顺序处理写，不同时处理其他写或读。这确保了通知的严格连续。请注意，服务器在本地处理通知。只有客户端连接的服务器才会跟踪并触发该客户端的通知。 读取请求在每个服务器上进行本地处理。每个读取请求都会被处理，并被标记为一个 zxid ，该 zxid 对应于服务器看到的最后一个事务。这个 zxid 定义了读请求相对于写请求的部分顺序。通过本地处理读取，我们获得了出色的读取性能，因为它只是本地服务器上的一个内存操作，没有磁盘活动或协议运行。这一设计选择是实现我们对以读为主的工作负载的卓越性能目标的关键。 使用快速读取的一个缺点是不能保证读取操作的优先顺序。也就是说，一个读操作可能会返回一个陈旧的值，即使最近对同一个 znode 的更新已经提交。并非所有的应用都需要优先顺序，但对于需要优先顺序的应用，我们已经实现了同步。这个原语是异步执行的，并由领导者在对其本地副本的所有未决写操作之后进行排序。为了保证一个给定的读操作返回最新的更新值，客户端在读操作之后调用 sync 。客户端操作的FIFO顺序保证和同步的全局保证使得读取操作的结果能够反映在同步发出之前发生的任何变化。在我们的实现中，我们不需要原子化地广播同步，因为我们使用的是基于领导者的算法，我们只需将同步操作放在领导者和执行同步调用的服务器之间的请求队列的最后。为了使这一方法奏效，追随者必须确定领导者仍然是领导者。如果有未决的事务提交，那么服务器就不会怀疑领导者。如果挂起的队列是空的，领导者需要发出一个空事务来提交，并在该事务之后下令同步。这有一个很好的特性，就是当领导者处于负载状态时，不会产生额外的广播流量。在我们的实现中，超时的设置使领导者在追随者放弃他们之前意识到他们不是领导者，所以我们不发布空事务。 ZooKeeper服务器按照先进先出的顺序处理来自客户端的请求。响应包括响应所对应的 zxid 。甚至在没有活动的间隔期间的心跳信息也包括客户端连接到的服务器所看到的最后一个 zxid 。如果客户端连接到一个新的服务器，新的服务器会通过检查客户端的最后一个 zxid 和它的最后一个 zxid 来确保它对ZooKeeper数据的查看至少和客户端的查看一样是最新的。如果客户端的视图比服务器的更新，服务器就不会重新建立与客户端的会话，直到服务器赶上。由于客户端只看到复制到大多数ZooKeeper服务器上的变化，因此可以保证客户端能够找到另一个拥有最新视图的服务器。这种行为对于保证持久性非常重要。 ZooKeeper使用超时来检测客户端会话的失败。如果在会话超时内没有其他服务器收到来自客户端会话的任何信息，领导者就会确定发生了故障。如果客户端发送的请求足够频繁，那么就不需要发送任何其他消息。否则，客户端会在活动少的时期发送心跳消息。如果客户端不能与服务器通信以发送请求或心跳，它会连接到不同的ZooKeeper服务器以重新建立会话。为了防止会话超时，ZooKeeper客户端库在会话空闲 s/3 ms后发送心跳信息，如果2s/3 ms内没有服务器的消息，则切换到新的服务器，其中s是会话超时时间，单位为毫秒。 五、 评估我们在一个50台服务器组成的集群上做了所有评估。每台服务器都配有有 Xeon 2.1GHz 的双核处理器，4GB内存，千兆以太网，以及2块SATA硬盘。我们将后面的讨论分为两部分：吞吐量和请求的延迟。 5.1 吞吐量为了评估我们的系统，我们对系统饱和时的吞吐量以及各种注入的故障的吞吐量变化进行了基准测试。我们改变了组成ZooKeeper服务的服务器的数量，但始终保持客户端的数量不变。为了模拟大量的客户，我们用35台机器来模拟250个同时进行的客户。 我们有一个Java实现的ZooKeeper服务器，以及Java和C客户端。在这些实验中，我们使用Java服务器，配置为在一个专用磁盘上记录，在另一个磁盘上进行快照。我们的基准客户端使用异步的Java客户端API，每个客户端至少有100个请求未完成。每个请求包括对1K数据的读或写。我们没有显示其他操作的基准，因为所有修改状态的操作的性能大致相同，而非状态修改操作的性能，不包括同步，也大致相同。(同步的性能接近于轻量级的写，因为请求必须送到领导那里，但不会被广播。) 客户端每300ms发送一次已完成操作数的计数，我们每6s采样一次。为了防止内存溢出，服务器对系统中的并发请求数量进行节制。ZooKeeper使用请求节流来保持服务器不被淹没。在这些实验中，我们将ZooKeeper服务器配置为最多有2,000个总请求在处理中。 图5：饱和系统随读写比率吞吐量变化。 表1：饱和系统极限吞吐量。 在图5中，我们显示了当我们改变读和写请求的比例时的吞吐量，每条曲线对应于提供ZooKeeper服务的不同数量的服务器。表1显示了在读取负载的极端情况下的数字。读取吞吐量比写入吞吐量高，因为读取不使用原子广播。该图还显示，服务器的数量对广播协议的性能也有负面影响。从这些图表中，我们观察到，系统中的服务器数量不仅影响到服务可以处理的故障数量，而且还影响到服务可以处理的工作量。请注意，三台服务器的曲线在60%左右与其他服务器相交。这种情况并不是三台服务器配置所独有的，由于本地读取的并行性，所有的配置都会发生。然而，在图中的其他配置中无法观察到这种情况，因为我们为可读性设定了Y轴最大吞吐量的上限。 有两个原因导致写请求比读请求耗时更长。首先，写请求必须经过原子广播，这需要一些额外的处理并增加请求的延迟。写请求处理时间较长的另一个原因是，服务器必须确保在向领导发送确认信息之前，将事务记录到非易失性存储中。原则上，这个要求是过分的，但对于我们的生产系统，我们用性能来换取可靠性，因为ZooKeeper构成了应用的基础真理。我们使用更多的服务器来容忍更多的故障。我们通过将ZooKeeper数据分割成多个ZooKeeper集合来增加写入量。Gray等人[12]曾观察到复制和分区之间的这种性能权衡。 图6：所有客户端连接到领导者时，饱和系统随读写比率吞吐量变化。 ZooKeeper能够通过在组成服务的服务器之间分配负载来实现如此高的吞吐量。我们可以分配负载，因为我们有宽松的一致性保证。Chubby客户端会将所有的请求指向领导者。图6显示了如果我们不利用这种放松，强迫客户只连接到领导者，会发生什么。正如预期的那样，以读为主的工作负载的吞吐量要低得多，但即使是以写为主的工作负载的吞吐量也要低。为客户提供服务所造成的额外的CPU和网络负载影响了领导者协调建议广播的能力，这反过来又对整个写性能产生了不利影响。 图7：孤立原子广播的平均吞吐量。误差线表示了最大最小值。 原子广播协议完成了系统的大部分工作，因此对ZooKeeper性能的限制超过了其他任何组件。图7显示了原子广播组件的吞吐量。为了衡量其性能，我们通过直接在领导者处生成事务来模拟客户，所以没有客户连接或客户请求和回复。在最大的吞吐量下，原子广播组件成为CPU的约束。理论上，图7的性能将与ZooKeeper的100%写入性能相匹配。然而，ZooKeeper客户端通信、ACL检查和请求到事务的转换都需要CPU。对CPU的争夺降低了ZooKeeper的吞吐量，大大低于孤立的原子广播组件。因为ZooKeeper是一个重要的生产组件，到目前为止，我们对ZooKeeper的开发重点是正确性和健壮性。有很多机会可以通过消除额外的拷贝、同一对象的多次序列化、更有效的内部数据结构等来大幅提高性能。 图8：故障情形下的吞吐量。 为了显示系统随着时间的推移在故障注入时的行为，我们运行了一个由5台机器组成的ZooKeeper服务。我们运行了与之前相同的饱和基准，但这次我们将写入比例保持在30%，这是我们预期工作负载的一个保守比例。我们定期地杀死一些服务器进程。图8显示了系统的吞吐量随时间的变化情况。图中标出的事件有以下几点： 一个追随者的故障和恢复； 一个不同的追随者的故障和恢复； 领导者的故障； 两个追随者（a，b）在前两个标记中故障，在第三个标记中恢复（c）； 领导者的失败； 领导者的恢复； 从这张图中有几个重要的观察。首先，如果追随者故障并迅速恢复，那么ZooKeeper能够在故障的情况下保持高吞吐量。一个追随者的故障并不妨碍服务器形成一个法定人数，而且只减少了服务器在故障前所处理的读取请求的份额，所以吞吐量大致如此。其次，我们的领导者选举算法能够快速恢复，足以防止吞吐量大幅下降。根据我们的观察，ZooKeeper选举一个新的领导者需要不到200ms。因此，尽管服务器在几分之一秒内停止提供请求，但由于我们的采样周期为秒级，我们没有观察到吞吐量为零的情况。第三，即使追随者需要更多的时间来恢复，一旦他们开始处理请求，ZooKeeper也能够再次提高吞吐量。在事件1、2和4之后，我们没有恢复到完整的吞吐量水平，原因之一是客户端只有在与追随者的连接中断时才会切换追随者。因此，在事件4之后，客户端没有重新分配自己，直到事件3和5的领导者失败。在实践中，这种不平衡随着时间的推移，随着客户端的到来和离开而自行解决。 5.2 请求延迟为了评估请求的延迟，我们创建了一个以Chubby基准[6]为模型的基准测试。我们创建了一个工作进程，简单地发送一个创建，等待它完成，发送一个新节点的异步删除，然后开始下一个创建。我们相应地改变工人的数量，在每次运行中，我们让每个工人创建50,000个节点。我们通过将完成的创建请求数除以所有工作者完成的总时间来计算吞吐量。 表2：每秒创建请求处理。 表2显示了我们基准测试的结果。创建请求包括1K的数据，而不是Chubby基准中的5字节，以更好地与我们的预期使用相吻合。即使有这些较大的请求，ZooKeeper的吞吐量也比Chubby公布的吞吐量高3倍多。单个ZooKeeper工作者基准的吞吐量表明，3台服务器的平均请求延迟为1.2ms，9台服务器为1.4ms。 5.3 屏障的性能 表3：秒级屏障实验。每个点都是客户端运行5次后的平均值。 在该实验中，我们按顺序执行一些障碍，以评估用ZooKeeper实现的原语的性能。对于给定的障碍数量b，每个客户端首先进入所有b个障碍，然后连续离开所有b个障碍。由于我们使用了第2.4节的双壁垒算法，一个客户端首先等待所有其他客户端执行enter()程序，然后再进入下一个调用（与leave()类似）。 我们在表3中报告了我们的实验结果。在该实验中，我们有50、100和200个客户端连续进入一定数量的障碍，b∈{200、400、800、1600}。尽管一个应用程序可以有成千上万的ZooKeeper客户端，但相当多的时候，只有一个小得多的子集参与每个协调操作，因为客户端通常是根据应用程序的具体情况分组的。 从这个实验中可以看到两个有趣的现象：处理所有障碍的时间与障碍的数量大致呈线性增长，这表明对数据树的同一部分的并发访问并没有产生任何意外的延迟，而延迟的增加与客户端的数量成比例。这是不使ZooKeeper服务饱和的结果。事实上，我们观察到，即使客户端以锁步方式进行，在所有情况下，障碍操作（进入和离开）的吞吐量都在每秒1,950到3,100次之间。在ZooKeeper操作中，这相当于每秒10,700到17,000个操作的吞吐量值。由于在我们的实现中，我们的读与写的比例为4:1（80%的读操作），我们的基准代码使用的吞吐量与ZooKeeper可以达到的原始吞吐量（根据图5，超过40,000）相比要低很多。这是由于客户端在等待其他客户端。 六、 相关工作ZooKeeper的目标是提供一种服务，缓解分布式应用中协调进程的问题。为了实现这个目标，它的设计采用了以前的协调服务、容错系统、分布式算法和文件系统的思想。 我们并不是第一个提出分布式应用协调系统的人。一些早期的系统提出了用于事务性应用的分布式锁服务[13]，以及用于计算机集群中的信息共享[19]。最近，Chubby提出了一个为分布式应用管理咨询锁的系统[6]。Chubby分享了ZooKeeper的几个目标。它也有一个类似文件系统的接口，并且它使用一个协议来保证副本的一致性。然而，ZooKeeper不是一个锁服务。它可以被客户端用来实现锁，但它的API中没有锁操作。与Chubby不同，ZooKeeper允许客户端连接到任何ZooKeeper服务器，而不仅仅是领导者。ZooKeeper客户端可以使用他们的本地副本来提供数据和管理观察器，因为它的一致性模型要比Chubby宽松得多。这使得ZooKeeper能够提供比Chubby更高的性能，使应用程序能够更广泛地使用ZooKeeper。 文献中已经提出了一些容错系统，目的是为了减轻建立容错的分布式应用的问题。一个早期的系统是ISIS[5]。ISIS系统将抽象的类型规范转化为容错的分布式对象，从而使容错机制对用户透明。Horus[30]和Ensemble[31]是由ISIS发展而来的系统。ZooKeeper接纳了ISIS的虚拟同步概念。最后，Totem在一个利用局域网硬件广播的架构中保证了消息传递的总顺序[22]。ZooKeeper在各种网络拓扑结构下工作，这促使我们依靠服务器进程之间的TCP连接，而不假设任何特殊的拓扑结构或硬件特征。我们也没有公开ZooKeeper内部使用的任何集合通信。 构建容错服务的一个重要技术是状态机复制[26]，而Paxos[20]是一种算法，能够有效实现异步系统的状态机复制。我们使用的算法具有Paxos的一些特点，但它将共识所需的交易日志与数据树恢复所需的写前日志结合起来，以实现高效的实施。已经有一些关于实际实现拜占庭容忍复制状态机的协议建议[7, 10, 18, 1, 28]。ZooKeeper并不假定服务器可以是拜占庭的，但我们确实采用了诸如校验和和理智检查等机制来捕捉非恶意的拜占庭故障。Clement等人讨论了一种使ZooKeeper完全拜占庭式容错的方法，而无需修改当前的服务器代码库[9]。到目前为止，我们还没有在生产中观察到使用完全拜占庭容错协议可以防止的故障。[29]. Boxwood[21]是一个使用分布式锁服务器的系统。Boxwood为应用程序提供了更高层次的抽象，它依赖于一个基于Paxos的分布式锁服务。与Boxwood一样，ZooKeeper也是一个用于构建分布式系统的组件。然而，ZooKeeper有高性能的要求，并且更广泛地用于客户端应用程序。ZooKeeper暴露了低级别的基元，应用程序用它来实现高级别的基元。 ZooKeeper类似于一个小型的文件系统，但它只提供了文件系统操作的一个小子集，并增加了大多数文件系统不具备的功能，如排序保证和条件写入。然而，ZooKeeper监视在精神上与AFS[16]的缓存回调相似。 Sinfonia[2]引入了迷你交易，这是一种构建可扩展分布式系统的新模式。Sinfonia被设计用来存储应用数据，而ZooKeeper则存储应用元数据。ZooKeeper将其状态完全复制并保存在内存中，以获得高性能和一致的延迟。我们使用类似文件系统的操作和排序，实现了类似于小型交易的功能。znode是一个方便的抽象，我们在此基础上添加了观察器，这是Sinfonia中缺少的功能。Dynamo[11]允许客户在一个分布式键值存储中获取和放置相对较小（小于1M）的数据量。与ZooKeeper不同，Dynamo的键空间不是分层的。Dynamo也没有为写入提供强大的耐久性和一致性保证，而是在读取时解决冲突。 DepSpace[4]使用元组空间来提供拜占庭式的容错服务。像ZooKeeper一样，DepSpace使用一个简单的服务器接口，在客户端实现强大的同步基元。虽然DepSpace的性能比ZooKeeper低得多，但它提供了更强的容错和保密性保证。 七、 结论ZooKeeper采用了一种无等待的方法来解决分布式系统中协调进程的问题，它向客户公开无等待的对象。我们发现ZooKeeper对雅虎内部和外部的一些应用很有用。ZooKeeper通过使用快速读取和观察，实现了每秒数十万次的操作，这些操作都是由本地副本提供的，从而实现了以读为主的工作负载。虽然我们对读和监视的一致性保证似乎很弱，但我们的用例表明，这种组合允许我们在客户端实现高效和复杂的协调协议，即使读不是按优先级排序的，数据对象的实现是无等待的。事实证明，无等待的特性对高性能是至关重要的。 虽然我们只描述了几个应用，但还有很多其他的应用在使用ZooKeeper。我们相信这样的成功是由于其简单的接口以及人们可以通过这个接口实现的强大的抽象功能。此外，由于ZooKeeper的高吞吐量，应用程序可以广泛地使用它，不仅仅是过程粒度的锁。","link":"/2022/12/31/ZooKeeper-Wait-free-coordination-for-Internet-scale-systems/"}],"tags":[{"name":"distributed system","slug":"distributed-system","link":"/tags/distributed-system/"},{"name":"consensus algorithm","slug":"consensus-algorithm","link":"/tags/consensus-algorithm/"},{"name":"Zab","slug":"Zab","link":"/tags/Zab/"},{"name":"Raft","slug":"Raft","link":"/tags/Raft/"},{"name":"big data","slug":"big-data","link":"/tags/big-data/"},{"name":"virtual machine","slug":"virtual-machine","link":"/tags/virtual-machine/"},{"name":"programming language","slug":"programming-language","link":"/tags/programming-language/"},{"name":"go","slug":"go","link":"/tags/go/"},{"name":"zookeeper","slug":"zookeeper","link":"/tags/zookeeper/"}],"categories":[{"name":"Course","slug":"Course","link":"/categories/Course/"},{"name":"Paper","slug":"Course/Paper","link":"/categories/Course/Paper/"},{"name":"Document","slug":"Course/Document","link":"/categories/Course/Document/"}],"pages":[]}